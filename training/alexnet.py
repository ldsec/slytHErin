# -*- coding: utf-8 -*-
"""AlexNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H_YJAXxS_ALNqGzdv6PtDOLP2iUuSJpw

Training of AlexNet with simplified pooling on MNIST
"""
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
import math
from activation import ReLUApprox, SigmoidApprox
from utils import *
from dataHandler import DataHandlerAlex
from logger import Logger

####################
#                  #
# MODEL DEFINITION #
#                  #
####################

class AlexNet(nn.Module):
  """AlexNet"""
  def __init__(self, simplified : bool, verbose: bool):
    super().__init__()
    self.verbose = verbose
    ## input size for MNIST = 227
    
    self.simplified = simplified
    if self.simplified:
      self.pool1 = nn.AvgPool2d(kernel_size=3, stride=2)
      self.pool2 = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)
      self.relu = ReLUApprox()
      self.sigmoid = SigmoidApprox()
    else:
      self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)
      self.pool2 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
      self.relu = nn.ReLU(inplace=True)
      self.sigmoid = nn.Sigmoid()

    self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=11, stride=4, padding=2)
    self.conv2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=5, stride=1, padding=2)
    self.conv3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, stride=1, padding=1)
    self.conv4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)
    self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)
    self.classifier = nn.Sequential(
        nn.Dropout(p=0.5),
        nn.Linear(in_features= 9216, out_features= 4096),
        self.relu,
        nn.Dropout(p=0.5),
        nn.Linear(in_features= 4096, out_features= 4096),
        self.relu,
        nn.Linear(in_features= 4096, out_features=10),
        self.sigmoid
    )
    ## init weights
    nn.init.kaiming_uniform_(self.conv1.weight, a=0, mode='fan_out', nonlinearity='relu')
    nn.init.kaiming_uniform_(self.conv2.weight, a=0, mode='fan_out', nonlinearity='relu')
    nn.init.kaiming_uniform_(self.conv3.weight, a=0, mode='fan_out', nonlinearity='relu')
    nn.init.kaiming_uniform_(self.conv4.weight, a=0, mode='fan_out', nonlinearity='relu')
    nn.init.kaiming_uniform_(self.conv5.weight, a=0, mode='fan_out', nonlinearity='relu')
    
    i = 0
    for layer in self.classifier.children():
      if isinstance(layer, nn.Linear):
        if i < 2:
          nn.init.kaiming_uniform_(layer.weight, a=0, mode='fan_out', nonlinearity='relu')
          i += 1
        else:
          nn.init.xavier_uniform_(layer.weight, gain=math.sqrt(2))    
          i += 1
    

  def forward(self,x):
    '''
    Sizes for batch 128:
      torch.Size([128, 3, 227, 227])
      Conv1
      torch.Size([128, 64, 56, 56])
      Pool1
      torch.Size([128, 64, 27, 27]) <<
      Conv2
      torch.Size([128, 192, 27, 27])
      Pool1
      torch.Size([128, 192, 13, 13])
      Conv3
      torch.Size([128, 384, 13, 13])
      Conv4
      torch.Size([128, 256, 13, 13])
      Conv5
      torch.Size([128, 256, 13, 13])
      Pool1
      torch.Size([128, 256, 6, 6])
      Pool2
      torch.Size([128, 256, 6, 6])
      Reshape
      torch.Size([128, 9216])
      Final
      torch.Size([128, 10])
    '''
    print("Start")
    print(x.shape)
    x = self.relu(self.conv1(x))
    print("Conv1")
    print(x.shape)
    x = self.pool1(x)
    print("Pool1")
    print(x.shape)
    x = self.relu(self.conv2(x))
    print("Conv2")
    print(x.shape)
    x = self.pool1(x)
    print("Pool1")
    print(x.shape)
    x = self.relu(self.conv3(x))
    print("Conv3")
    print(x.shape)
    x = self.relu(self.conv4(x))
    print("Conv4")
    print(x.shape)
    x = self.relu(self.conv5(x))
    print("Conv5")
    print(x.shape)
    #print("Conv5",x.max())
    x = self.pool1(x)
    print("Pool1")
    print(x.shape)
    x = self.pool2(x)
    print("Pool2")
    print(x.shape)
    #print("Pools",x.max())
    x = x.reshape(x.shape[0], -1)
    print("Reshape")
    print(x.shape)
    x = self.classifier(x)
    print("Final")
    print(x.shape)
    return x

#########################
#                       #
# TRAIN + TEST PIPELINE #
#                       #
#########################

if __name__ == "__main__":
  parser = argparse.ArgumentParser()
  parser.add_argument("--verbose", help="increase output verbosity", action="store_true")
  parser.add_argument("--simplified", help="use HE friendly functions and pooling", action="store_true")
  args = parser.parse_args()
  
  if args.verbose:
    verbose = True
  else:
    verbose = False
  if args.simplified:
    simplified = True
    name = "AlexNet_simplified"
    lr = 0.001
  else:
    simplified = False
    name = "AlexNet"
    lr = 0.05
  
  print(name)
  dataHandler = DataHandlerAlex("MNIST",128)
  logger = Logger("./logs/", name)
  model = AlexNet(simplified=simplified, verbose=verbose).to(device=device)
  train(logger, model, dataHandler, 50, lr=lr, TPU=False) ##if simplified set lr=0.001
  eval(logger, model, dataHandler)
  #torch.save(model, f"{name}.pt")