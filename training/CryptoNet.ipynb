{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYmdRutcpdeR"
      },
      "source": [
        "# Implementation of (simplified) CryptoNet and AlexNet for inference under homomorphic encryption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "prOXZ9RESeYD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import save_image\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import math\n",
        "\n",
        "## interactive off\n",
        "plt.ioff()\n",
        "## setup torch enviro\n",
        "torch.manual_seed(42)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aopv51NWVxYq"
      },
      "source": [
        "Approximated Relus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "wuBiEkXTV0Cn"
      },
      "outputs": [],
      "source": [
        "def approx_relu_2d(x):\n",
        "  \"\"\"2-degree approx of relu in [-6,6] from https://arxiv.org/pdf/2009.03727.pdf\"\"\"\n",
        "  a = 0.563059\n",
        "  b = 0.5\n",
        "  c = 0.078047\n",
        "  x_2 = torch.square(x)\n",
        "  return a + b*x + c*x_2\n",
        "  \n",
        "def approx_relu_4d(x):\n",
        "  \"\"\"4-degree approx of relu in [-6,6] from https://arxiv.org/pdf/2009.03727.pdf\"\"\"\n",
        "  a = 0.119782\n",
        "  b = 0.5\n",
        "  c = 0.147298\n",
        "  d = -0.002015\n",
        "  x_2 = torch.square(x)\n",
        "  x_4 = torch.square(x_2)\n",
        "  return a + b*x + c*x_2 + d*x_4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrxhF5Msda_u"
      },
      "source": [
        "CryptoNet from [Microsoft](https://www.microsoft.com/en-us/research/publication/cryptonets-applying-neural-networks-to-encrypted-data-with-high-throughput-and-accuracy/) ==> couldn't replicate results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bGKsu0dNuGEH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "9821ebf2-0c7d-42a4-dc53-39d3c11c03b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nclass ScaledAvgPool2d(nn.Module):\\n    \"\"\"Define the ScaledAvgPool layer, a.k.a the Sum Pool\"\"\"\\n    def __init__(self,kernel_size):\\n      super().__init__()\\n      self.kernel_size = kernel_size\\n      self.AvgPool = nn.AvgPool2d(kernel_size=self.kernel_size, stride=1, padding=int(math.ceil((kernel_size-1)/2)))\\n\\n    def forward(self,x):\\n      return (self.kernel_size**2)*self.AvgPool(x)\\n    \\n\\nclass CryptoNet(nn.Module):\\n  \"\"\"\\n    Original 9-layer network used during training\\n    CURRENTLY NOT WORKING\\n  \"\"\"\\n  def __init__(self, verbose):\\n    super().__init__()\\n    self.verbose = verbose\\n    self.pad = F.pad\\n    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\\n    self.square1 = torch.square\\n    self.scaledAvgPool1 = ScaledAvgPool2d(kernel_size=3)\\n    self.conv2 = nn.Conv2d(in_channels=5, out_channels=50, kernel_size=5, stride=2)\\n    self.scaledAvgPool2 = ScaledAvgPool2d(kernel_size=3)\\n    self.fc1 = nn.Linear(in_features=1250, out_features=100)\\n    self.square2 = torch.square\\n    self.fc2 = nn.Linear(in_features=100, out_features=10)\\n    self.sigmoid = nn.Sigmoid()\\n\\n  def forward(self, x):\\n    x = self.pad(x, (1,0,1,0))\\n    if self.verbose:\\n      print(\"Start --> \",x.mean())\\n    x = self.conv1(x)\\n    if self.verbose:\\n      print(\"Conv1 --> \",x.mean())\\n    x = self.square1(x)\\n    if self.verbose:\\n      print(\"Sq --> \",x.mean())\\n    x = self.scaledAvgPool1(x)\\n    if self.verbose:\\n      print(\"Pool --> \",x.mean())\\n    x = self.conv2(x)\\n    if self.verbose:\\n      print(\"Conv2 --> \",x.mean())\\n    x = self.scaledAvgPool2(x)\\n    if self.verbose:\\n      print(\"Pool --> \",x.mean())\\n    ## Flatten\\n    x = x.reshape(x.shape[0], -1)\\n    x = self.fc1(x)\\n    if self.verbose:\\n      print(\"fc1 --> \",x.mean())\\n    x = self.square2(x)\\n    if self.verbose:\\n      print(\"Square --> \",x.mean())\\n    x = self.fc2(x)\\n    if self.verbose:\\n      print(\"fc2 --> \",x.mean())\\n    x = self.sigmoid(x)\\n    return x\\n\\n  def weights_init(self, m):\\n    \"\"\" Custom initilization to avoid square activation to blow up \"\"\"\\n    for m in self.children():\\n      if isinstance(m,nn.Conv2d):\\n        nn.init.kaiming_uniform_(m.weight, a=0, mode=\\'fan_in\\', nonlinearity=\\'relu\\')\\n      elif isinstance(m, nn.Linear):\\n        nn.init.uniform_(m.weight, 1e-4,1e-3)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "'''\n",
        "class ScaledAvgPool2d(nn.Module):\n",
        "    \"\"\"Define the ScaledAvgPool layer, a.k.a the Sum Pool\"\"\"\n",
        "    def __init__(self,kernel_size):\n",
        "      super().__init__()\n",
        "      self.kernel_size = kernel_size\n",
        "      self.AvgPool = nn.AvgPool2d(kernel_size=self.kernel_size, stride=1, padding=int(math.ceil((kernel_size-1)/2)))\n",
        "\n",
        "    def forward(self,x):\n",
        "      return (self.kernel_size**2)*self.AvgPool(x)\n",
        "    \n",
        "\n",
        "class CryptoNet(nn.Module):\n",
        "  \"\"\"\n",
        "    Original 9-layer network used during training\n",
        "    CURRENTLY NOT WORKING\n",
        "  \"\"\"\n",
        "  def __init__(self, verbose):\n",
        "    super().__init__()\n",
        "    self.verbose = verbose\n",
        "    self.pad = F.pad\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\n",
        "    self.square1 = torch.square\n",
        "    self.scaledAvgPool1 = ScaledAvgPool2d(kernel_size=3)\n",
        "    self.conv2 = nn.Conv2d(in_channels=5, out_channels=50, kernel_size=5, stride=2)\n",
        "    self.scaledAvgPool2 = ScaledAvgPool2d(kernel_size=3)\n",
        "    self.fc1 = nn.Linear(in_features=1250, out_features=100)\n",
        "    self.square2 = torch.square\n",
        "    self.fc2 = nn.Linear(in_features=100, out_features=10)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pad(x, (1,0,1,0))\n",
        "    if self.verbose:\n",
        "      print(\"Start --> \",x.mean())\n",
        "    x = self.conv1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Conv1 --> \",x.mean())\n",
        "    x = self.square1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Sq --> \",x.mean())\n",
        "    x = self.scaledAvgPool1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Pool --> \",x.mean())\n",
        "    x = self.conv2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Conv2 --> \",x.mean())\n",
        "    x = self.scaledAvgPool2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Pool --> \",x.mean())\n",
        "    ## Flatten\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.fc1(x)\n",
        "    if self.verbose:\n",
        "      print(\"fc1 --> \",x.mean())\n",
        "    x = self.square2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Square --> \",x.mean())\n",
        "    x = self.fc2(x)\n",
        "    if self.verbose:\n",
        "      print(\"fc2 --> \",x.mean())\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "  def weights_init(self, m):\n",
        "    \"\"\" Custom initilization to avoid square activation to blow up \"\"\"\n",
        "    for m in self.children():\n",
        "      if isinstance(m,nn.Conv2d):\n",
        "        nn.init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        nn.init.uniform_(m.weight, 1e-4,1e-3)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tunwxIS3XLgm"
      },
      "outputs": [],
      "source": [
        "class SimpleNet(nn.Module):\n",
        "  '''\n",
        "    Simpliefied network used in paper for inference https://www.microsoft.com/en-us/research/publication/cryptonets-applying-neural-networks-to-encrypted-data-with-high-throughput-and-accuracy/\n",
        "  '''\n",
        "  def __init__(self, batch_size : int, activation : str, init_method : str, verbose : bool):\n",
        "    super().__init__()\n",
        "    self.verbose = verbose\n",
        "    self.init_method = init_method\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    if activation == \"square\":\n",
        "      self.activation = torch.square\n",
        "    elif activation == \"relu\":\n",
        "      self.activation = nn.ReLU()\n",
        "    elif activation == \"a-relu-2d\":\n",
        "      self.activation = approx_relu_2d\n",
        "    elif activation == \"a-relu-4d\":\n",
        "      self.activation = approx_relu_4d\n",
        "\n",
        "    self.pad = F.pad\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\n",
        "    self.pool1 = nn.Conv2d(in_channels=5, out_channels=100, kernel_size=13, stride=1000)\n",
        "    self.pool2 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(100,1), stride=1000)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pad(x, (1,0,1,0))\n",
        "    x = self.conv1(x)\n",
        "    x = self.activation(self.pool1(x))\n",
        "    x = x.reshape([self.batch_size,1,100,1]) #batch_size tensors in 1 channel, 100x1\n",
        "    x = self.activation(self.pool2(x))\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    return x\n",
        " \n",
        "  def weights_init(self, m):\n",
        "    for m in self.children():\n",
        "      if isinstance(m,nn.Conv2d):\n",
        "        if self.init_method == \"he\":\n",
        "          nn.init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "        elif self.init_method == \"xavier\":\n",
        "          nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2))\n",
        "        elif self.init_method == \"uniform\":\n",
        "          nn.init.uniform_(m.weight, -0.5, 0.5)\n",
        "        elif self.init_method == \"norm\":\n",
        "          nn.init.normal_(m.weight, 0.0, 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYTSG_NYkN8e"
      },
      "source": [
        "Modified AlexNet with ReLU approximation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "68bU0ORv0Xb5"
      },
      "outputs": [],
      "source": [
        "class AlexNet(nn.Module):\n",
        "  def __init__(self, verbose: bool):\n",
        "    super().__init__()\n",
        "    self.verbose = verbose\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4, padding=0)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=3, stride=2) \n",
        "    self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding= 2)\n",
        "    self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding= 1)\n",
        "    self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "    self.fc1  = nn.Linear(in_features= 9216, out_features= 4096)\n",
        "    self.fc2  = nn.Linear(in_features= 4096, out_features= 4096)\n",
        "    self.fc3 = nn.Linear(in_features=4096 , out_features=10)\n",
        "    self.ReLU = nn.ReLU()\n",
        "\n",
        "    nn.init.kaiming_uniform_(self.conv1.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.conv2.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.conv3.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.conv4.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.conv5.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.fc1.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.kaiming_uniform_(self.fc2.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "    nn.init.xavier_uniform_(self.fc3.weight, gain=math.sqrt(2))\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.ReLU(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = self.ReLU(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    x = self.ReLU(self.conv3(x))\n",
        "    x = self.ReLU(self.conv4(x))\n",
        "    x = self.ReLU(self.conv5(x))\n",
        "    x = self.pool(x)\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.ReLU(self.fc1(x))\n",
        "    x = self.ReLU(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    x = torch.sigmoid(x) ## this used to be softmax\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sX-7JDDtHOo"
      },
      "source": [
        "Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3zxoMQRRsF1o"
      },
      "outputs": [],
      "source": [
        "class DataHandler():\n",
        "  def __init__(self, dataset : str, batch_size : int):\n",
        "    if dataset == \"MNIST\":\n",
        "      self.batch_size = batch_size\n",
        "      \n",
        "     \n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "      \n",
        "    train_ds = MNIST(\"data/\", train=True, download=True, transform=transform)\n",
        "    test_ds = MNIST(\"data/\", train=False, download=True, transform=transform)\n",
        "    self.train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle=True, drop_last=True,num_workers=2, pin_memory=True)\n",
        "    self.test_dl = DataLoader(test_ds, batch_size = batch_size, shuffle=True, drop_last=True,num_workers=2, pin_memory=True)\n",
        "\n",
        "dataHandler = DataHandler(dataset=\"MNIST\", batch_size=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXXqZm508qA1"
      },
      "source": [
        "Plot gradient flow for debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "h3f19IYJ8nIQ"
      },
      "outputs": [],
      "source": [
        "def plot_grad_flow(named_parameters):\n",
        "    ## From https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063\n",
        "    ## Beware it's a little bit tricky to interpret results\n",
        "    '''Plots the gradients flowing through different layers in the net during training.\n",
        "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
        "    \n",
        "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
        "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
        "\n",
        "    ave_grads = []\n",
        "    max_grads = []\n",
        "    layers = []\n",
        "    for n, p in named_parameters:\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\n",
        "            layers.append(n)\n",
        "            ave_grads.append(p.grad.abs().mean())\n",
        "            max_grads.append(p.grad.abs().max())\n",
        "            print(f\"Layer {n}, grad avg {p.grad.mean()}, data {p.data.mean()}\")\n",
        "    plt.bar(np.arange(len(max_grads)), max(max_grads), alpha=0.1, lw=1, color=\"c\")\n",
        "    plt.bar(np.arange(len(max_grads)), np.mean(ave_grads), alpha=0.1, lw=1, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(left=0, right=len(ave_grads))\n",
        "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")\n",
        "    plt.grid(True)\n",
        "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
        "                Line2D([0], [0], color=\"b\", lw=4),\n",
        "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OsCh6ldz9PN"
      },
      "source": [
        "Train and test pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XfZdPxHKZwKi"
      },
      "outputs": [],
      "source": [
        "## training params setup\n",
        "learning_rate = 3e-4\n",
        "total_step = len(dataHandler.train_dl)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "## PLOT HELPER\n",
        "def plot_history(key, train, history):\n",
        "  \"\"\" \n",
        "    Plot loss and accuracy history during model run\n",
        "    Input:\n",
        "          key : str => name of the model\n",
        "          train : bool => training 1 or test 0\n",
        "          history : dict{str : list of floats}\n",
        "  \"\"\"\n",
        "  if train:\n",
        "    when = \"train\"\n",
        "  else:\n",
        "    when = \"test\"\n",
        "  fig, ax = plt.subplots( 1, 2, figsize = (12,4) )\n",
        "  ax[0].plot(history['loss'], label = when+\"----\"+key)\n",
        "  ax[0].set_title( \"Loss\" )\n",
        "  ax[0].set_xlabel( \"Epochs\" )\n",
        "  ax[0].set_ylabel( \"Loss\" )\n",
        "  ax[0].grid( True )\n",
        "  ax[0].legend()\n",
        "\n",
        "  ax[1].plot(history['accuracy'], label = when+\"----\"+key)\n",
        "  ax[1].set_title( \"Accuracy\" )\n",
        "  ax[1].set_xlabel( \"Epochs\" )\n",
        "  ax[1].set_ylabel( \"Accuracy\" )\n",
        "  ax[1].grid( True )\n",
        "  ax[1].legend()\n",
        "\n",
        "  plt.savefig(f\"{key}_{when}.png\")\n",
        "\n",
        "## TRAIN\n",
        "def train(key, model, dataHandler, num_epochs, TPU=False):\n",
        "  num_epochs = num_epochs\n",
        "  model.train()\n",
        "  #optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  trainHistory = {}\n",
        "  trainHistory['loss'] = []\n",
        "  trainHistory['accuracy'] = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    for i, (data, labels) in enumerate(dataHandler.train_dl):\n",
        "      data = data.to(device=device)\n",
        "      labels = labels.to(device=device)\n",
        "      #labels = labels.to(torch.float32)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      predictions = model(data)\n",
        "      loss = criterion(predictions, labels)\n",
        "      loss.backward()\n",
        "      \n",
        "      if model.verbose:\n",
        "        print(f\"[?] Step {i+1} Epoch {epoch+1}\")\n",
        "        plot_grad_flow(model.named_parameters())\n",
        "      \n",
        "      if not TPU:\n",
        "        optimizer.step()\n",
        "      else:\n",
        "        xm.optimizer_step(optimizer, barrier=True) ## if TPU \n",
        "      \n",
        "      _, predicted_labels = predictions.max(1)\n",
        "      num_correct += (predicted_labels == labels).sum()\n",
        "      num_samples += predicted_labels.size(0)\n",
        "      \n",
        "      epoch_accuracy += num_correct/num_samples\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      if (i+1) % 100 == 0:\n",
        "        print(\"=====================================================================================================================\")\n",
        "        print ('[!] Train Epoch [{}/{}], Step [{}/{}] ==> Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "      \n",
        "    trainHistory['loss'].append(epoch_loss/len(dataHandler.train_dl))\n",
        "    trainHistory['accuracy'].append(epoch_accuracy/len(dataHandler.train_dl))\n",
        "    \n",
        "  plot_history(key, True, trainHistory)\n",
        "\n",
        "\n",
        "## EVAL \n",
        "def eval(key, model, dataHandler):\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "\n",
        "  model.eval()\n",
        "  testHistory = {}\n",
        "  testHistory['loss'] = []\n",
        "  testHistory['accuracy'] = []\n",
        "  test_loss = 0\n",
        "  accuracy = 0\n",
        "  with torch.no_grad():\n",
        "    for _, (data,labels) in enumerate(dataHandler.test_dl):\n",
        "        data = data.to(device=\"cpu\")\n",
        "        labels = labels.to(device=\"cpu\")\n",
        "        ## Forward Pass\n",
        "        predictions = model(data)\n",
        "        loss = criterion(predictions, labels).item()\n",
        "        test_loss += loss\n",
        "        _, predicted_labels = predictions.max(1)\n",
        "        num_correct += (predicted_labels == labels).sum()\n",
        "        num_samples += predicted_labels.size(0)\n",
        "        testHistory['loss'].append(loss)\n",
        "        testHistory['accuracy'].append(float(num_correct) / float(num_samples))\n",
        "  \n",
        "    accuracy = float(num_correct) / float(num_samples)\n",
        "    test_loss = test_loss/len(dataHandler.test_dl)\n",
        "    print(\"=============================\")\n",
        "    print(f\"Average test Loss ==> {test_loss}\")\n",
        "    print(f\"Test accuracy ==> {float(num_correct) / float(num_samples) * 100:.2f}\")\n",
        "\n",
        "    plot_history(key, False, testHistory)\n",
        "\n",
        "  return test_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30gVJjuXdxc1"
      },
      "source": [
        "\n",
        "Training and evaluation of SimpleNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn61Mth4dz-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8c7e91-84e1-4108-9724-d63bec9824df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================================================================================\n",
            "[!] Train Epoch [1/10], Step [100/234] ==> Loss: 0.4587\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [1/10], Step [200/234] ==> Loss: 0.3362\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [2/10], Step [100/234] ==> Loss: 0.2130\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [2/10], Step [200/234] ==> Loss: 0.2530\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [3/10], Step [100/234] ==> Loss: 0.1318\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [3/10], Step [200/234] ==> Loss: 0.1515\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [4/10], Step [100/234] ==> Loss: 0.1708\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [4/10], Step [200/234] ==> Loss: 0.1400\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [5/10], Step [100/234] ==> Loss: 0.0843\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [5/10], Step [200/234] ==> Loss: 0.0835\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [6/10], Step [100/234] ==> Loss: 0.1128\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [6/10], Step [200/234] ==> Loss: 0.1780\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [7/10], Step [100/234] ==> Loss: 0.1083\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [7/10], Step [200/234] ==> Loss: 0.1103\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [8/10], Step [100/234] ==> Loss: 0.0748\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [8/10], Step [200/234] ==> Loss: 0.0897\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [9/10], Step [100/234] ==> Loss: 0.0803\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [9/10], Step [200/234] ==> Loss: 0.0408\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [10/10], Step [100/234] ==> Loss: 0.1132\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [10/10], Step [200/234] ==> Loss: 0.0730\n",
            "=============================\n",
            "Average test Loss ==> 0.08482991034785907\n",
            "Test accuracy ==> 97.34\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [1/10], Step [100/234] ==> Loss: 0.2409\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [1/10], Step [200/234] ==> Loss: 0.1953\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [2/10], Step [100/234] ==> Loss: 0.0814\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [2/10], Step [200/234] ==> Loss: 0.0818\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [3/10], Step [100/234] ==> Loss: 0.0869\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [3/10], Step [200/234] ==> Loss: 0.0593\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [4/10], Step [100/234] ==> Loss: 0.0740\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [4/10], Step [200/234] ==> Loss: 0.0563\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [5/10], Step [100/234] ==> Loss: 0.0768\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [5/10], Step [200/234] ==> Loss: 0.0705\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [6/10], Step [100/234] ==> Loss: 0.0552\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [6/10], Step [200/234] ==> Loss: 0.0647\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [7/10], Step [100/234] ==> Loss: 0.0377\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [7/10], Step [200/234] ==> Loss: 0.1261\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [8/10], Step [100/234] ==> Loss: 0.0451\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [8/10], Step [200/234] ==> Loss: 0.0215\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [9/10], Step [100/234] ==> Loss: 0.0376\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [9/10], Step [200/234] ==> Loss: 0.0252\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [10/10], Step [100/234] ==> Loss: 0.0274\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [10/10], Step [200/234] ==> Loss: 0.0198\n",
            "=============================\n",
            "Average test Loss ==> 0.11321319439090215\n",
            "Test accuracy ==> 97.14\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [1/10], Step [100/234] ==> Loss: 0.3596\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [1/10], Step [200/234] ==> Loss: 0.2261\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [2/10], Step [100/234] ==> Loss: 0.3179\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [2/10], Step [200/234] ==> Loss: 0.1912\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [3/10], Step [100/234] ==> Loss: 0.2348\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [3/10], Step [200/234] ==> Loss: 0.1909\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [4/10], Step [100/234] ==> Loss: 0.1585\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [4/10], Step [200/234] ==> Loss: 0.1978\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [5/10], Step [100/234] ==> Loss: 0.1462\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [5/10], Step [200/234] ==> Loss: 0.1260\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [6/10], Step [100/234] ==> Loss: 0.1384\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [6/10], Step [200/234] ==> Loss: 0.1466\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [7/10], Step [100/234] ==> Loss: 0.1040\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [7/10], Step [200/234] ==> Loss: 0.1178\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [8/10], Step [100/234] ==> Loss: 0.0423\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [8/10], Step [200/234] ==> Loss: 0.0807\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [9/10], Step [100/234] ==> Loss: 0.0900\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [9/10], Step [200/234] ==> Loss: 0.0764\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [10/10], Step [100/234] ==> Loss: 0.1036\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [10/10], Step [200/234] ==> Loss: 0.0617\n",
            "=============================\n",
            "Average test Loss ==> 0.08863635112841924\n",
            "Test accuracy ==> 97.46\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [1/10], Step [100/234] ==> Loss: 0.5199\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [1/10], Step [200/234] ==> Loss: 0.3435\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [2/10], Step [100/234] ==> Loss: 0.3869\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [2/10], Step [200/234] ==> Loss: 0.2419\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [3/10], Step [100/234] ==> Loss: 0.2538\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [3/10], Step [200/234] ==> Loss: 0.2225\n"
          ]
        }
      ],
      "source": [
        "##############################\n",
        "#                            #\n",
        "# TRAINING AND EVAL PIPELINE #\n",
        "#                            #\n",
        "##############################\n",
        "\n",
        "## init models\n",
        "methods = [\"random\", \"he\", \"xavier\", \"uniform\", \"norm\"]\n",
        "activations = [\"relu\",\"square\", \"a-relu-2d\", \"a-relu-4d\"]\n",
        "models = {}\n",
        "\n",
        "for method in methods:\n",
        "  for activation in activations:\n",
        "    models[method+\"_\"+activation] = SimpleNet(batch_size=dataHandler.batch_size, activation=activation, init_method=method,verbose=False).to(device=device)\n",
        "scores = {}\n",
        "\n",
        "## Testing of different stuff ==> result was best xavier+square\n",
        "for key, model in models.items():\n",
        "  model.apply(model.weights_init)\n",
        "  train(key, model, dataHandler, num_epochs=10)\n",
        "  loss, accuracy = eval(key,model, dataHandler)\n",
        "  scores[key] = {\"loss\":loss, \"accuracy\":accuracy}\n",
        "  torch.save(model, f\"SimpleNet_{key}.pt\")\n",
        "\n",
        "## Best Model on 60 epochs\n",
        "#key = \"xavier_square\"\n",
        "#model = models[key]\n",
        "#model.apply(model.weights_init)\n",
        "#train(key, model, dataHandler, num_epochs=150, TPU=False)\n",
        "#loss, accuracy = eval(key,model, dataHandler)\n",
        "#scores[key] = {\"loss\":loss, \"accuracy\":accuracy}\n",
        "#torch.save(model, f\"SimpleNet_{key}.pt\")\n",
        "\n",
        "for key, metrics in scores.items():\n",
        "  print(\"=====================================================================\")\n",
        "  print(f\"[+] Model with {key}: Avg test Loss ==> {metrics['loss']}, Accuracy ==> {metrics['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PAiAjR4u4gH"
      },
      "source": [
        "# Results of SimpleNet evaluation\n",
        "Best model seems to be the one with square activation and xavier initialization."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}