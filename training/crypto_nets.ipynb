{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crypto_nets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CryptoNet implementation and training on MNIST dataset"
      ],
      "metadata": {
        "id": "HYmdRutcpdeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "prOXZ9RESeYD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import save_image\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "DUluBfyZuKld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DUMMY MODEL\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()        \n",
        "        self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=1,              \n",
        "                out_channels=16,            \n",
        "                kernel_size=5,              \n",
        "                stride=1,                   \n",
        "                padding=2,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(kernel_size=2),    \n",
        "        )\n",
        "        self.conv2 = nn.Sequential(         \n",
        "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(2),                \n",
        "        )        # fully connected layer, output 10 classes\n",
        "        self.out = nn.Linear(32 * 7 * 7, 10)    \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "        x = x.view(x.size(0), -1)       \n",
        "        output = self.out(x)\n",
        "        return output  # return x for visualization"
      ],
      "metadata": {
        "id": "Ii4rDPKqVPYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledAvgPool2d(nn.Module):\n",
        "    \"\"\"Define the ScaledAvgPool layer, a.k.a the Sum Pool\"\"\"\n",
        "    def __init__(self,kernel_size):\n",
        "      super().__init__()\n",
        "      self.kernel_size = kernel_size\n",
        "      self.AvgPool = nn.AvgPool2d(kernel_size=self.kernel_size, stride=1, padding=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "      return (self.kernel_size**2)*self.AvgPool(x)\n",
        "    \n",
        "\n",
        "class CryptoNet(nn.Module):\n",
        "  '''\n",
        "    TO DO: check how in the paper the avg pool does not downscale the input size...weird padding?\n",
        "    EDIT: probably yes, it's a same convolution\n",
        "  '''\n",
        "  def __init__(self, verbose):\n",
        "    super().__init__()\n",
        "    self.verbose = verbose\n",
        "    self.pad = F.pad\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\n",
        "    self.square1 = torch.square\n",
        "    self.scaledAvgPool1 = ScaledAvgPool2d(kernel_size=3)\n",
        "    self.conv2 = nn.Conv2d(in_channels=5, out_channels=50, kernel_size=5, stride=2)\n",
        "    self.scaledAvgPool2 = ScaledAvgPool2d(kernel_size=3)\n",
        "    self.fc1 = nn.Linear(in_features=1250, out_features=100) # in paper in_features was 1250\n",
        "    self.square2 = torch.square\n",
        "    self.fc2 = nn.Linear(in_features=100, out_features=10)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pad(x, (1,0,1,0))\n",
        "    if self.verbose:\n",
        "      print(\"Start --> \",x.mean())\n",
        "    x = self.conv1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Conv1 --> \",x.mean())\n",
        "    x = self.square1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Sq --> \",x.mean())\n",
        "    x = self.scaledAvgPool1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Pool --> \",x.mean())\n",
        "    x = self.conv2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Conv2 --> \",x.mean())\n",
        "    x = self.scaledAvgPool2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Pool --> \",x.mean())\n",
        "    ## Flatten\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.fc1(x)\n",
        "    if self.verbose:\n",
        "      print(\"fc1 --> \",x.mean())\n",
        "    x = self.square2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Square --> \",x.mean())\n",
        "    x = self.fc2(x)\n",
        "    if self.verbose:\n",
        "      print(\"fc2 --> \",x.mean())\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "  def weights_init(self, m):\n",
        "    \"\"\" Custom initilization to avoid square activation to blow up \"\"\"\n",
        "    for m in self.children():\n",
        "      if isinstance(m,nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "        #nn.init.uniform_(m.weight, 1e-4,1e-2)\n"
      ],
      "metadata": {
        "id": "bGKsu0dNuGEH"
      },
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Datasets"
      ],
      "metadata": {
        "id": "7sX-7JDDtHOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataHandler():\n",
        "  def __init__(self, dataset : str):\n",
        "    if dataset == \"MNIST\":\n",
        "      transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "      train_ds = MNIST(\"data/\", train=True, download=True, transform=transform)\n",
        "      test_ds = MNIST(\"data/\", train=False, download=True)\n",
        "\n",
        "      self.train_dl = DataLoader(train_ds, batch_size = 512, shuffle=True, drop_last=True)\n",
        "      self.test_dl = DataLoader(test_ds, batch_size = 512, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "3zxoMQRRsF1o"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot gradient flow"
      ],
      "metadata": {
        "id": "xXXqZm508qA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_grad_flow(named_parameters):\n",
        "    ## From https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063\n",
        "    ## Beware it's a little bit tricky to interpret results\n",
        "    '''Plots the gradients flowing through different layers in the net during training.\n",
        "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
        "    \n",
        "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
        "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
        "\n",
        "    ave_grads = []\n",
        "    max_grads = []\n",
        "    layers = []\n",
        "    for n, p in named_parameters:\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\n",
        "            layers.append(n)\n",
        "            ave_grads.append(p.grad.abs().mean())\n",
        "            max_grads.append(p.grad.abs().max())\n",
        "            print(f\"Layer {n}, grad avg {p.grad.mean()}, data {p.data.mean()}\")\n",
        "    plt.bar(np.arange(len(max_grads)), max(max_grads), alpha=0.1, lw=1, color=\"c\")\n",
        "    plt.bar(np.arange(len(max_grads)), np.mean(ave_grads), alpha=0.1, lw=1, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(left=0, right=len(ave_grads))\n",
        "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")\n",
        "    plt.grid(True)\n",
        "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
        "                Line2D([0], [0], color=\"b\", lw=4),\n",
        "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
        "    \n"
      ],
      "metadata": {
        "id": "h3f19IYJ8nIQ"
      },
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "TuwFtAqgtLYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## setup torch enviro\n",
        "torch.manual_seed(9329582034)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "## init model\n",
        "model = CryptoNet(verbose=True)\n",
        "model.apply(model.weights_init)\n",
        "model = model.to(device=device)\n",
        "\n",
        "dataHandler = DataHandler(\"MNIST\")\n",
        "\n",
        "## training params setup\n",
        "learning_rate = 1e-4\n",
        "momentum = 0.9\n",
        "num_epochs = 5\n",
        "total_step = len(dataHandler.train_dl)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (data, labels) in enumerate(dataHandler.train_dl):\n",
        "    data = data.to(device=device)\n",
        "    labels = labels.to(device=device)\n",
        "    #labels = labels.to(torch.float32)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(data)\n",
        "    loss = criterion(predictions, labels)\n",
        "    loss.backward()\n",
        "    if model.verbose:\n",
        "      print(f\"[?] Step {i+1} Epoch {epoch+1}\")\n",
        "      plot_grad_flow(model.named_parameters())\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 10 == 0:\n",
        "      print ('[!] Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "\n",
        "torch.save(model, \"cryptoNet.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xnkgcAJktEu9",
        "outputId": "6bdf9d57-1e09-4461-c81c-8e7b180b6bd4"
      },
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start -->  tensor(0.0042)\n",
            "Conv1 -->  tensor(-0.0353, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.6098, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.2325, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.9093, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(6.1172, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(43.1328, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(163992.2031, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(37076.7617, grad_fn=<MeanBackward0>)\n",
            "[?] Step 1 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.002935816301032901\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0005131831276230514\n",
            "Layer fc1.weight, grad avg 0.0, data 8.436525968136266e-05\n",
            "Layer fc2.weight, grad avg 0.0, data 0.00010607475269353017\n",
            "Start -->  tensor(0.0038)\n",
            "Conv1 -->  tensor(-0.0354, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.6046, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.1876, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.8944, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(6.0548, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(42.8319, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(164321.1875, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(37076.5234, grad_fn=<MeanBackward0>)\n",
            "[?] Step 2 Epoch 1\n",
            "Layer conv1.weight, grad avg 1.7362717485980284e-08, data -0.002935816301032901\n",
            "Layer conv2.weight, grad avg 2.592988934679852e-09, data 0.0005131831276230514\n",
            "Layer fc1.weight, grad avg -1.1443133766941838e-11, data 8.436525968136266e-05\n",
            "Layer fc2.weight, grad avg 6.311472677822394e-09, data 0.00010607475269353017\n",
            "Start -->  tensor(0.0122)\n",
            "Conv1 -->  tensor(-0.0362, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.6470, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.5744, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.9318, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(6.2948, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(43.7127, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(173517.4219, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(40662.4531, grad_fn=<MeanBackward0>)\n",
            "[?] Step 3 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.002935816301032901\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0005131831276230514\n",
            "Layer fc1.weight, grad avg 0.0, data 8.436525968136266e-05\n",
            "Layer fc2.weight, grad avg 0.0, data 0.00010607475269353017\n",
            "Start -->  tensor(0.0016)\n",
            "Conv1 -->  tensor(-0.0355, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.5965, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.1104, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.8534, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(5.6905, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(43.0411, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(160872.4375, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(33952.4375, grad_fn=<MeanBackward0>)\n",
            "[?] Step 4 Epoch 1\n",
            "Layer conv1.weight, grad avg -0.08674988895654678, data -0.002935816301032901\n",
            "Layer conv2.weight, grad avg -0.014279861934483051, data 0.0005131831276230514\n",
            "Layer fc1.weight, grad avg -1.464112301619025e-05, data 8.436525968136266e-05\n",
            "Layer fc2.weight, grad avg 0.06732183694839478, data 0.00010607475269353017\n",
            "Start -->  tensor(-0.0068)\n",
            "Conv1 -->  tensor(-0.0346, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.5619, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(13.8110, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.8497, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(5.5828, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(42.5556, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(157184.0625, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(31382.8242, grad_fn=<MeanBackward0>)\n",
            "[?] Step 5 Epoch 1\n",
            "Layer conv1.weight, grad avg 1.2140579948774048e-36, data -0.0029271417297422886\n",
            "Layer conv2.weight, grad avg 1.8460642069805167e-37, data 0.000514611485414207\n",
            "Layer fc1.weight, grad avg -9.269557111643985e-39, data 8.436672942480072e-05\n",
            "Layer fc2.weight, grad avg 6.900390486060496e-37, data 9.934318222803995e-05\n",
            "Start -->  tensor(-0.0058)\n",
            "Conv1 -->  tensor(-0.0343, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.5670, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(13.8555, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.8894, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(6.0265, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(41.9444, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(158093.2656, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(30616.9414, grad_fn=<MeanBackward0>)\n",
            "[?] Step 6 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.0029193295631557703\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0005158965941518545\n",
            "Layer fc1.weight, grad avg 0.0, data 8.436801726929843e-05\n",
            "Layer fc2.weight, grad avg 0.0, data 9.328377200290561e-05\n",
            "Start -->  tensor(0.0088)\n",
            "Conv1 -->  tensor(-0.0359, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.6261, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.3847, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.8640, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(5.6111, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(42.2485, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(166096.0469, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(23397.6719, grad_fn=<MeanBackward0>)\n",
            "[?] Step 7 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.0029123066924512386\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0005170533549971879\n",
            "Layer fc1.weight, grad avg 0.0, data 8.436919597443193e-05\n",
            "Layer fc2.weight, grad avg 0.0, data 8.7830783741083e-05\n",
            "Start -->  tensor(-0.0060)\n",
            "Conv1 -->  tensor(-0.0348, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.5645, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(13.8327, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.8398, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(5.6744, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(42.2637, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(153684.5469, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(22757.3164, grad_fn=<MeanBackward0>)\n",
            "[?] Step 8 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.0029059844091534615\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0005180941079743207\n",
            "Layer fc1.weight, grad avg 0.0, data 8.437026554020122e-05\n",
            "Layer fc2.weight, grad avg 0.0, data 8.292275742860511e-05\n",
            "Start -->  tensor(0.0050)\n",
            "Conv1 -->  tensor(-0.0353, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.6095, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.2268, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.8856, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(5.9071, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(42.3222, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(157756.3750, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(19677.5703, grad_fn=<MeanBackward0>)\n",
            "[?] Step 9 Epoch 1\n",
            "Layer conv1.weight, grad avg 2.584165304142516e-05, data -0.0029002923984080553\n",
            "Layer conv2.weight, grad avg -1.8081658708979376e-05, data 0.000519030902069062\n",
            "Layer fc1.weight, grad avg -6.676953745454739e-08, data 8.437122596660629e-05\n",
            "Layer fc2.weight, grad avg -6.977642624406144e-05, data 7.850658585084602e-05\n",
            "Start -->  tensor(0.0107)\n",
            "Conv1 -->  tensor(-0.0361, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.6274, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.3983, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.8569, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(5.7381, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(42.5639, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(166968.3125, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(17933.0977, grad_fn=<MeanBackward0>)\n",
            "[?] Step 10 Epoch 1\n",
            "Layer conv1.weight, grad avg 3.383390188217163, data -0.0028951687272638083\n",
            "Layer conv2.weight, grad avg 0.10760490596294403, data 0.0005198761355131865\n",
            "Layer fc1.weight, grad avg -0.0015695869224146008, data 8.437208452960476e-05\n",
            "Layer fc2.weight, grad avg 0.6939783692359924, data 7.453835132764652e-05\n",
            "[!] Epoch [1/5], Step [10/117], Loss: 2.4444\n",
            "Start -->  tensor(0.0039)\n",
            "Conv1 -->  tensor(-0.0365, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.6019, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.1653, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.8281, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(5.5487, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(40.8882, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(171167.9375, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-37913.9336, grad_fn=<MeanBackward0>)\n",
            "[?] Step 11 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.0032289002556353807\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0005098762921988964\n",
            "Layer fc1.weight, grad avg 0.0, data 8.452984184259549e-05\n",
            "Layer fc2.weight, grad avg 0.0, data 1.5680790284022805e-06\n",
            "Start -->  tensor(-0.0043)\n",
            "Conv1 -->  tensor(-0.0362, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.5665, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(13.8453, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.6801, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(4.3819, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(38.4973, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(165683.2344, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-98159.8281, grad_fn=<MeanBackward0>)\n",
            "[?] Step 12 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.003529258770868182\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0005008765147067606\n",
            "Layer fc1.weight, grad avg 0.0, data 8.467183943139389e-05\n",
            "Layer fc2.weight, grad avg 0.0, data -6.410384230548516e-05\n",
            "Start -->  tensor(0.0063)\n",
            "Conv1 -->  tensor(-0.0383, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.6238, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.3671, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.7754, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(5.1551, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(37.3568, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(187140.2031, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-160642.7812, grad_fn=<MeanBackward0>)\n",
            "[?] Step 13 Epoch 1\n",
            "Layer conv1.weight, grad avg -0.541276752948761, data -0.003799581900238991\n",
            "Layer conv2.weight, grad avg 0.06092662736773491, data 0.0004927766276523471\n",
            "Layer fc1.weight, grad avg 0.001561817480251193, data 8.479963435092941e-05\n",
            "Layer fc2.weight, grad avg 0.149735689163208, data -0.00012320911628194153\n",
            "Start -->  tensor(-0.0033)\n",
            "Conv1 -->  tensor(-0.0377, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.5869, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.0240, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.5914, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(3.7803, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(34.7517, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(182204.1875, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-208721.5938, grad_fn=<MeanBackward0>)\n",
            "[?] Step 14 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.0039887442253530025\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0004793935513589531\n",
            "Layer fc1.weight, grad avg 0.0, data 8.475839422317222e-05\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00019137715571559966\n",
            "Start -->  tensor(-0.0058)\n",
            "Conv1 -->  tensor(-0.0377, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.5832, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(13.9975, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.4759, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(2.7333, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(33.0344, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(191723.9219, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-272188.2500, grad_fn=<MeanBackward0>)\n",
            "[?] Step 15 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.004158991854637861\n",
            "Layer conv2.weight, grad avg 0.0, data 0.00046734916395507753\n",
            "Layer fc1.weight, grad avg 0.0, data 8.472133777104318e-05\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0002527286414988339\n",
            "Start -->  tensor(-0.0002)\n",
            "Conv1 -->  tensor(-0.0392, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.6023, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.1671, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.5320, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(3.2944, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(32.1917, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(211397.0781, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-334373.6562, grad_fn=<MeanBackward0>)\n",
            "[?] Step 16 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.004312214441597462\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0004565093549899757\n",
            "Layer fc1.weight, grad avg 0.0, data 8.468798478133976e-05\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00030794512713328004\n",
            "Start -->  tensor(0.0012)\n",
            "Conv1 -->  tensor(-0.0396, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.6135, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.2622, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.4784, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(2.8981, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(29.7516, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(212139.9531, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-374235.7500, grad_fn=<MeanBackward0>)\n",
            "[?] Step 17 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.004450113978236914\n",
            "Layer conv2.weight, grad avg 0.0, data 0.00044675328535959125\n",
            "Layer fc1.weight, grad avg 0.0, data 8.465795690426603e-05\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0003576400922611356\n",
            "Start -->  tensor(-0.0060)\n",
            "Conv1 -->  tensor(-0.0390, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.5909, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.0656, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.4046, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(2.3101, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(27.9405, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(217375.5938, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-414178.0938, grad_fn=<MeanBackward0>)\n",
            "[?] Step 18 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.004574221558868885\n",
            "Layer conv2.weight, grad avg 0.0, data 0.00043797309626825154\n",
            "Layer fc1.weight, grad avg 0.0, data 8.463092672172934e-05\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004023647343274206\n",
            "Start -->  tensor(0.0072)\n",
            "Conv1 -->  tensor(-0.0411, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.6336, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.4484, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.4368, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(2.6752, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(25.6473, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(234033.8438, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-470298.3438, grad_fn=<MeanBackward0>)\n",
            "[?] Step 19 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.004685920663177967\n",
            "Layer conv2.weight, grad avg 0.0, data 0.00043007079511880875\n",
            "Layer fc1.weight, grad avg 0.0, data 8.460662502329797e-05\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004426169325597584\n",
            "Start -->  tensor(0.0015)\n",
            "Conv1 -->  tensor(-0.0404, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.6165, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.3019, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.3750, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(2.2513, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(25.8656, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(237540.9375, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-504496.9062, grad_fn=<MeanBackward0>)\n",
            "[?] Step 20 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.004786447621881962\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0004229587211739272\n",
            "Layer fc1.weight, grad avg 0.0, data 8.458472439087927e-05\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00047884471132420003\n",
            "[!] Epoch [1/5], Step [20/117], Loss: 2.4105\n",
            "Start -->  tensor(-0.0003)\n",
            "Conv1 -->  tensor(-0.0407, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.6077, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.2159, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.3872, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(2.2424, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(24.0316, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(243407.2812, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-536871.3750, grad_fn=<MeanBackward0>)\n",
            "[?] Step 21 Epoch 1\n",
            "Layer conv1.weight, grad avg -1.544040100361728e-22, data -0.004876924678683281\n",
            "Layer conv2.weight, grad avg -1.400538227710661e-24, data 0.00041655797394923866\n",
            "Layer fc1.weight, grad avg 4.139503226721341e-26, data 8.456503564957529e-05\n",
            "Layer fc2.weight, grad avg 2.6896805786911124e-23, data -0.0005114491796121001\n",
            "Start -->  tensor(-0.0018)\n",
            "Conv1 -->  tensor(-0.0407, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.6008, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.1503, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.3201, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(1.7237, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(23.0156, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(247257.2969, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-563512.1250, grad_fn=<MeanBackward0>)\n",
            "[?] Step 22 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.004958353005349636\n",
            "Layer conv2.weight, grad avg 0.0, data 0.00041079672519117594\n",
            "Layer fc1.weight, grad avg 0.0, data 8.454731869278476e-05\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0005407937569543719\n",
            "Start -->  tensor(0.0027)\n",
            "Conv1 -->  tensor(-0.0414, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.6303, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.4151, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.3487, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(1.9940, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(22.9933, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(272929.5000, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-633739.5000, grad_fn=<MeanBackward0>)\n",
            "[?] Step 23 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.005031639710068703\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0004056126927025616\n",
            "Layer fc1.weight, grad avg 0.0, data 8.453136979369447e-05\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0005672033876180649\n",
            "Start -->  tensor(0.0070)\n",
            "Conv1 -->  tensor(-0.0423, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(1.6493, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(14.5838, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(0.3086, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(1.5880, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(22.2557, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(277645.4375, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-667154.8125, grad_fn=<MeanBackward0>)\n",
            "[?] Step 24 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data -0.005097593180835247\n",
            "Layer conv2.weight, grad avg 0.0, data 0.00040094638825394213\n",
            "Layer fc1.weight, grad avg 0.0, data 8.451699977740645e-05\n",
            "Layer fc2.weight, grad avg 0.0, data -0.000590972718782723\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-294-687324a43e08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "lz5pM3GavjgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = CNN()\n",
        "\n",
        "def grad_clipping(model):\n",
        "  for p in model.parameters():\n",
        "    p.register_hook(lambda grad: print(grad))\n",
        "    p.register_hook(lambda grad: torch.clamp(grad, 0, 1.0))\n",
        "\n",
        "    p.register_hook(lambda grad: print(f\"{p} -> {grad}\"))\n",
        "\n",
        "#grad_clipping(model)\n",
        "model = model.to(device=device)\n",
        "dataHandler = DataHandler(\"MNIST\")\n",
        "\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9\n",
        "num_epochs = 1\n",
        "total_step = len(dataHandler.train_dl)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (data, labels) in enumerate(dataHandler.train_dl):\n",
        "    data = data.to(device=device)\n",
        "    labels = labels.to(device=device)\n",
        "    #labels = labels.to(torch.float32)\n",
        "\n",
        "    ## Forward\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(data)\n",
        "    loss = criterion(predictions, labels)\n",
        "    loss.backward()\n",
        "    plot_grad_flow(model.named_parameters())\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "      print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "torch.save(model, \"cryptoNet.pt\")"
      ],
      "metadata": {
        "id": "uD7LT4QNvkaJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d46c1a6-e6cd-46f1-cd85-f4ac5cc39dd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0055), tensor(0.0038), tensor(0.0065)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0054), tensor(0.0037), tensor(0.0060)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0063), tensor(0.0039), tensor(0.0060)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0080), tensor(0.0043), tensor(0.0066)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0090), tensor(0.0047), tensor(0.0065)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0138), tensor(0.0054), tensor(0.0082)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0172), tensor(0.0060), tensor(0.0076)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0229), tensor(0.0062), tensor(0.0079)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0239), tensor(0.0062), tensor(0.0082)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0258), tensor(0.0066), tensor(0.0108)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0271), tensor(0.0063), tensor(0.0107)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0260), tensor(0.0069), tensor(0.0166)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0299), tensor(0.0067), tensor(0.0149)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0284), tensor(0.0062), tensor(0.0144)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0300), tensor(0.0063), tensor(0.0134)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0291), tensor(0.0064), tensor(0.0156)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0251), tensor(0.0047), tensor(0.0108)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0219), tensor(0.0046), tensor(0.0144)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0174), tensor(0.0043), tensor(0.0142)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0199), tensor(0.0052), tensor(0.0150)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0149), tensor(0.0039), tensor(0.0147)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0131), tensor(0.0042), tensor(0.0168)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0168), tensor(0.0050), tensor(0.0189)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0075), tensor(0.0043), tensor(0.0186)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0152), tensor(0.0045), tensor(0.0180)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0098), tensor(0.0040), tensor(0.0157)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0139), tensor(0.0059), tensor(0.0222)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0093), tensor(0.0049), tensor(0.0220)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0104), tensor(0.0035), tensor(0.0146)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0103), tensor(0.0042), tensor(0.0175)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0099), tensor(0.0044), tensor(0.0201)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0087), tensor(0.0041), tensor(0.0198)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0127), tensor(0.0046), tensor(0.0162)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0069), tensor(0.0035), tensor(0.0152)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0089), tensor(0.0040), tensor(0.0146)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0079), tensor(0.0042), tensor(0.0170)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0148), tensor(0.0053), tensor(0.0198)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0123), tensor(0.0055), tensor(0.0160)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0090), tensor(0.0039), tensor(0.0131)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0069), tensor(0.0036), tensor(0.0165)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0067), tensor(0.0031), tensor(0.0126)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0058), tensor(0.0029), tensor(0.0116)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0077), tensor(0.0027), tensor(0.0100)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0119), tensor(0.0056), tensor(0.0176)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0066), tensor(0.0031), tensor(0.0115)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0088), tensor(0.0027), tensor(0.0112)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0079), tensor(0.0033), tensor(0.0146)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0066), tensor(0.0033), tensor(0.0128)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0084), tensor(0.0034), tensor(0.0149)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0079), tensor(0.0033), tensor(0.0143)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0090), tensor(0.0035), tensor(0.0122)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0119), tensor(0.0043), tensor(0.0140)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0054), tensor(0.0028), tensor(0.0089)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0093), tensor(0.0039), tensor(0.0161)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0150), tensor(0.0074), tensor(0.0212)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0098), tensor(0.0049), tensor(0.0193)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0084), tensor(0.0033), tensor(0.0100)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0108), tensor(0.0032), tensor(0.0142)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0087), tensor(0.0029), tensor(0.0090)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0060), tensor(0.0028), tensor(0.0110)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0083), tensor(0.0034), tensor(0.0122)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0128), tensor(0.0060), tensor(0.0157)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0123), tensor(0.0052), tensor(0.0157)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0078), tensor(0.0038), tensor(0.0143)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0070), tensor(0.0031), tensor(0.0109)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0059), tensor(0.0044), tensor(0.0180)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0053), tensor(0.0029), tensor(0.0110)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0067), tensor(0.0029), tensor(0.0080)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0142), tensor(0.0063), tensor(0.0156)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0078), tensor(0.0038), tensor(0.0118)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0112), tensor(0.0044), tensor(0.0137)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0083), tensor(0.0033), tensor(0.0093)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0136), tensor(0.0050), tensor(0.0124)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0133), tensor(0.0050), tensor(0.0132)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0066), tensor(0.0025), tensor(0.0081)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0080), tensor(0.0036), tensor(0.0139)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0103), tensor(0.0044), tensor(0.0142)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0085), tensor(0.0037), tensor(0.0107)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0063), tensor(0.0024), tensor(0.0083)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0055), tensor(0.0026), tensor(0.0064)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0062), tensor(0.0029), tensor(0.0074)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0166), tensor(0.0053), tensor(0.0127)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0072), tensor(0.0030), tensor(0.0085)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0091), tensor(0.0036), tensor(0.0097)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0062), tensor(0.0038), tensor(0.0118)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0079), tensor(0.0038), tensor(0.0133)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0092), tensor(0.0034), tensor(0.0090)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0076), tensor(0.0041), tensor(0.0139)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0078), tensor(0.0029), tensor(0.0085)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0086), tensor(0.0041), tensor(0.0103)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0061), tensor(0.0028), tensor(0.0091)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0063), tensor(0.0023), tensor(0.0070)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0065), tensor(0.0030), tensor(0.0080)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0121), tensor(0.0043), tensor(0.0090)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0053), tensor(0.0013), tensor(0.0043)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0058), tensor(0.0026), tensor(0.0069)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0055), tensor(0.0023), tensor(0.0061)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0063), tensor(0.0030), tensor(0.0135)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0048), tensor(0.0021), tensor(0.0086)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0073), tensor(0.0035), tensor(0.0079)]\n",
            "Epoch [1/1], Step [100/468], Loss: 0.2237\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0058), tensor(0.0031), tensor(0.0102)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0075), tensor(0.0031), tensor(0.0098)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0064), tensor(0.0026), tensor(0.0101)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0069), tensor(0.0026), tensor(0.0078)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0079), tensor(0.0031), tensor(0.0090)]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-1705c379236f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m## Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-29e930ea2d70>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# flatten the output of conv2 to (batch_size, 32 * 7 * 7)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}