{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYmdRutcpdeR"
      },
      "source": [
        "# CryptoNet implementation and training on MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "prOXZ9RESeYD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import save_image\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUluBfyZuKld"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii4rDPKqVPYT"
      },
      "outputs": [],
      "source": [
        "## DUMMY MODEL\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()        \n",
        "        self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=1,              \n",
        "                out_channels=16,            \n",
        "                kernel_size=5,              \n",
        "                stride=1,                   \n",
        "                padding=2,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(kernel_size=2),    \n",
        "        )\n",
        "        self.conv2 = nn.Sequential(         \n",
        "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(2),                \n",
        "        )        # fully connected layer, output 10 classes\n",
        "        self.out = nn.Linear(32 * 7 * 7, 10)    \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "        x = x.view(x.size(0), -1)       \n",
        "        output = self.out(x)\n",
        "        return output  # return x for visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "bGKsu0dNuGEH"
      },
      "outputs": [],
      "source": [
        "class ScaledAvgPool2d(nn.Module):\n",
        "    \"\"\"Define the ScaledAvgPool layer, a.k.a the Sum Pool\"\"\"\n",
        "    def __init__(self,kernel_size):\n",
        "      super().__init__()\n",
        "      self.kernel_size = kernel_size\n",
        "      self.AvgPool = nn.AvgPool2d(kernel_size=self.kernel_size, stride=1, padding=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "      return (self.kernel_size**2)*self.AvgPool(x)\n",
        "    \n",
        "\n",
        "class CryptoNet(nn.Module):\n",
        "  '''\n",
        "    TO DO: check how in the paper the avg pool does not downscale the input size...weird padding?\n",
        "    EDIT: probably yes, it's a same convolution\n",
        "  '''\n",
        "  def __init__(self, verbose):\n",
        "    super().__init__()\n",
        "    self.verbose = verbose\n",
        "    self.pad = F.pad\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\n",
        "    self.square1 = torch.square\n",
        "    self.scaledAvgPool1 = ScaledAvgPool2d(kernel_size=3)\n",
        "    self.conv2 = nn.Conv2d(in_channels=5, out_channels=50, kernel_size=5, stride=2)\n",
        "    self.scaledAvgPool2 = ScaledAvgPool2d(kernel_size=3)\n",
        "    self.fc1 = nn.Linear(in_features=1250, out_features=100) # in paper in_features was 1250\n",
        "    self.square2 = torch.square\n",
        "    self.fc2 = nn.Linear(in_features=100, out_features=10)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pad(x, (1,0,1,0))\n",
        "    if self.verbose:\n",
        "      print(\"Start --> \",x.mean())\n",
        "    x = self.conv1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Conv1 --> \",x.mean())\n",
        "    x = self.square1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Sq --> \",x.mean())\n",
        "    x = self.scaledAvgPool1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Pool --> \",x.mean())\n",
        "    x = self.conv2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Conv2 --> \",x.mean())\n",
        "    x = self.scaledAvgPool2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Pool --> \",x.mean())\n",
        "    ## Flatten\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.fc1(x)\n",
        "    if self.verbose:\n",
        "      print(\"fc1 --> \",x.mean())\n",
        "    x = self.square2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Square --> \",x.mean())\n",
        "    x = self.fc2(x)\n",
        "    if self.verbose:\n",
        "      print(\"fc2 --> \",x.mean())\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "  def weights_init(self, m):\n",
        "    \"\"\" Custom initilization to avoid square activation to blow up \"\"\"\n",
        "    for m in self.children():\n",
        "      if isinstance(m,nn.Conv2d):\n",
        "        nn.init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        nn.init.uniform_(m.weight, 1e-4,1e-3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sX-7JDDtHOo"
      },
      "source": [
        "Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3zxoMQRRsF1o"
      },
      "outputs": [],
      "source": [
        "class DataHandler():\n",
        "  def __init__(self, dataset : str):\n",
        "    if dataset == \"MNIST\":\n",
        "      transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "      train_ds = MNIST(\"data/\", train=True, download=True, transform=transform)\n",
        "      test_ds = MNIST(\"data/\", train=False, download=True)\n",
        "\n",
        "      self.train_dl = DataLoader(train_ds, batch_size = 256, shuffle=True, drop_last=True)\n",
        "      self.test_dl = DataLoader(test_ds, batch_size = 256, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXXqZm508qA1"
      },
      "source": [
        "Plot gradient flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h3f19IYJ8nIQ"
      },
      "outputs": [],
      "source": [
        "def plot_grad_flow(named_parameters):\n",
        "    ## From https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063\n",
        "    ## Beware it's a little bit tricky to interpret results\n",
        "    '''Plots the gradients flowing through different layers in the net during training.\n",
        "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
        "    \n",
        "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
        "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
        "\n",
        "    ave_grads = []\n",
        "    max_grads = []\n",
        "    layers = []\n",
        "    for n, p in named_parameters:\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\n",
        "            layers.append(n)\n",
        "            ave_grads.append(p.grad.abs().mean())\n",
        "            max_grads.append(p.grad.abs().max())\n",
        "            print(f\"Layer {n}, grad avg {p.grad.mean()}, data {p.data.mean()}\")\n",
        "    plt.bar(np.arange(len(max_grads)), max(max_grads), alpha=0.1, lw=1, color=\"c\")\n",
        "    plt.bar(np.arange(len(max_grads)), np.mean(ave_grads), alpha=0.1, lw=1, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(left=0, right=len(ave_grads))\n",
        "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")\n",
        "    plt.grid(True)\n",
        "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
        "                Line2D([0], [0], color=\"b\", lw=4),\n",
        "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuwFtAqgtLYp"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xnkgcAJktEu9",
        "outputId": "1c34e54c-2f1e-414f-8455-1dd7a9be4912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start -->  tensor(0.0074)\n",
            "Conv1 -->  tensor(0.1108, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.6308, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.2587, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(8.6173, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(65.5936, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(45.7207, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(2315.8179, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(125.3483, grad_fn=<MeanBackward0>)\n",
            "[?] Step 1 Epoch 1\n",
            "Layer conv1.weight, grad avg 3.4531000281390334e-09, data 0.018138675019145012\n",
            "Layer conv2.weight, grad avg 2.1051262955040784e-09, data 0.0020927374716848135\n",
            "Layer fc1.weight, grad avg 4.2409784017927166e-10, data 0.0005501683335751295\n",
            "Layer fc2.weight, grad avg 2.9257774869506648e-08, data 0.0005407884600572288\n",
            "Start -->  tensor(-0.0198)\n",
            "Conv1 -->  tensor(0.0963, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4360, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.5160, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(7.7303, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(58.8068, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(17.7227, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(566.1976, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(16.9637, grad_fn=<MeanBackward0>)\n",
            "[?] Step 2 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.00027881088317371905, data 0.018087103962898254\n",
            "Layer conv2.weight, grad avg -0.0006877747364342213, data 0.002041497966274619\n",
            "Layer fc1.weight, grad avg -0.00021885489695705473, data 0.000542128924280405\n",
            "Layer fc2.weight, grad avg -0.086203433573246, data 0.0003512549155857414\n",
            "Start -->  tensor(0.0184)\n",
            "Conv1 -->  tensor(0.1181, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.6944, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.8264, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(8.9861, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(68.3054, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(20.5085, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(6437.5942, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(169.9697, grad_fn=<MeanBackward0>)\n",
            "[?] Step 3 Epoch 1\n",
            "Layer conv1.weight, grad avg -0.0033710524439811707, data 0.01808532327413559\n",
            "Layer conv2.weight, grad avg -0.0031938531901687384, data 0.0021857903338968754\n",
            "Layer fc1.weight, grad avg -0.0006205388344824314, data 0.0005464772693812847\n",
            "Layer fc2.weight, grad avg 0.9134727120399475, data 0.00026552900089882314\n",
            "Start -->  tensor(0.0050)\n",
            "Conv1 -->  tensor(0.1117, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5857, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.8313, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(9.3881, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(71.2683, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(28.1356, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(13929.0391, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(260.1442, grad_fn=<MeanBackward0>)\n",
            "[?] Step 4 Epoch 1\n",
            "Layer conv1.weight, grad avg -1.7186828245030483e-06, data 0.018182383850216866\n",
            "Layer conv2.weight, grad avg 1.366430296911858e-05, data 0.0023858968634158373\n",
            "Layer fc1.weight, grad avg 3.6607698348234408e-06, data 0.0005496074445545673\n",
            "Layer fc2.weight, grad avg 0.015401128679513931, data 0.00021105514315422624\n",
            "Start -->  tensor(0.0050)\n",
            "Conv1 -->  tensor(0.1117, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.6136, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.0738, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(10.1723, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(76.8906, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(35.8202, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(28127.8379, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(380.4500, grad_fn=<MeanBackward0>)\n",
            "[?] Step 5 Epoch 1\n",
            "Layer conv1.weight, grad avg -2.415899132354997e-33, data 0.018261177465319633\n",
            "Layer conv2.weight, grad avg -3.3372472882611866e-34, data 0.002548972610384226\n",
            "Layer fc1.weight, grad avg -5.892823205237948e-35, data 0.0005522181163541973\n",
            "Layer fc2.weight, grad avg 1.3901345646567287e-31, data 0.0001615817309357226\n",
            "Start -->  tensor(-0.0117)\n",
            "Conv1 -->  tensor(0.1032, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4714, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.8110, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(10.2627, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(77.4752, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(41.5798, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(41804.7500, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(396.3125, grad_fn=<MeanBackward0>)\n",
            "[?] Step 6 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.018327778205275536\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0026868078857660294\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005544247687794268\n",
            "Layer fc2.weight, grad avg 0.0, data 0.00011976547830272466\n",
            "Start -->  tensor(0.0025)\n",
            "Conv1 -->  tensor(0.1103, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5760, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.7231, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(10.9974, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(83.0893, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(46.9200, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(61507.8398, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(365.7927, grad_fn=<MeanBackward0>)\n",
            "[?] Step 7 Epoch 1\n",
            "Layer conv1.weight, grad avg 5.1134115892637055e-06, data 0.01838517189025879\n",
            "Layer conv2.weight, grad avg 4.626624649972655e-05, data 0.0028056048322468996\n",
            "Layer fc1.weight, grad avg 1.199180132971378e-05, data 0.0005563265876844525\n",
            "Layer fc2.weight, grad avg 0.02126125618815422, data 8.372530282940716e-05\n",
            "Start -->  tensor(0.0021)\n",
            "Conv1 -->  tensor(0.1088, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5644, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.6409, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(11.2981, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(85.3525, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(50.2110, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(80070.0312, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(129.0670, grad_fn=<MeanBackward0>)\n",
            "[?] Step 8 Epoch 1\n",
            "Layer conv1.weight, grad avg -0.0007976042106747627, data 0.018435630947351456\n",
            "Layer conv2.weight, grad avg -0.00148665567394346, data 0.0029068386647850275\n",
            "Layer fc1.weight, grad avg -0.0003895350673701614, data 0.0005579829448834062\n",
            "Layer fc2.weight, grad avg 0.3244117796421051, data 4.118180731893517e-05\n",
            "Start -->  tensor(0.0100)\n",
            "Conv1 -->  tensor(0.1158, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.6261, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.1873, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(12.1299, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(91.1805, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(61.0380, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(101448.4922, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-343.5036, grad_fn=<MeanBackward0>)\n",
            "[?] Step 9 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.018495876342058182\n",
            "Layer conv2.weight, grad avg 0.0, data 0.003049981314688921\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005621258751489222\n",
            "Layer fc2.weight, grad avg 0.0, data -8.65803758642869e-06\n",
            "Start -->  tensor(-0.0170)\n",
            "Conv1 -->  tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4657, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.7600, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(11.7786, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(88.8324, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(65.7864, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(108252.2031, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-848.3937, grad_fn=<MeanBackward0>)\n",
            "[?] Step 10 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.0185493603348732\n",
            "Layer conv2.weight, grad avg 0.0, data 0.003177054226398468\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005658035515807569\n",
            "Layer fc2.weight, grad avg 0.0, data -5.2902643801644444e-05\n",
            "Start -->  tensor(-0.0069)\n",
            "Conv1 -->  tensor(0.1054, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5469, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.4604, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(12.6998, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(95.9041, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(78.0891, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(133601.7969, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-1564.8336, grad_fn=<MeanBackward0>)\n",
            "[?] Step 11 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.018597092479467392\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0032904634717851877\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005690859397873282\n",
            "Layer fc2.weight, grad avg 0.0, data -9.23899351619184e-05\n",
            "Start -->  tensor(0.0045)\n",
            "Conv1 -->  tensor(0.1115, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5970, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.9399, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(13.2572, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(99.4955, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(85.5228, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(152464.9062, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-2311.8535, grad_fn=<MeanBackward0>)\n",
            "[?] Step 12 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01863986998796463\n",
            "Layer conv2.weight, grad avg 0.0, data 0.003392099402844906\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005720274639315903\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00012777792289853096\n",
            "Start -->  tensor(-0.0058)\n",
            "Conv1 -->  tensor(0.1070, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5086, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.1530, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(13.0730, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(98.2137, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(87.5418, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(160811.9219, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-2937.2454, grad_fn=<MeanBackward0>)\n",
            "[?] Step 13 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.0186783317476511\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0034834835678339005\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005746723036281765\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0001595965150045231\n",
            "Start -->  tensor(-0.0022)\n",
            "Conv1 -->  tensor(0.1087, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5360, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.3820, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(13.6467, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(102.6940, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(97.4747, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(188873.8594, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-3969.3574, grad_fn=<MeanBackward0>)\n",
            "[?] Step 14 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.018713004887104034\n",
            "Layer conv2.weight, grad avg 0.0, data 0.00356586673296988\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005770566640421748\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0001882810320239514\n",
            "Start -->  tensor(0.0112)\n",
            "Conv1 -->  tensor(0.1163, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.6410, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.3412, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(14.4862, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(108.7166, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(104.3212, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(212726.2188, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-5006.8984, grad_fn=<MeanBackward0>)\n",
            "[?] Step 15 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01874432899057865\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0036402936093509197\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005792106967419386\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00021419512631837279\n",
            "Start -->  tensor(-0.0009)\n",
            "Conv1 -->  tensor(0.1089, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5599, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.5804, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(14.3539, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(107.8966, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(108.9994, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(220014.0469, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-5680.9692, grad_fn=<MeanBackward0>)\n",
            "[?] Step 16 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.018772678449749947\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0037076473236083984\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005811600713059306\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00023764681827742606\n",
            "Start -->  tensor(-0.0227)\n",
            "Conv1 -->  tensor(0.0962, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4255, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.4248, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(13.9332, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(104.7391, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(109.2566, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(213793.4844, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-5957.1353, grad_fn=<MeanBackward0>)\n",
            "[?] Step 17 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01879836991429329\n",
            "Layer conv2.weight, grad avg 0.0, data 0.003768687369301915\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005829266738146544\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00025889999233186245\n",
            "Start -->  tensor(-0.0146)\n",
            "Conv1 -->  tensor(0.1009, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4793, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.8915, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(14.2862, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(107.4871, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(112.8578, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(235028.0781, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-6978.2588, grad_fn=<MeanBackward0>)\n",
            "[?] Step 18 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.018821679055690765\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0038240686990320683\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005845295381732285\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0002781827934086323\n",
            "Start -->  tensor(0.0165)\n",
            "Conv1 -->  tensor(0.1199, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.6634, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.5413, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(15.6455, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(117.4342, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(125.0904, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(281526.2188, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-8849.5645, grad_fn=<MeanBackward0>)\n",
            "[?] Step 19 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.018842849880456924\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0038743631448596716\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005859852535650134\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00029569450998678803\n",
            "Start -->  tensor(-0.0115)\n",
            "Conv1 -->  tensor(0.1039, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4826, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.8968, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(14.6451, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(110.2223, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(120.1558, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(252254.1875, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-8323.7900, grad_fn=<MeanBackward0>)\n",
            "[?] Step 20 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.018862087279558182\n",
            "Layer conv2.weight, grad avg 0.0, data 0.003920072223991156\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005873080808669329\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00031160982325673103\n",
            "Start -->  tensor(-0.0093)\n",
            "Conv1 -->  tensor(0.1051, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4831, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.9213, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(14.9190, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(112.1417, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(124.8842, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(277602.2500, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-9536.5371, grad_fn=<MeanBackward0>)\n",
            "[?] Step 21 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01887958124279976\n",
            "Layer conv2.weight, grad avg 0.0, data 0.003961640875786543\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005885112332180142\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0003260834200773388\n",
            "Start -->  tensor(-0.0100)\n",
            "Conv1 -->  tensor(0.1053, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4861, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.9497, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(14.9360, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(112.5956, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(123.8507, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(279443.0625, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-9970.6846, grad_fn=<MeanBackward0>)\n",
            "[?] Step 22 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.018895503133535385\n",
            "Layer conv2.weight, grad avg 0.0, data 0.003999462816864252\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005896058864891529\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00033925246680155396\n",
            "Start -->  tensor(-0.0099)\n",
            "Conv1 -->  tensor(0.1055, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4863, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.9475, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(15.1449, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(113.7326, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(127.8104, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(289319.4062, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-10646.6328, grad_fn=<MeanBackward0>)\n",
            "[?] Step 23 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.018909992650151253\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004033889155834913\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005906023434363306\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00035123919951729476\n",
            "Start -->  tensor(0.0098)\n",
            "Conv1 -->  tensor(0.1163, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.6196, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.1448, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.0482, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(120.5235, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(138.0931, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(327675.0938, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-12406.4736, grad_fn=<MeanBackward0>)\n",
            "[?] Step 24 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.018923183903098106\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004065235145390034\n",
            "Layer fc1.weight, grad avg 0.0, data 0.000591509509831667\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0003621532814577222\n",
            "Start -->  tensor(0.0030)\n",
            "Conv1 -->  tensor(0.1120, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5662, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.6540, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(15.8510, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(118.8055, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(136.1636, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(319941.8125, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-12418.3613, grad_fn=<MeanBackward0>)\n",
            "[?] Step 25 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.018935201689600945\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004093782510608435\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005923357093706727\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00037209311267361045\n",
            "Start -->  tensor(-0.0075)\n",
            "Conv1 -->  tensor(0.1068, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5146, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.2204, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(15.6944, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(117.9818, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(138.0294, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(320874.1875, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-12730.4307, grad_fn=<MeanBackward0>)\n",
            "[?] Step 26 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.018946144729852676\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004119786433875561\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005930883344262838\n",
            "Layer fc2.weight, grad avg 0.0, data -0.000381147168809548\n",
            "Start -->  tensor(0.0028)\n",
            "Conv1 -->  tensor(0.1127, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5517, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.5341, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(15.9312, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(119.7120, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(141.1062, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(330181.2188, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-13358.6504, grad_fn=<MeanBackward0>)\n",
            "[?] Step 27 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01895611546933651\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004143476486206055\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005937739624641836\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00038939554360695183\n",
            "Start -->  tensor(-0.0010)\n",
            "Conv1 -->  tensor(0.1110, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5687, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.6988, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.0232, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(119.8506, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(141.3850, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(346979.2812, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-14276.1299, grad_fn=<MeanBackward0>)\n",
            "[?] Step 28 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01896520145237446\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004165059421211481\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005943985306657851\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00039691050187684596\n",
            "Start -->  tensor(-0.0036)\n",
            "Conv1 -->  tensor(0.1084, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5205, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.2614, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(15.9264, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(119.9258, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(141.6807, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(337593.9062, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-14142.2861, grad_fn=<MeanBackward0>)\n",
            "[?] Step 29 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01897347718477249\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004184724297374487\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005949678015895188\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00040375743992626667\n",
            "Start -->  tensor(-0.0151)\n",
            "Conv1 -->  tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4733, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.8620, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(15.3985, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(115.1400, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(133.5195, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(324002.4688, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-13768.3516, grad_fn=<MeanBackward0>)\n",
            "[?] Step 30 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.018981022760272026\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0042026410810649395\n",
            "Layer fc1.weight, grad avg 0.0, data 0.000595486315432936\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004099960206076503\n",
            "Start -->  tensor(-0.0131)\n",
            "Conv1 -->  tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4893, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.9851, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(15.6907, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(117.3883, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(138.2330, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(331479.6875, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-14254.2910, grad_fn=<MeanBackward0>)\n",
            "[?] Step 31 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01898789033293724\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004218965768814087\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005959587870165706\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004156799695920199\n",
            "Start -->  tensor(-0.0065)\n",
            "Conv1 -->  tensor(0.1073, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5037, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.1196, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.1654, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(121.8458, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(147.2687, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(365506.5938, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-15897.9268, grad_fn=<MeanBackward0>)\n",
            "[?] Step 32 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.018994148820638657\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004233838524669409\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005963892908766866\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00042085847235284746\n",
            "Start -->  tensor(0.0177)\n",
            "Conv1 -->  tensor(0.1224, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.7045, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.9176, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(17.3746, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(130.3089, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(156.3890, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(410733.1250, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-18050.2461, grad_fn=<MeanBackward0>)\n",
            "[?] Step 33 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.018999850377440453\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004247387871146202\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005967813776805997\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00042557623237371445\n",
            "Start -->  tensor(0.0002)\n",
            "Conv1 -->  tensor(0.1108, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5726, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.7450, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.5286, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(123.3767, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(146.9002, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(376123.4688, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-16672.8398, grad_fn=<MeanBackward0>)\n",
            "[?] Step 34 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019005047157406807\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004259730689227581\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005971386563032866\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00042987376218661666\n",
            "Start -->  tensor(-0.0062)\n",
            "Conv1 -->  tensor(0.1062, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5274, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.3175, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.3434, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(122.7184, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(151.2695, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(367075.4375, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-16425.8457, grad_fn=<MeanBackward0>)\n",
            "[?] Step 35 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019009776413440704\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004270973149687052\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005974640371277928\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00043378822738304734\n",
            "Start -->  tensor(0.0180)\n",
            "Conv1 -->  tensor(0.1213, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.6432, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.3460, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(17.1102, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(128.0954, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(155.1275, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(402983.5938, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-18164.5508, grad_fn=<MeanBackward0>)\n",
            "[?] Step 36 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019014088436961174\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004281212575733662\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005977603723295033\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004373534466139972\n",
            "Start -->  tensor(0.0005)\n",
            "Conv1 -->  tensor(0.1108, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5709, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.6976, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.5224, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(123.7906, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(149.9132, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(377044.4062, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-17128.2148, grad_fn=<MeanBackward0>)\n",
            "[?] Step 37 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019018013030290604\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004290537443011999\n",
            "Layer fc1.weight, grad avg 0.0, data 0.000598030281253159\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004406000371091068\n",
            "Start -->  tensor(0.0251)\n",
            "Conv1 -->  tensor(0.1247, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.6959, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.8143, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(17.6276, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(132.2132, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(161.9282, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(427334.5312, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-19515.7148, grad_fn=<MeanBackward0>)\n",
            "[?] Step 38 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019021591171622276\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004299027845263481\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005982759757898748\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00044355628779157996\n",
            "Start -->  tensor(-0.0003)\n",
            "Conv1 -->  tensor(0.1111, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5706, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.6896, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.5439, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(123.8442, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(149.4123, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(372986.8125, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-17142.8379, grad_fn=<MeanBackward0>)\n",
            "[?] Step 39 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019024839624762535\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004306757356971502\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005984996678307652\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00044624769361689687\n",
            "Start -->  tensor(0.0051)\n",
            "Conv1 -->  tensor(0.1144, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.6062, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.0319, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.7230, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(124.5604, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(150.8438, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(393532.0312, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-18169.4102, grad_fn=<MeanBackward0>)\n",
            "[?] Step 40 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019027799367904663\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0043137939646840096\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005987033946439624\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004486977122724056\n",
            "Start -->  tensor(-0.0037)\n",
            "Conv1 -->  tensor(0.1084, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5251, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.3193, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.6126, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(124.9575, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(154.7879, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(388852.8750, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-18027.9102, grad_fn=<MeanBackward0>)\n",
            "[?] Step 41 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01903049647808075\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004320198204368353\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005988887860439718\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004509277059696615\n",
            "Start -->  tensor(0.0098)\n",
            "Conv1 -->  tensor(0.1166, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5930, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.8927, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.9441, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(127.1534, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(157.6160, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(405688.6250, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-18894.7969, grad_fn=<MeanBackward0>)\n",
            "[?] Step 42 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019032945856451988\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0043260264210402966\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005990574136376381\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004529570578597486\n",
            "Start -->  tensor(-0.0144)\n",
            "Conv1 -->  tensor(0.1029, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4720, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.8579, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.2871, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(122.3289, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(151.8857, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(392231.2812, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-18334.8535, grad_fn=<MeanBackward0>)\n",
            "[?] Step 43 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019035179167985916\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004331330303102732\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005992109654471278\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00045480355038307607\n",
            "Start -->  tensor(0.0138)\n",
            "Conv1 -->  tensor(0.1194, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.6944, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.7826, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(17.3039, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(129.3721, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(157.9653, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(412010.3750, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-19355.6387, grad_fn=<MeanBackward0>)\n",
            "[?] Step 44 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019037215039134026\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004336155019700527\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005993506056256592\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004564835107885301\n",
            "Start -->  tensor(0.0053)\n",
            "Conv1 -->  tensor(0.1122, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5946, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.8960, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.8027, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(125.6409, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(154.6283, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(393020.3750, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-18518.3262, grad_fn=<MeanBackward0>)\n",
            "[?] Step 45 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019039059057831764\n",
            "Layer conv2.weight, grad avg 0.0, data 0.00434054434299469\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005994776147417724\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00045801163651049137\n",
            "Start -->  tensor(-0.0064)\n",
            "Conv1 -->  tensor(0.1077, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5245, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.3062, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.5051, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(123.6461, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(151.1326, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(377801.1562, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-17842.8008, grad_fn=<MeanBackward0>)\n",
            "[?] Step 46 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01904073916375637\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004344535060226917\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005995931569486856\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004594014899339527\n",
            "Start -->  tensor(0.0339)\n",
            "Conv1 -->  tensor(0.1314, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.8166, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(24.9034, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(18.1383, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(135.2601, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(165.0156, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(463885.3750, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-21965.0645, grad_fn=<MeanBackward0>)\n",
            "[?] Step 47 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01904226839542389\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004348165355622768\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005996981053613126\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00046066538197919726\n",
            "Start -->  tensor(0.0130)\n",
            "Conv1 -->  tensor(0.1183, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.6402, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.3078, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(17.3039, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(129.5081, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(160.0981, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(428523.1250, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-20334.6367, grad_fn=<MeanBackward0>)\n",
            "[?] Step 48 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019043656066060066\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004351466428488493\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005997936823405325\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00046181457582861185\n",
            "Start -->  tensor(-0.0176)\n",
            "Conv1 -->  tensor(0.1010, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4420, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.5513, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.0753, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(120.7258, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(149.9839, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(368871.4688, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-17543.7539, grad_fn=<MeanBackward0>)\n",
            "[?] Step 49 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019044918939471245\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004354466684162617\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005998804699629545\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00046285922871902585\n",
            "Start -->  tensor(0.0038)\n",
            "Conv1 -->  tensor(0.1142, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5887, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.8671, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.9721, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(127.1467, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(157.0774, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(404377.9688, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-19269.0547, grad_fn=<MeanBackward0>)\n",
            "[?] Step 50 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01904606819152832\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004357193596661091\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0005999594577588141\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00046380888670682907\n",
            "[!] Epoch [1/5000], Step [50/468], Loss: 2.5367\n",
            "Start -->  tensor(-0.0204)\n",
            "Conv1 -->  tensor(0.0996, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4365, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.5325, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(15.9279, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(119.5664, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(148.0256, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(371882.8438, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-17756.3438, grad_fn=<MeanBackward0>)\n",
            "[?] Step 51 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01904711127281189\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004359672777354717\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0006000312278047204\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004646719607990235\n",
            "Start -->  tensor(-0.0030)\n",
            "Conv1 -->  tensor(0.1090, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5333, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.3514, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.5276, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(123.4687, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(154.1394, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(375936.5938, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-17981.4727, grad_fn=<MeanBackward0>)\n",
            "[?] Step 52 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019048063084483147\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004361925181001425\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0006000964203849435\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00046545625082217157\n",
            "Start -->  tensor(0.0011)\n",
            "Conv1 -->  tensor(0.1111, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5668, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.6815, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.8571, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(126.6921, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(157.2147, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(400353.8125, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-19173.4023, grad_fn=<MeanBackward0>)\n",
            "[?] Step 53 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019048919901251793\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004363971762359142\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0006001555593684316\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00046616888721473515\n",
            "Start -->  tensor(-0.0207)\n",
            "Conv1 -->  tensor(0.0994, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.3939, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.1576, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(15.8684, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(119.4516, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(148.9034, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(369672.4375, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-17718.9902, grad_fn=<MeanBackward0>)\n",
            "[?] Step 54 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01904970593750477\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004365831147879362\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0006002094596624374\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00046681618550792336\n",
            "Start -->  tensor(-0.0204)\n",
            "Conv1 -->  tensor(0.0990, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4251, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.3920, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(15.9799, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(120.1755, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(150.2455, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(375406.4062, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-18019.1367, grad_fn=<MeanBackward0>)\n",
            "[?] Step 55 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019050419330596924\n",
            "Layer conv2.weight, grad avg 0.0, data 0.00436751963570714\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0006002583540976048\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00046740422840230167\n",
            "Start -->  tensor(0.0119)\n",
            "Conv1 -->  tensor(0.1178, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.6228, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.2020, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(17.3135, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(129.7738, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(158.7330, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(426129.7500, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-20463.4121, grad_fn=<MeanBackward0>)\n",
            "[?] Step 56 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019051063805818558\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004369053523987532\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0006003027083352208\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004679382254835218\n",
            "Start -->  tensor(-0.0054)\n",
            "Conv1 -->  tensor(0.1086, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5432, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.4429, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.7332, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(125.2225, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(157.0136, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(404648.5938, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-19457.4258, grad_fn=<MeanBackward0>)\n",
            "[?] Step 57 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019051648676395416\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004370446316897869\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0006003430462442338\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00046842321171425283\n",
            "Start -->  tensor(0.0235)\n",
            "Conv1 -->  tensor(0.1247, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.7225, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(24.0437, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(18.0051, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(134.9377, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(167.6361, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(460970.1250, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-22175.4414, grad_fn=<MeanBackward0>)\n",
            "[?] Step 58 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019052181392908096\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004371711518615484\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0006003796588629484\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00046886358177289367\n",
            "Start -->  tensor(-0.0101)\n",
            "Conv1 -->  tensor(0.1058, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4911, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.9908, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.2776, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(121.9590, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(152.2675, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(387414.8125, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-18671.2676, grad_fn=<MeanBackward0>)\n",
            "[?] Step 59 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019052663818001747\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0043728589080274105\n",
            "Layer fc1.weight, grad avg 0.0, data 0.00060041289543733\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004692633228842169\n",
            "Start -->  tensor(0.0161)\n",
            "Conv1 -->  tensor(0.1203, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.6605, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.5130, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(17.4172, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(130.3207, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(161.2898, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(433460.2812, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-20893.6211, grad_fn=<MeanBackward0>)\n",
            "[?] Step 60 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019053105264902115\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004373901057988405\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0006004430470056832\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00046962618944235146\n",
            "Start -->  tensor(-0.0032)\n",
            "Conv1 -->  tensor(0.1099, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5486, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.5220, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.8427, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(126.2999, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(157.3241, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(411742.1562, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-19871.0391, grad_fn=<MeanBackward0>)\n",
            "[?] Step 61 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01905350387096405\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0043748472817242146\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0006004704046063125\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00046995561569929123\n",
            "Start -->  tensor(0.0059)\n",
            "Conv1 -->  tensor(0.1159, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.6041, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.0008, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(17.0933, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(128.1395, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(160.5204, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(415165.9062, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-20035.7715, grad_fn=<MeanBackward0>)\n",
            "[?] Step 62 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019053863361477852\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004375705495476723\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0006004952592775226\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00047025454114191234\n",
            "Start -->  tensor(-0.0003)\n",
            "Conv1 -->  tensor(0.1104, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5666, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.6559, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.7382, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(125.2733, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(154.4131, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(394870.1562, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-19092.6758, grad_fn=<MeanBackward0>)\n",
            "[?] Step 63 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019054191187024117\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004376484081149101\n",
            "Layer fc1.weight, grad avg 0.0, data 0.000600517843849957\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00047052581794559956\n",
            "Start -->  tensor(-0.0035)\n",
            "Conv1 -->  tensor(0.1088, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5082, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.1124, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.6200, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(125.3162, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(157.9640, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(400087.1875, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-19346.6133, grad_fn=<MeanBackward0>)\n",
            "[?] Step 64 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019054489210247993\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004377191886305809\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0006005382747389376\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004707719781436026\n",
            "Start -->  tensor(0.0110)\n",
            "Conv1 -->  tensor(0.1172, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.6111, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(23.0800, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(17.2313, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(129.0405, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(162.9247, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(427335.0938, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-20649.1055, grad_fn=<MeanBackward0>)\n",
            "[?] Step 65 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019054755568504333\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0043778326362371445\n",
            "Layer fc1.weight, grad avg 0.0, data 0.000600556842982769\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004709951754193753\n",
            "Start -->  tensor(-0.0046)\n",
            "Conv1 -->  tensor(0.1072, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5306, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.3415, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.8154, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(126.5258, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(157.7421, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(396192.1250, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-19158.3047, grad_fn=<MeanBackward0>)\n",
            "[?] Step 66 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01905500330030918\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004378414247184992\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0006005736067891121\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004711977671831846\n",
            "Start -->  tensor(-0.0037)\n",
            "Conv1 -->  tensor(0.1097, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.5569, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.5884, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.8651, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(126.1658, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(156.2327, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(407937.9062, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-19734.0469, grad_fn=<MeanBackward0>)\n",
            "[?] Step 67 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.01905522122979164\n",
            "Layer conv2.weight, grad avg 0.0, data 0.0043789418414235115\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0006005889154039323\n",
            "Layer fc2.weight, grad avg 0.0, data -0.00047138152876868844\n",
            "Start -->  tensor(-0.0142)\n",
            "Conv1 -->  tensor(0.1016, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4601, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(21.7146, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.1189, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(120.7710, grad_fn=<MeanBackward0>)\n",
            "fc1 -->  tensor(150.5174, grad_fn=<MeanBackward0>)\n",
            "Square -->  tensor(368127.8438, grad_fn=<MeanBackward0>)\n",
            "fc2 -->  tensor(-17814.5195, grad_fn=<MeanBackward0>)\n",
            "[?] Step 68 Epoch 1\n",
            "Layer conv1.weight, grad avg 0.0, data 0.019055426120758057\n",
            "Layer conv2.weight, grad avg 0.0, data 0.004379421006888151\n",
            "Layer fc1.weight, grad avg 0.0, data 0.0006006027688272297\n",
            "Layer fc2.weight, grad avg 0.0, data -0.0004715481190942228\n",
            "Start -->  tensor(-0.0100)\n",
            "Conv1 -->  tensor(0.1044, grad_fn=<MeanBackward0>)\n",
            "Sq -->  tensor(2.4993, grad_fn=<MeanBackward0>)\n",
            "Pool -->  tensor(22.0624, grad_fn=<MeanBackward0>)\n",
            "Conv2 -->  tensor(16.4166, grad_fn=<MeanBackward0>)\n",
            "Pool -->  "
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-ad06fa285d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-24dd42bc2918>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaledAvgPool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pool --> \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;31m## Flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# All strings are unicode in Python 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_str_intern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str_intern\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m    388\u001b[0m                     \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                     \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_formatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimag_formatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mtensor_view\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "## setup torch enviro\n",
        "torch.manual_seed(9325345339582034)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "## init model\n",
        "model = CryptoNet(verbose=True)\n",
        "model.apply(model.weights_init)\n",
        "model = model.to(device=device)\n",
        "\n",
        "dataHandler = DataHandler(\"MNIST\")\n",
        "\n",
        "## training params setup\n",
        "learning_rate = 3e-4\n",
        "momentum = 0.9\n",
        "num_epochs = 5000\n",
        "total_step = len(dataHandler.train_dl)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (data, labels) in enumerate(dataHandler.train_dl):\n",
        "    data = data.to(device=device)\n",
        "    labels = labels.to(device=device)\n",
        "    #labels = labels.to(torch.float32)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(data)\n",
        "    loss = criterion(predictions, labels)\n",
        "    loss.backward()\n",
        "    if model.verbose:\n",
        "      print(f\"[?] Step {i+1} Epoch {epoch+1}\")\n",
        "      plot_grad_flow(model.named_parameters())\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 50 == 0:\n",
        "      print ('[!] Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "\n",
        "torch.save(model, \"cryptoNet.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analysis on this output: I don't understand how come, even if the mean of the input tensor\n",
        "of the sigmoid is 125, this time the avg grad. is not zero...then it does not depend on the gradient of the sigmoid going to 0 for large outputs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dummy Testing, feel free to ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz5pM3GavjgC"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uD7LT4QNvkaJ",
        "outputId": "5d46c1a6-e6cd-46f1-cd85-f4ac5cc39dd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0055), tensor(0.0038), tensor(0.0065)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0054), tensor(0.0037), tensor(0.0060)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0063), tensor(0.0039), tensor(0.0060)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0080), tensor(0.0043), tensor(0.0066)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0090), tensor(0.0047), tensor(0.0065)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0138), tensor(0.0054), tensor(0.0082)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0172), tensor(0.0060), tensor(0.0076)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0229), tensor(0.0062), tensor(0.0079)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0239), tensor(0.0062), tensor(0.0082)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0258), tensor(0.0066), tensor(0.0108)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0271), tensor(0.0063), tensor(0.0107)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0260), tensor(0.0069), tensor(0.0166)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0299), tensor(0.0067), tensor(0.0149)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0284), tensor(0.0062), tensor(0.0144)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0300), tensor(0.0063), tensor(0.0134)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0291), tensor(0.0064), tensor(0.0156)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0251), tensor(0.0047), tensor(0.0108)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0219), tensor(0.0046), tensor(0.0144)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0174), tensor(0.0043), tensor(0.0142)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0199), tensor(0.0052), tensor(0.0150)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0149), tensor(0.0039), tensor(0.0147)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0131), tensor(0.0042), tensor(0.0168)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0168), tensor(0.0050), tensor(0.0189)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0075), tensor(0.0043), tensor(0.0186)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0152), tensor(0.0045), tensor(0.0180)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0098), tensor(0.0040), tensor(0.0157)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0139), tensor(0.0059), tensor(0.0222)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0093), tensor(0.0049), tensor(0.0220)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0104), tensor(0.0035), tensor(0.0146)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0103), tensor(0.0042), tensor(0.0175)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0099), tensor(0.0044), tensor(0.0201)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0087), tensor(0.0041), tensor(0.0198)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0127), tensor(0.0046), tensor(0.0162)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0069), tensor(0.0035), tensor(0.0152)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0089), tensor(0.0040), tensor(0.0146)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0079), tensor(0.0042), tensor(0.0170)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0148), tensor(0.0053), tensor(0.0198)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0123), tensor(0.0055), tensor(0.0160)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0090), tensor(0.0039), tensor(0.0131)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0069), tensor(0.0036), tensor(0.0165)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0067), tensor(0.0031), tensor(0.0126)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0058), tensor(0.0029), tensor(0.0116)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0077), tensor(0.0027), tensor(0.0100)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0119), tensor(0.0056), tensor(0.0176)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0066), tensor(0.0031), tensor(0.0115)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0088), tensor(0.0027), tensor(0.0112)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0079), tensor(0.0033), tensor(0.0146)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0066), tensor(0.0033), tensor(0.0128)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0084), tensor(0.0034), tensor(0.0149)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0079), tensor(0.0033), tensor(0.0143)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0090), tensor(0.0035), tensor(0.0122)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0119), tensor(0.0043), tensor(0.0140)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0054), tensor(0.0028), tensor(0.0089)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0093), tensor(0.0039), tensor(0.0161)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0150), tensor(0.0074), tensor(0.0212)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0098), tensor(0.0049), tensor(0.0193)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0084), tensor(0.0033), tensor(0.0100)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0108), tensor(0.0032), tensor(0.0142)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0087), tensor(0.0029), tensor(0.0090)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0060), tensor(0.0028), tensor(0.0110)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0083), tensor(0.0034), tensor(0.0122)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0128), tensor(0.0060), tensor(0.0157)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0123), tensor(0.0052), tensor(0.0157)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0078), tensor(0.0038), tensor(0.0143)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0070), tensor(0.0031), tensor(0.0109)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0059), tensor(0.0044), tensor(0.0180)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0053), tensor(0.0029), tensor(0.0110)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0067), tensor(0.0029), tensor(0.0080)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0142), tensor(0.0063), tensor(0.0156)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0078), tensor(0.0038), tensor(0.0118)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0112), tensor(0.0044), tensor(0.0137)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0083), tensor(0.0033), tensor(0.0093)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0136), tensor(0.0050), tensor(0.0124)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0133), tensor(0.0050), tensor(0.0132)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0066), tensor(0.0025), tensor(0.0081)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0080), tensor(0.0036), tensor(0.0139)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0103), tensor(0.0044), tensor(0.0142)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0085), tensor(0.0037), tensor(0.0107)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0063), tensor(0.0024), tensor(0.0083)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0055), tensor(0.0026), tensor(0.0064)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0062), tensor(0.0029), tensor(0.0074)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0166), tensor(0.0053), tensor(0.0127)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0072), tensor(0.0030), tensor(0.0085)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0091), tensor(0.0036), tensor(0.0097)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0062), tensor(0.0038), tensor(0.0118)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0079), tensor(0.0038), tensor(0.0133)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0092), tensor(0.0034), tensor(0.0090)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0076), tensor(0.0041), tensor(0.0139)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0078), tensor(0.0029), tensor(0.0085)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0086), tensor(0.0041), tensor(0.0103)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0061), tensor(0.0028), tensor(0.0091)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0063), tensor(0.0023), tensor(0.0070)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0065), tensor(0.0030), tensor(0.0080)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0121), tensor(0.0043), tensor(0.0090)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0053), tensor(0.0013), tensor(0.0043)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0058), tensor(0.0026), tensor(0.0069)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0055), tensor(0.0023), tensor(0.0061)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0063), tensor(0.0030), tensor(0.0135)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0048), tensor(0.0021), tensor(0.0086)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0073), tensor(0.0035), tensor(0.0079)]\n",
            "Epoch [1/1], Step [100/468], Loss: 0.2237\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0058), tensor(0.0031), tensor(0.0102)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0075), tensor(0.0031), tensor(0.0098)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0064), tensor(0.0026), tensor(0.0101)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0069), tensor(0.0026), tensor(0.0078)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0079), tensor(0.0031), tensor(0.0090)]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-1705c379236f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m## Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-29e930ea2d70>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# flatten the output of conv2 to (batch_size, 32 * 7 * 7)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = CNN()\n",
        "\n",
        "def grad_clipping(model):\n",
        "  for p in model.parameters():\n",
        "    p.register_hook(lambda grad: print(grad))\n",
        "    p.register_hook(lambda grad: torch.clamp(grad, 0, 1.0))\n",
        "\n",
        "    p.register_hook(lambda grad: print(f\"{p} -> {grad}\"))\n",
        "\n",
        "#grad_clipping(model)\n",
        "model = model.to(device=device)\n",
        "dataHandler = DataHandler(\"MNIST\")\n",
        "\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9\n",
        "num_epochs = 1\n",
        "total_step = len(dataHandler.train_dl)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (data, labels) in enumerate(dataHandler.train_dl):\n",
        "    data = data.to(device=device)\n",
        "    labels = labels.to(device=device)\n",
        "    #labels = labels.to(torch.float32)\n",
        "\n",
        "    ## Forward\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(data)\n",
        "    loss = criterion(predictions, labels)\n",
        "    loss.backward()\n",
        "    plot_grad_flow(model.named_parameters())\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "      print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "torch.save(model, \"cryptoNet.pt\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "crypto_nets.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
