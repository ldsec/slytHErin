{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crypto_nets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CryptoNet implementation and training on MNIST dataset"
      ],
      "metadata": {
        "id": "HYmdRutcpdeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "prOXZ9RESeYD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import save_image\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "DUluBfyZuKld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_grad_flow(named_parameters):\n",
        "    '''Plots the gradients flowing through different layers in the net during training.\n",
        "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
        "    \n",
        "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
        "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
        "    ave_grads = []\n",
        "    max_grads= []\n",
        "    layers = []\n",
        "    for n, p in named_parameters:\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\n",
        "            layers.append(n)\n",
        "            ave_grads.append(p.grad.abs().mean())\n",
        "            max_grads.append(p.grad.abs().max())\n",
        "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
        "    #plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(left=0, right=len(ave_grads))\n",
        "    plt.ylim(bottom = -0.1, top=0.1) # zoom in on the lower gradient regions\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")\n",
        "    plt.grid(True)\n",
        "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
        "                Line2D([0], [0], color=\"b\", lw=4),\n",
        "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
        "    print(layers)\n",
        "    print(ave_grads)"
      ],
      "metadata": {
        "id": "fqM585LsKVQh"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DUMMY MODEL\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()        \n",
        "        self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=1,              \n",
        "                out_channels=16,            \n",
        "                kernel_size=5,              \n",
        "                stride=1,                   \n",
        "                padding=2,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(kernel_size=2),    \n",
        "        )\n",
        "        self.conv2 = nn.Sequential(         \n",
        "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(2),                \n",
        "        )        # fully connected layer, output 10 classes\n",
        "        self.out = nn.Linear(32 * 7 * 7, 10)    \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "        x = x.view(x.size(0), -1)       \n",
        "        output = self.out(x)\n",
        "        return output  # return x for visualization"
      ],
      "metadata": {
        "id": "Ii4rDPKqVPYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledAvgPool2d(nn.Module):\n",
        "    def __init__(self,kernel_size):\n",
        "      super().__init__()\n",
        "      self.kernel_size = kernel_size\n",
        "      self.AvgPool = nn.AvgPool2d(kernel_size=self.kernel_size, stride=1, padding=1)\n",
        "    \n",
        "    def forward(self,x):\n",
        "      return (self.kernel_size**2)*self.AvgPool(x)\n",
        "\n",
        "class CryptoNet(nn.Module):\n",
        "  '''\n",
        "    TO DO: check how in the paper the avg pool does not downscale the input size...weird padding?\n",
        "  '''\n",
        "  def __init__(self, verbose):\n",
        "    super().__init__()\n",
        "    self.verbose = verbose\n",
        "    self.pad = F.pad\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\n",
        "    self.square1 = torch.square\n",
        "    self.scaledAvgPool1 = ScaledAvgPool2d(kernel_size=3)\n",
        "    self.conv2 = nn.Conv2d(in_channels=5, out_channels=50, kernel_size=5, stride=2)\n",
        "    self.scaledAvgPool2 = ScaledAvgPool2d(kernel_size=3)\n",
        "    self.fc1 = nn.Linear(in_features=1250, out_features=100) # in paper in_features was 1250\n",
        "    self.square2 = torch.square\n",
        "    self.fc2 = nn.Linear(in_features=100, out_features=10)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    #self.ReLU = torch.nn.ReLU() # testing on vanishing\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pad(x, (1,0,1,0))\n",
        "    x = self.conv1(x)\n",
        "    x = self.square1(x)\n",
        "    x = self.scaledAvgPool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.scaledAvgPool2(x)\n",
        "    ## Flatten\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.square2(x)\n",
        "    x = self.fc2(x)\n",
        "    if self.verbose:\n",
        "      print(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "  def weights_init(self):\n",
        "    linear = 1\n",
        "    for l in self.children():\n",
        "      if isinstance(l, nn.Conv2d):\n",
        "        n = l.in_channels\n",
        "        for k in l.kernel_size:\n",
        "          n *= k\n",
        "        stdv = 3/ math.sqrt(n)\n",
        "        l.weight.data.uniform_(-1e-8, 1e-8)\n",
        "        if l.bias is not None:\n",
        "           l.weight.data.uniform_(-1e-8, 1e-8)\n",
        "\n",
        "      elif isinstance(l, nn.Linear):\n",
        "        linear += 1\n",
        "        if linear == 1:\n",
        "          stdv = 3 / math.sqrt(l.weight.size(1))\n",
        "          l.weight.data.uniform_(-1e-8, 1e-8)\n",
        "          if l.bias is not None:\n",
        "            l.bias.data.uniform_(-stdv, stdv)\n",
        "        elif linear == 2:\n",
        "          l.weight.data.uniform_(-1e-8, 1e-8)\n"
      ],
      "metadata": {
        "id": "bGKsu0dNuGEH"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Datasets"
      ],
      "metadata": {
        "id": "7sX-7JDDtHOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DataHandler():\n",
        "  def __init__(self, dataset : str):\n",
        "    if dataset == \"MNIST\":\n",
        "      transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "      train_ds = MNIST(\"data/\", train=True, download=True, transform=transform)\n",
        "      test_ds = MNIST(\"data/\", train=False, download=True)\n",
        "\n",
        "      self.train_dl = DataLoader(train_ds, batch_size = 256, shuffle=True, drop_last=True)\n",
        "      self.test_dl = DataLoader(test_ds, batch_size = 256, shuffle=True, drop_last=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "3zxoMQRRsF1o"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "TuwFtAqgtLYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(9329582034)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = CryptoNet(verbose=False)\n",
        "model.weights_init()\n",
        "#model = CNN()\n",
        "\n",
        "def grad_clipping(model):\n",
        "  for p in model.parameters():\n",
        "    p.register_hook(lambda grad: print(grad))\n",
        "    p.register_hook(lambda grad: torch.clamp(grad, 0, 1.0))\n",
        "\n",
        "    p.register_hook(lambda grad: print(f\"{p} -> {grad}\"))\n",
        "\n",
        "\n",
        "#grad_clipping(model)\n",
        "model = model.to(device=device)\n",
        "dataHandler = DataHandler(\"MNIST\")\n",
        "\n",
        "learning_rate = 3e-4\n",
        "momentum = 0.9\n",
        "num_epochs = 5\n",
        "total_step = len(dataHandler.train_dl)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (data, labels) in enumerate(dataHandler.train_dl):\n",
        "    data = data.to(device=device)\n",
        "    labels = labels.to(device=device)\n",
        "    #labels = labels.to(torch.float32)\n",
        "\n",
        "    ## Forward\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(data)\n",
        "    loss = criterion(predictions, labels)\n",
        "    loss.backward()\n",
        "    if model.verbose:\n",
        "      print(f\"[?] Step {i+1} Epoch {epoch+1}\")\n",
        "      plot_grad_flow(model.named_parameters())\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 20 == 0:\n",
        "      print ('[!] Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "torch.save(model, \"cryptoNet.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnkgcAJktEu9",
        "outputId": "0ae2e124-ad8b-48d3-a847-d3e5cb30f828"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[!] Epoch [1/5], Step [20/234], Loss: 2.2951\n",
            "[!] Epoch [1/5], Step [40/234], Loss: 2.1058\n",
            "[!] Epoch [1/5], Step [60/234], Loss: 1.9556\n",
            "[!] Epoch [1/5], Step [80/234], Loss: 1.8818\n",
            "[!] Epoch [1/5], Step [100/234], Loss: 1.8630\n",
            "[!] Epoch [1/5], Step [120/234], Loss: 1.8581\n",
            "[!] Epoch [1/5], Step [140/234], Loss: 1.8475\n",
            "[!] Epoch [1/5], Step [160/234], Loss: 1.8195\n",
            "[!] Epoch [1/5], Step [180/234], Loss: 1.8066\n",
            "[!] Epoch [1/5], Step [200/234], Loss: 1.7799\n",
            "[!] Epoch [1/5], Step [220/234], Loss: 1.7439\n",
            "[!] Epoch [2/5], Step [20/234], Loss: 1.7231\n",
            "[!] Epoch [2/5], Step [40/234], Loss: 1.6997\n",
            "[!] Epoch [2/5], Step [60/234], Loss: 1.7419\n",
            "[!] Epoch [2/5], Step [80/234], Loss: 1.6978\n",
            "[!] Epoch [2/5], Step [100/234], Loss: 1.7100\n",
            "[!] Epoch [2/5], Step [120/234], Loss: 1.6563\n",
            "[!] Epoch [2/5], Step [140/234], Loss: 1.6612\n",
            "[!] Epoch [2/5], Step [160/234], Loss: 1.6475\n",
            "[!] Epoch [2/5], Step [180/234], Loss: 1.6162\n",
            "[!] Epoch [2/5], Step [200/234], Loss: 1.7035\n",
            "[!] Epoch [2/5], Step [220/234], Loss: 1.6456\n",
            "[!] Epoch [3/5], Step [20/234], Loss: 1.6461\n",
            "[!] Epoch [3/5], Step [40/234], Loss: 1.6241\n",
            "[!] Epoch [3/5], Step [60/234], Loss: 1.6508\n",
            "[!] Epoch [3/5], Step [80/234], Loss: 1.6355\n",
            "[!] Epoch [3/5], Step [100/234], Loss: 1.6167\n",
            "[!] Epoch [3/5], Step [120/234], Loss: 1.6346\n",
            "[!] Epoch [3/5], Step [140/234], Loss: 1.6245\n",
            "[!] Epoch [3/5], Step [160/234], Loss: 1.6347\n",
            "[!] Epoch [3/5], Step [180/234], Loss: 1.6045\n",
            "[!] Epoch [3/5], Step [200/234], Loss: 1.6166\n",
            "[!] Epoch [3/5], Step [220/234], Loss: 1.5581\n",
            "[!] Epoch [4/5], Step [20/234], Loss: 1.5579\n",
            "[!] Epoch [4/5], Step [40/234], Loss: 1.5693\n",
            "[!] Epoch [4/5], Step [60/234], Loss: 1.5583\n",
            "[!] Epoch [4/5], Step [80/234], Loss: 1.5414\n",
            "[!] Epoch [4/5], Step [100/234], Loss: 1.5369\n",
            "[!] Epoch [4/5], Step [120/234], Loss: 1.5203\n",
            "[!] Epoch [4/5], Step [140/234], Loss: 1.5190\n",
            "[!] Epoch [4/5], Step [160/234], Loss: 1.5099\n",
            "[!] Epoch [4/5], Step [180/234], Loss: 1.5182\n",
            "[!] Epoch [4/5], Step [200/234], Loss: 1.5190\n",
            "[!] Epoch [4/5], Step [220/234], Loss: 1.5343\n",
            "[!] Epoch [5/5], Step [20/234], Loss: 1.5671\n",
            "[!] Epoch [5/5], Step [40/234], Loss: 1.5313\n",
            "[!] Epoch [5/5], Step [60/234], Loss: 1.5327\n",
            "[!] Epoch [5/5], Step [80/234], Loss: 1.5011\n",
            "[!] Epoch [5/5], Step [100/234], Loss: 1.5305\n",
            "[!] Epoch [5/5], Step [120/234], Loss: 1.5297\n",
            "[!] Epoch [5/5], Step [140/234], Loss: 1.5027\n",
            "[!] Epoch [5/5], Step [160/234], Loss: 1.5188\n",
            "[!] Epoch [5/5], Step [180/234], Loss: 1.5037\n",
            "[!] Epoch [5/5], Step [200/234], Loss: 1.5075\n",
            "[!] Epoch [5/5], Step [220/234], Loss: 1.5212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "lz5pM3GavjgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = CNN()\n",
        "\n",
        "def grad_clipping(model):\n",
        "  for p in model.parameters():\n",
        "    p.register_hook(lambda grad: print(grad))\n",
        "    p.register_hook(lambda grad: torch.clamp(grad, 0, 1.0))\n",
        "\n",
        "    p.register_hook(lambda grad: print(f\"{p} -> {grad}\"))\n",
        "\n",
        "#grad_clipping(model)\n",
        "model = model.to(device=device)\n",
        "dataHandler = DataHandler(\"MNIST\")\n",
        "\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9\n",
        "num_epochs = 1\n",
        "total_step = len(dataHandler.train_dl)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (data, labels) in enumerate(dataHandler.train_dl):\n",
        "    data = data.to(device=device)\n",
        "    labels = labels.to(device=device)\n",
        "    #labels = labels.to(torch.float32)\n",
        "\n",
        "    ## Forward\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(data)\n",
        "    loss = criterion(predictions, labels)\n",
        "    loss.backward()\n",
        "    plot_grad_flow(model.named_parameters())\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "      print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "torch.save(model, \"cryptoNet.pt\")"
      ],
      "metadata": {
        "id": "uD7LT4QNvkaJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d46c1a6-e6cd-46f1-cd85-f4ac5cc39dd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0055), tensor(0.0038), tensor(0.0065)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0054), tensor(0.0037), tensor(0.0060)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0063), tensor(0.0039), tensor(0.0060)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0080), tensor(0.0043), tensor(0.0066)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0090), tensor(0.0047), tensor(0.0065)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0138), tensor(0.0054), tensor(0.0082)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0172), tensor(0.0060), tensor(0.0076)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0229), tensor(0.0062), tensor(0.0079)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0239), tensor(0.0062), tensor(0.0082)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0258), tensor(0.0066), tensor(0.0108)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0271), tensor(0.0063), tensor(0.0107)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0260), tensor(0.0069), tensor(0.0166)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0299), tensor(0.0067), tensor(0.0149)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0284), tensor(0.0062), tensor(0.0144)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0300), tensor(0.0063), tensor(0.0134)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0291), tensor(0.0064), tensor(0.0156)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0251), tensor(0.0047), tensor(0.0108)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0219), tensor(0.0046), tensor(0.0144)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0174), tensor(0.0043), tensor(0.0142)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0199), tensor(0.0052), tensor(0.0150)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0149), tensor(0.0039), tensor(0.0147)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0131), tensor(0.0042), tensor(0.0168)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0168), tensor(0.0050), tensor(0.0189)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0075), tensor(0.0043), tensor(0.0186)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0152), tensor(0.0045), tensor(0.0180)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0098), tensor(0.0040), tensor(0.0157)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0139), tensor(0.0059), tensor(0.0222)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0093), tensor(0.0049), tensor(0.0220)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0104), tensor(0.0035), tensor(0.0146)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0103), tensor(0.0042), tensor(0.0175)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0099), tensor(0.0044), tensor(0.0201)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0087), tensor(0.0041), tensor(0.0198)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0127), tensor(0.0046), tensor(0.0162)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0069), tensor(0.0035), tensor(0.0152)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0089), tensor(0.0040), tensor(0.0146)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0079), tensor(0.0042), tensor(0.0170)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0148), tensor(0.0053), tensor(0.0198)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0123), tensor(0.0055), tensor(0.0160)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0090), tensor(0.0039), tensor(0.0131)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0069), tensor(0.0036), tensor(0.0165)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0067), tensor(0.0031), tensor(0.0126)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0058), tensor(0.0029), tensor(0.0116)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0077), tensor(0.0027), tensor(0.0100)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0119), tensor(0.0056), tensor(0.0176)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0066), tensor(0.0031), tensor(0.0115)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0088), tensor(0.0027), tensor(0.0112)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0079), tensor(0.0033), tensor(0.0146)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0066), tensor(0.0033), tensor(0.0128)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0084), tensor(0.0034), tensor(0.0149)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0079), tensor(0.0033), tensor(0.0143)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0090), tensor(0.0035), tensor(0.0122)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0119), tensor(0.0043), tensor(0.0140)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0054), tensor(0.0028), tensor(0.0089)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0093), tensor(0.0039), tensor(0.0161)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0150), tensor(0.0074), tensor(0.0212)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0098), tensor(0.0049), tensor(0.0193)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0084), tensor(0.0033), tensor(0.0100)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0108), tensor(0.0032), tensor(0.0142)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0087), tensor(0.0029), tensor(0.0090)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0060), tensor(0.0028), tensor(0.0110)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0083), tensor(0.0034), tensor(0.0122)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0128), tensor(0.0060), tensor(0.0157)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0123), tensor(0.0052), tensor(0.0157)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0078), tensor(0.0038), tensor(0.0143)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0070), tensor(0.0031), tensor(0.0109)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0059), tensor(0.0044), tensor(0.0180)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0053), tensor(0.0029), tensor(0.0110)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0067), tensor(0.0029), tensor(0.0080)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0142), tensor(0.0063), tensor(0.0156)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0078), tensor(0.0038), tensor(0.0118)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0112), tensor(0.0044), tensor(0.0137)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0083), tensor(0.0033), tensor(0.0093)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0136), tensor(0.0050), tensor(0.0124)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0133), tensor(0.0050), tensor(0.0132)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0066), tensor(0.0025), tensor(0.0081)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0080), tensor(0.0036), tensor(0.0139)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0103), tensor(0.0044), tensor(0.0142)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0085), tensor(0.0037), tensor(0.0107)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0063), tensor(0.0024), tensor(0.0083)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0055), tensor(0.0026), tensor(0.0064)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0062), tensor(0.0029), tensor(0.0074)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0166), tensor(0.0053), tensor(0.0127)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0072), tensor(0.0030), tensor(0.0085)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0091), tensor(0.0036), tensor(0.0097)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0062), tensor(0.0038), tensor(0.0118)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0079), tensor(0.0038), tensor(0.0133)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0092), tensor(0.0034), tensor(0.0090)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0076), tensor(0.0041), tensor(0.0139)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0078), tensor(0.0029), tensor(0.0085)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0086), tensor(0.0041), tensor(0.0103)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0061), tensor(0.0028), tensor(0.0091)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0063), tensor(0.0023), tensor(0.0070)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0065), tensor(0.0030), tensor(0.0080)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0121), tensor(0.0043), tensor(0.0090)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0053), tensor(0.0013), tensor(0.0043)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0058), tensor(0.0026), tensor(0.0069)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0055), tensor(0.0023), tensor(0.0061)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0063), tensor(0.0030), tensor(0.0135)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0048), tensor(0.0021), tensor(0.0086)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0073), tensor(0.0035), tensor(0.0079)]\n",
            "Epoch [1/1], Step [100/468], Loss: 0.2237\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0058), tensor(0.0031), tensor(0.0102)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0075), tensor(0.0031), tensor(0.0098)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0064), tensor(0.0026), tensor(0.0101)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0069), tensor(0.0026), tensor(0.0078)]\n",
            "['conv1.0.weight', 'conv2.0.weight', 'out.weight']\n",
            "[tensor(0.0079), tensor(0.0031), tensor(0.0090)]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-1705c379236f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m## Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-29e930ea2d70>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# flatten the output of conv2 to (batch_size, 32 * 7 * 7)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}