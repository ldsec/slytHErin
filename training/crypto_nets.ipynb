{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crypto_nets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CryptoNet implementation and training on MNIST dataset"
      ],
      "metadata": {
        "id": "HYmdRutcpdeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "prOXZ9RESeYD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import save_image\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dummy model for testing training pipeline"
      ],
      "metadata": {
        "id": "DUluBfyZuKld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## DUMMY MODEL\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()        \n",
        "        self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=1,              \n",
        "                out_channels=16,            \n",
        "                kernel_size=5,              \n",
        "                stride=1,                   \n",
        "                padding=2,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(kernel_size=2),    \n",
        "        )\n",
        "        self.conv2 = nn.Sequential(         \n",
        "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(2),                \n",
        "        )        # fully connected layer, output 10 classes\n",
        "        self.out = nn.Linear(32 * 7 * 7, 10)    \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "        x = x.view(x.size(0), -1)       \n",
        "        output = self.out(x)\n",
        "        return output  # return x for visualization\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "Ii4rDPKqVPYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CryptoNet from [Microsoft](https://www.microsoft.com/en-us/research/publication/cryptonets-applying-neural-networks-to-encrypted-data-with-high-throughput-and-accuracy/)"
      ],
      "metadata": {
        "id": "hrxhF5Msda_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledAvgPool2d(nn.Module):\n",
        "    \"\"\"Define the ScaledAvgPool layer, a.k.a the Sum Pool\"\"\"\n",
        "    def __init__(self,kernel_size):\n",
        "      super().__init__()\n",
        "      self.kernel_size = kernel_size\n",
        "      self.AvgPool = nn.AvgPool2d(kernel_size=self.kernel_size, stride=1, padding=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "      return (self.kernel_size**2)*self.AvgPool(x)\n",
        "    \n",
        "\n",
        "class CryptoNet(nn.Module):\n",
        "  '''\n",
        "    Original 9-layer network used during training\n",
        "    CURRENTLY NOT WORKING\n",
        "  '''\n",
        "  def __init__(self, verbose):\n",
        "    super().__init__()\n",
        "    self.verbose = verbose\n",
        "    self.pad = F.pad\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\n",
        "    self.square1 = torch.square\n",
        "    self.scaledAvgPool1 = ScaledAvgPool2d(kernel_size=3)\n",
        "    self.conv2 = nn.Conv2d(in_channels=5, out_channels=50, kernel_size=5, stride=2)\n",
        "    self.scaledAvgPool2 = ScaledAvgPool2d(kernel_size=3)\n",
        "    self.fc1 = nn.Linear(in_features=1250, out_features=100) # in paper in_features was 1250\n",
        "    self.square2 = torch.square\n",
        "    self.fc2 = nn.Linear(in_features=100, out_features=10)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pad(x, (1,0,1,0))\n",
        "    if self.verbose:\n",
        "      print(\"Start --> \",x.mean())\n",
        "    x = self.conv1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Conv1 --> \",x.mean())\n",
        "    x = self.square1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Sq --> \",x.mean())\n",
        "    x = self.scaledAvgPool1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Pool --> \",x.mean())\n",
        "    x = self.conv2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Conv2 --> \",x.mean())\n",
        "    x = self.scaledAvgPool2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Pool --> \",x.mean())\n",
        "    ## Flatten\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.fc1(x)\n",
        "    if self.verbose:\n",
        "      print(\"fc1 --> \",x.mean())\n",
        "    x = self.square2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Square --> \",x.mean())\n",
        "    x = self.fc2(x)\n",
        "    if self.verbose:\n",
        "      print(\"fc2 --> \",x.mean())\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "  def weights_init(self, m):\n",
        "    \"\"\" Custom initilization to avoid square activation to blow up \"\"\"\n",
        "    for m in self.children():\n",
        "      if isinstance(m,nn.Conv2d):\n",
        "        nn.init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        nn.init.uniform_(m.weight, 1e-4,1e-3)\n"
      ],
      "metadata": {
        "id": "bGKsu0dNuGEH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNet(nn.Module):\n",
        "  '''\n",
        "    Simpliefied network used in paper for inference\n",
        "  '''\n",
        "  def __init__(self, batch_size, verbose):\n",
        "    super().__init__()\n",
        "    self.verbose = verbose\n",
        "    self.batch_size = batch_size\n",
        "    self.pad = F.pad\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\n",
        "    self.square1 = torch.square\n",
        "    self.pool1 = nn.Conv2d(in_channels=5, out_channels=100, kernel_size=13, stride=1000)\n",
        "    self.square2 = torch.square\n",
        "    self.pool2 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(100,1), stride=1000)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pad(x, (1,0,1,0))\n",
        "    x = self.conv1(x)\n",
        "    x = self.square1(self.pool1(x))\n",
        "    x = x.reshape([self.batch_size,1,100,1]) #batch_size tensors in 1 channel, 100x1\n",
        "    x = self.square2(self.pool2(x))\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    return x\n",
        "\n",
        "  def weights_init(self, m):\n",
        "    \"\"\" HE weigth init --> do not use, worse performance\"\"\"\n",
        "    for m in self.children():\n",
        "      if isinstance(m,nn.Conv2d):\n",
        "        nn.init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "  def set_batch_size(self, batch_size):\n",
        "    self.batch_size = batch_size"
      ],
      "metadata": {
        "id": "tunwxIS3XLgm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Datasets"
      ],
      "metadata": {
        "id": "7sX-7JDDtHOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataHandler():\n",
        "  def __init__(self, dataset : str, batch_size : int):\n",
        "    if dataset == \"MNIST\":\n",
        "      self.batch_size = batch_size\n",
        "      transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "      train_ds = MNIST(\"data/\", train=True, download=True, transform=transform)\n",
        "      test_ds = MNIST(\"data/\", train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "      self.train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle=True, drop_last=True)\n",
        "      self.test_dl = DataLoader(test_ds, batch_size = batch_size, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "3zxoMQRRsF1o"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot gradient flow"
      ],
      "metadata": {
        "id": "xXXqZm508qA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_grad_flow(named_parameters):\n",
        "    ## From https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063\n",
        "    ## Beware it's a little bit tricky to interpret results\n",
        "    '''Plots the gradients flowing through different layers in the net during training.\n",
        "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
        "    \n",
        "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
        "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
        "\n",
        "    ave_grads = []\n",
        "    max_grads = []\n",
        "    layers = []\n",
        "    for n, p in named_parameters:\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\n",
        "            layers.append(n)\n",
        "            ave_grads.append(p.grad.abs().mean())\n",
        "            max_grads.append(p.grad.abs().max())\n",
        "            print(f\"Layer {n}, grad avg {p.grad.mean()}, data {p.data.mean()}\")\n",
        "    plt.bar(np.arange(len(max_grads)), max(max_grads), alpha=0.1, lw=1, color=\"c\")\n",
        "    plt.bar(np.arange(len(max_grads)), np.mean(ave_grads), alpha=0.1, lw=1, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(left=0, right=len(ave_grads))\n",
        "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")\n",
        "    plt.grid(True)\n",
        "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
        "                Line2D([0], [0], color=\"b\", lw=4),\n",
        "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
        "    \n"
      ],
      "metadata": {
        "id": "h3f19IYJ8nIQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training of 9-layer CryptoNet"
      ],
      "metadata": {
        "id": "TuwFtAqgtLYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## setup torch enviro\n",
        "torch.manual_seed(9325345339582034)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "## init model\n",
        "model = CryptoNet(verbose=False)\n",
        "model.apply(model.weights_init)\n",
        "model = model.to(device=device)\n",
        "\n",
        "dataHandler = DataHandler(\"MNIST\")\n",
        "\n",
        "## training params setup\n",
        "learning_rate = 3e-4\n",
        "momentum = 0.9\n",
        "num_epochs = 5000\n",
        "total_step = len(dataHandler.train_dl)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (data, labels) in enumerate(dataHandler.train_dl):\n",
        "    data = data.to(device=device)\n",
        "    labels = labels.to(device=device)\n",
        "    #labels = labels.to(torch.float32)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(data)\n",
        "    loss = criterion(predictions, labels)\n",
        "    loss.backward()\n",
        "    if model.verbose:\n",
        "      print(f\"[?] Step {i+1} Epoch {epoch+1}\")\n",
        "      plot_grad_flow(model.named_parameters())\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 50 == 0:\n",
        "      print ('[!] Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "\n",
        "torch.save(model, \"cryptoNet.pt\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "xnkgcAJktEu9",
        "outputId": "7d68416e-81e9-401b-f215-c1fb3a94247b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[!] Epoch [1/5000], Step [50/234], Loss: 2.3518\n",
            "[!] Epoch [1/5000], Step [100/234], Loss: 2.3479\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6a6ff05b73b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[?] Step {i+1} Epoch {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Model"
      ],
      "metadata": {
        "id": "BUesY20pbmSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## setup torch enviro\n",
        "torch.manual_seed(9325345339582034)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dataHandler = DataHandler(dataset=\"MNIST\", batch_size=256)\n",
        "\n",
        "## init model\n",
        "model = SimpleNet(batch_size=dataHandler.batch_size, verbose=False,)\n",
        "#model.apply(model.weights_init)\n",
        "model = model.to(device=device)\n",
        "\n",
        "## training params setup\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 50\n",
        "total_step = len(dataHandler.train_dl)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (data, labels) in enumerate(dataHandler.train_dl):\n",
        "    data = data.to(device=device)\n",
        "    labels = labels.to(device=device)\n",
        "    #labels = labels.to(torch.float32)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(data)\n",
        "    loss = criterion(predictions, labels)\n",
        "    loss.backward()\n",
        "    if model.verbose:\n",
        "      print(f\"[?] Step {i+1} Epoch {epoch+1}\")\n",
        "      plot_grad_flow(model.named_parameters())\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 50 == 0:\n",
        "      print ('[!] Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "\n",
        "torch.save(model, \"simpleNet.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfZdPxHKZwKi",
        "outputId": "a35d6f56-fafd-4f48-fbc5-98fcdd1b9b08"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[!] Epoch [1/50], Step [50/234], Loss: 0.7270\n",
            "[!] Epoch [1/50], Step [100/234], Loss: 0.3414\n",
            "[!] Epoch [1/50], Step [150/234], Loss: 0.2210\n",
            "[!] Epoch [1/50], Step [200/234], Loss: 0.1596\n",
            "[!] Epoch [2/50], Step [50/234], Loss: 0.1058\n",
            "[!] Epoch [2/50], Step [100/234], Loss: 0.1671\n",
            "[!] Epoch [2/50], Step [150/234], Loss: 0.0880\n",
            "[!] Epoch [2/50], Step [200/234], Loss: 0.1568\n",
            "[!] Epoch [3/50], Step [50/234], Loss: 0.1020\n",
            "[!] Epoch [3/50], Step [100/234], Loss: 0.0552\n",
            "[!] Epoch [3/50], Step [150/234], Loss: 0.1263\n",
            "[!] Epoch [3/50], Step [200/234], Loss: 0.1348\n",
            "[!] Epoch [4/50], Step [50/234], Loss: 0.0698\n",
            "[!] Epoch [4/50], Step [100/234], Loss: 0.0528\n",
            "[!] Epoch [4/50], Step [150/234], Loss: 0.0784\n",
            "[!] Epoch [4/50], Step [200/234], Loss: 0.0814\n",
            "[!] Epoch [5/50], Step [50/234], Loss: 0.0540\n",
            "[!] Epoch [5/50], Step [100/234], Loss: 0.0786\n",
            "[!] Epoch [5/50], Step [150/234], Loss: 0.0745\n",
            "[!] Epoch [5/50], Step [200/234], Loss: 0.0932\n",
            "[!] Epoch [6/50], Step [50/234], Loss: 0.0704\n",
            "[!] Epoch [6/50], Step [100/234], Loss: 0.0317\n",
            "[!] Epoch [6/50], Step [150/234], Loss: 0.0322\n",
            "[!] Epoch [6/50], Step [200/234], Loss: 0.0857\n",
            "[!] Epoch [7/50], Step [50/234], Loss: 0.0271\n",
            "[!] Epoch [7/50], Step [100/234], Loss: 0.0310\n",
            "[!] Epoch [7/50], Step [150/234], Loss: 0.0366\n",
            "[!] Epoch [7/50], Step [200/234], Loss: 0.0418\n",
            "[!] Epoch [8/50], Step [50/234], Loss: 0.0067\n",
            "[!] Epoch [8/50], Step [100/234], Loss: 0.0536\n",
            "[!] Epoch [8/50], Step [150/234], Loss: 0.0301\n",
            "[!] Epoch [8/50], Step [200/234], Loss: 0.0812\n",
            "[!] Epoch [9/50], Step [50/234], Loss: 0.0294\n",
            "[!] Epoch [9/50], Step [100/234], Loss: 0.0267\n",
            "[!] Epoch [9/50], Step [150/234], Loss: 0.0312\n",
            "[!] Epoch [9/50], Step [200/234], Loss: 0.0168\n",
            "[!] Epoch [10/50], Step [50/234], Loss: 0.0095\n",
            "[!] Epoch [10/50], Step [100/234], Loss: 0.0252\n",
            "[!] Epoch [10/50], Step [150/234], Loss: 0.0547\n",
            "[!] Epoch [10/50], Step [200/234], Loss: 0.0262\n",
            "[!] Epoch [11/50], Step [50/234], Loss: 0.0409\n",
            "[!] Epoch [11/50], Step [100/234], Loss: 0.0146\n",
            "[!] Epoch [11/50], Step [150/234], Loss: 0.0527\n",
            "[!] Epoch [11/50], Step [200/234], Loss: 0.0155\n",
            "[!] Epoch [12/50], Step [50/234], Loss: 0.0021\n",
            "[!] Epoch [12/50], Step [100/234], Loss: 0.0223\n",
            "[!] Epoch [12/50], Step [150/234], Loss: 0.0087\n",
            "[!] Epoch [12/50], Step [200/234], Loss: 0.0297\n",
            "[!] Epoch [13/50], Step [50/234], Loss: 0.0085\n",
            "[!] Epoch [13/50], Step [100/234], Loss: 0.0171\n",
            "[!] Epoch [13/50], Step [150/234], Loss: 0.0289\n",
            "[!] Epoch [13/50], Step [200/234], Loss: 0.0278\n",
            "[!] Epoch [14/50], Step [50/234], Loss: 0.0336\n",
            "[!] Epoch [14/50], Step [100/234], Loss: 0.0095\n",
            "[!] Epoch [14/50], Step [150/234], Loss: 0.0067\n",
            "[!] Epoch [14/50], Step [200/234], Loss: 0.0179\n",
            "[!] Epoch [15/50], Step [50/234], Loss: 0.0044\n",
            "[!] Epoch [15/50], Step [100/234], Loss: 0.0153\n",
            "[!] Epoch [15/50], Step [150/234], Loss: 0.0333\n",
            "[!] Epoch [15/50], Step [200/234], Loss: 0.0319\n",
            "[!] Epoch [16/50], Step [50/234], Loss: 0.0020\n",
            "[!] Epoch [16/50], Step [100/234], Loss: 0.0129\n",
            "[!] Epoch [16/50], Step [150/234], Loss: 0.0358\n",
            "[!] Epoch [16/50], Step [200/234], Loss: 0.0515\n",
            "[!] Epoch [17/50], Step [50/234], Loss: 0.0017\n",
            "[!] Epoch [17/50], Step [100/234], Loss: 0.0258\n",
            "[!] Epoch [17/50], Step [150/234], Loss: 0.0057\n",
            "[!] Epoch [17/50], Step [200/234], Loss: 0.0119\n",
            "[!] Epoch [18/50], Step [50/234], Loss: 0.0038\n",
            "[!] Epoch [18/50], Step [100/234], Loss: 0.0571\n",
            "[!] Epoch [18/50], Step [150/234], Loss: 0.0084\n",
            "[!] Epoch [18/50], Step [200/234], Loss: 0.0015\n",
            "[!] Epoch [19/50], Step [50/234], Loss: 0.0056\n",
            "[!] Epoch [19/50], Step [100/234], Loss: 0.0013\n",
            "[!] Epoch [19/50], Step [150/234], Loss: 0.0090\n",
            "[!] Epoch [19/50], Step [200/234], Loss: 0.0344\n",
            "[!] Epoch [20/50], Step [50/234], Loss: 0.0146\n",
            "[!] Epoch [20/50], Step [100/234], Loss: 0.0183\n",
            "[!] Epoch [20/50], Step [150/234], Loss: 0.0079\n",
            "[!] Epoch [20/50], Step [200/234], Loss: 0.0131\n",
            "[!] Epoch [21/50], Step [50/234], Loss: 0.0048\n",
            "[!] Epoch [21/50], Step [100/234], Loss: 0.0025\n",
            "[!] Epoch [21/50], Step [150/234], Loss: 0.0107\n",
            "[!] Epoch [21/50], Step [200/234], Loss: 0.0024\n",
            "[!] Epoch [22/50], Step [50/234], Loss: 0.0029\n",
            "[!] Epoch [22/50], Step [100/234], Loss: 0.0015\n",
            "[!] Epoch [22/50], Step [150/234], Loss: 0.0040\n",
            "[!] Epoch [22/50], Step [200/234], Loss: 0.0029\n",
            "[!] Epoch [23/50], Step [50/234], Loss: 0.0006\n",
            "[!] Epoch [23/50], Step [100/234], Loss: 0.0215\n",
            "[!] Epoch [23/50], Step [150/234], Loss: 0.0060\n",
            "[!] Epoch [23/50], Step [200/234], Loss: 0.0119\n",
            "[!] Epoch [24/50], Step [50/234], Loss: 0.0024\n",
            "[!] Epoch [24/50], Step [100/234], Loss: 0.0158\n",
            "[!] Epoch [24/50], Step [150/234], Loss: 0.0042\n",
            "[!] Epoch [24/50], Step [200/234], Loss: 0.0015\n",
            "[!] Epoch [25/50], Step [50/234], Loss: 0.0040\n",
            "[!] Epoch [25/50], Step [100/234], Loss: 0.0029\n",
            "[!] Epoch [25/50], Step [150/234], Loss: 0.0009\n",
            "[!] Epoch [25/50], Step [200/234], Loss: 0.0133\n",
            "[!] Epoch [26/50], Step [50/234], Loss: 0.0033\n",
            "[!] Epoch [26/50], Step [100/234], Loss: 0.0308\n",
            "[!] Epoch [26/50], Step [150/234], Loss: 0.0160\n",
            "[!] Epoch [26/50], Step [200/234], Loss: 0.0057\n",
            "[!] Epoch [27/50], Step [50/234], Loss: 0.0032\n",
            "[!] Epoch [27/50], Step [100/234], Loss: 0.0193\n",
            "[!] Epoch [27/50], Step [150/234], Loss: 0.0125\n",
            "[!] Epoch [27/50], Step [200/234], Loss: 0.0218\n",
            "[!] Epoch [28/50], Step [50/234], Loss: 0.0066\n",
            "[!] Epoch [28/50], Step [100/234], Loss: 0.0098\n",
            "[!] Epoch [28/50], Step [150/234], Loss: 0.0002\n",
            "[!] Epoch [28/50], Step [200/234], Loss: 0.0023\n",
            "[!] Epoch [29/50], Step [50/234], Loss: 0.0163\n",
            "[!] Epoch [29/50], Step [100/234], Loss: 0.0015\n",
            "[!] Epoch [29/50], Step [150/234], Loss: 0.0101\n",
            "[!] Epoch [29/50], Step [200/234], Loss: 0.0098\n",
            "[!] Epoch [30/50], Step [50/234], Loss: 0.0002\n",
            "[!] Epoch [30/50], Step [100/234], Loss: 0.0083\n",
            "[!] Epoch [30/50], Step [150/234], Loss: 0.0013\n",
            "[!] Epoch [30/50], Step [200/234], Loss: 0.0035\n",
            "[!] Epoch [31/50], Step [50/234], Loss: 0.0005\n",
            "[!] Epoch [31/50], Step [100/234], Loss: 0.0020\n",
            "[!] Epoch [31/50], Step [150/234], Loss: 0.0077\n",
            "[!] Epoch [31/50], Step [200/234], Loss: 0.0001\n",
            "[!] Epoch [32/50], Step [50/234], Loss: 0.0102\n",
            "[!] Epoch [32/50], Step [100/234], Loss: 0.0090\n",
            "[!] Epoch [32/50], Step [150/234], Loss: 0.0063\n",
            "[!] Epoch [32/50], Step [200/234], Loss: 0.0163\n",
            "[!] Epoch [33/50], Step [50/234], Loss: 0.0041\n",
            "[!] Epoch [33/50], Step [100/234], Loss: 0.0026\n",
            "[!] Epoch [33/50], Step [150/234], Loss: 0.0105\n",
            "[!] Epoch [33/50], Step [200/234], Loss: 0.0556\n",
            "[!] Epoch [34/50], Step [50/234], Loss: 0.0254\n",
            "[!] Epoch [34/50], Step [100/234], Loss: 0.0033\n",
            "[!] Epoch [34/50], Step [150/234], Loss: 0.0021\n",
            "[!] Epoch [34/50], Step [200/234], Loss: 0.0042\n",
            "[!] Epoch [35/50], Step [50/234], Loss: 0.0037\n",
            "[!] Epoch [35/50], Step [100/234], Loss: 0.0019\n",
            "[!] Epoch [35/50], Step [150/234], Loss: 0.0014\n",
            "[!] Epoch [35/50], Step [200/234], Loss: 0.0018\n",
            "[!] Epoch [36/50], Step [50/234], Loss: 0.0008\n",
            "[!] Epoch [36/50], Step [100/234], Loss: 0.0001\n",
            "[!] Epoch [36/50], Step [150/234], Loss: 0.0005\n",
            "[!] Epoch [36/50], Step [200/234], Loss: 0.0003\n",
            "[!] Epoch [37/50], Step [50/234], Loss: 0.0011\n",
            "[!] Epoch [37/50], Step [100/234], Loss: 0.0002\n",
            "[!] Epoch [37/50], Step [150/234], Loss: 0.0002\n",
            "[!] Epoch [37/50], Step [200/234], Loss: 0.0026\n",
            "[!] Epoch [38/50], Step [50/234], Loss: 0.0005\n",
            "[!] Epoch [38/50], Step [100/234], Loss: 0.0007\n",
            "[!] Epoch [38/50], Step [150/234], Loss: 0.0014\n",
            "[!] Epoch [38/50], Step [200/234], Loss: 0.0086\n",
            "[!] Epoch [39/50], Step [50/234], Loss: 0.0002\n",
            "[!] Epoch [39/50], Step [100/234], Loss: 0.0126\n",
            "[!] Epoch [39/50], Step [150/234], Loss: 0.0166\n",
            "[!] Epoch [39/50], Step [200/234], Loss: 0.0007\n",
            "[!] Epoch [40/50], Step [50/234], Loss: 0.0006\n",
            "[!] Epoch [40/50], Step [100/234], Loss: 0.0051\n",
            "[!] Epoch [40/50], Step [150/234], Loss: 0.0004\n",
            "[!] Epoch [40/50], Step [200/234], Loss: 0.0351\n",
            "[!] Epoch [41/50], Step [50/234], Loss: 0.0013\n",
            "[!] Epoch [41/50], Step [100/234], Loss: 0.0079\n",
            "[!] Epoch [41/50], Step [150/234], Loss: 0.0005\n",
            "[!] Epoch [41/50], Step [200/234], Loss: 0.0006\n",
            "[!] Epoch [42/50], Step [50/234], Loss: 0.0098\n",
            "[!] Epoch [42/50], Step [100/234], Loss: 0.0048\n",
            "[!] Epoch [42/50], Step [150/234], Loss: 0.0042\n",
            "[!] Epoch [42/50], Step [200/234], Loss: 0.0164\n",
            "[!] Epoch [43/50], Step [50/234], Loss: 0.0010\n",
            "[!] Epoch [43/50], Step [100/234], Loss: 0.0036\n",
            "[!] Epoch [43/50], Step [150/234], Loss: 0.0010\n",
            "[!] Epoch [43/50], Step [200/234], Loss: 0.0130\n",
            "[!] Epoch [44/50], Step [50/234], Loss: 0.0002\n",
            "[!] Epoch [44/50], Step [100/234], Loss: 0.0002\n",
            "[!] Epoch [44/50], Step [150/234], Loss: 0.0013\n",
            "[!] Epoch [44/50], Step [200/234], Loss: 0.0027\n",
            "[!] Epoch [45/50], Step [50/234], Loss: 0.0003\n",
            "[!] Epoch [45/50], Step [100/234], Loss: 0.0021\n",
            "[!] Epoch [45/50], Step [150/234], Loss: 0.0004\n",
            "[!] Epoch [45/50], Step [200/234], Loss: 0.0019\n",
            "[!] Epoch [46/50], Step [50/234], Loss: 0.0055\n",
            "[!] Epoch [46/50], Step [100/234], Loss: 0.0081\n",
            "[!] Epoch [46/50], Step [150/234], Loss: 0.0019\n",
            "[!] Epoch [46/50], Step [200/234], Loss: 0.0064\n",
            "[!] Epoch [47/50], Step [50/234], Loss: 0.0031\n",
            "[!] Epoch [47/50], Step [100/234], Loss: 0.0008\n",
            "[!] Epoch [47/50], Step [150/234], Loss: 0.0014\n",
            "[!] Epoch [47/50], Step [200/234], Loss: 0.0431\n",
            "[!] Epoch [48/50], Step [50/234], Loss: 0.0003\n",
            "[!] Epoch [48/50], Step [100/234], Loss: 0.0012\n",
            "[!] Epoch [48/50], Step [150/234], Loss: 0.0315\n",
            "[!] Epoch [48/50], Step [200/234], Loss: 0.0053\n",
            "[!] Epoch [49/50], Step [50/234], Loss: 0.0010\n",
            "[!] Epoch [49/50], Step [100/234], Loss: 0.0003\n",
            "[!] Epoch [49/50], Step [150/234], Loss: 0.0041\n",
            "[!] Epoch [49/50], Step [200/234], Loss: 0.0009\n",
            "[!] Epoch [50/50], Step [50/234], Loss: 0.0004\n",
            "[!] Epoch [50/50], Step [100/234], Loss: 0.0115\n",
            "[!] Epoch [50/50], Step [150/234], Loss: 0.0004\n",
            "[!] Epoch [50/50], Step [200/234], Loss: 0.0003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "lz5pM3GavjgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_correct = 0\n",
        "num_samples = 0\n",
        "\n",
        "model.eval()\n",
        "for _, (data,labels) in enumerate(dataHandler.test_dl):\n",
        "    data = data.to(device=\"cpu\")\n",
        "    labels = labels.to(device=\"cpu\")\n",
        "    ## Forward Pass\n",
        "    predictions = model(data)\n",
        "    _, predictions = predictions.max(1)\n",
        "    num_correct += (predictions == labels).sum()\n",
        "    num_samples += predictions.size(0)\n",
        "print(f\"Accuracy {float(num_correct) / float(num_samples) * 100:.2f}\")"
      ],
      "metadata": {
        "id": "uD7LT4QNvkaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1267ee79-389c-41c9-ee38-15fbc38370c0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 95.03\n"
          ]
        }
      ]
    }
  ]
}