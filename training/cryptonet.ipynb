{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYmdRutcpdeR"
      },
      "source": [
        "# Implementation of (simplified) CryptoNet for inference under homomorphic encryption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "prOXZ9RESeYD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "from activation import *\n",
        "from logger import Logger\n",
        "from dataHandler import DataHandler\n",
        "from utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tunwxIS3XLgm"
      },
      "outputs": [],
      "source": [
        "class SimpleNet(nn.Module):\n",
        "  '''\n",
        "    Simpliefied network used in paper for inference https://www.microsoft.com/en-us/research/publication/cryptonets-applying-neural-networks-to-encrypted-data-with-high-throughput-and-accuracy/\n",
        "\n",
        "    Input size: 1x28x28 pixel\n",
        "    Pad: image is padded with 0s on left and top side --> 1x29x29\n",
        "    Conv1 --> 5x13x13\n",
        "    Pool1 --> 100x1x1 --> resized to 1x100x1\n",
        "    Pool2 --> 10x1x1 --> resized to a vector of len 10\n",
        "  '''\n",
        "  \n",
        "  def __init__(self, batch_size : int, activation : str, sigmoid : str, init_method : str, verbose : bool):\n",
        "    super().__init__()\n",
        "    self.verbose = verbose\n",
        "    self.init_method = init_method\n",
        "    self.batch_size = batch_size\n",
        "    self.activation = activation\n",
        "    self.sigmoid = sigmoid\n",
        "\n",
        "    if activation == \"square\":\n",
        "      self.activation = torch.square\n",
        "    elif activation == \"relu\":\n",
        "      self.activation = nn.ReLU()\n",
        "    elif activation == \"relu_approx\":\n",
        "      self.activation = ReLUApprox()\n",
        "\n",
        "    if sigmoid == \"sigmoid\":\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "    elif sigmoid == \"approx\":\n",
        "      self.sigmoid = SigmoidApprox()\n",
        "    elif sigmoid == \"none\":\n",
        "      self.sigmoid = identity\n",
        "\n",
        "    self.pad = F.pad\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\n",
        "    self.pool1 = nn.Conv2d(in_channels=5, out_channels=100, kernel_size=13, stride=1000)\n",
        "    self.pool2 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(100,1), stride=1000) # in chans is 1 not 100\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pad(x, (1,0,1,0))\n",
        "    \n",
        "    x = self.conv1(x)\n",
        "    \n",
        "    x = self.activation(self.pool1(x))\n",
        "    #print(x.shape)\n",
        "    x = x.reshape([self.batch_size,1,100,1]) #batch_size tensors in 1 channel, 100x1\n",
        "    x = self.activation(self.pool2(x))\n",
        "    #print(x.shape)\n",
        "    \"\"\"\n",
        "        |  legacy code:\n",
        "        |  sigmoid as last activation improved performance, but it was removed\n",
        "        v  to stick with the original architecture. sigmoid can (and is in final model) be just f(x) = x\n",
        "    x = self.sigmoid(x)\n",
        "    \"\"\"\n",
        "    x = self.sigmoid(x)\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    #print(x.shape)\n",
        "    return x\n",
        " \n",
        "  def weights_init(self, m):\n",
        "    for m in self.children():\n",
        "      if isinstance(m,nn.Conv2d):\n",
        "        if self.init_method == \"he\":\n",
        "          nn.init.kaiming_uniform_(m.weight, a=0, mode='fan_out', nonlinearity='relu')\n",
        "        elif self.init_method == \"xavier\":\n",
        "          nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2))\n",
        "        #elif self.init_method == \"uniform\":\n",
        "        #  nn.init.uniform_(m.weight, -0.5, 0.5)\n",
        "        #elif self.init_method == \"norm\":\n",
        "        #  nn.init.normal_(m.weight, 0.0, 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sX-7JDDtHOo"
      },
      "source": [
        "Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3zxoMQRRsF1o"
      },
      "outputs": [],
      "source": [
        "dataHandler = DataHandler(dataset=\"MNIST\", batch_size=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OsCh6ldz9PN"
      },
      "source": [
        "Train and test pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tn61Mth4dz-l",
        "outputId": "917579cf-bbc8-4371-b1c3-a799e53d6153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[?] SimpleNet_xavier_relu_approx Epoch 1/10 Loss 0.1487\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 2/10 Loss 0.0805\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 3/10 Loss 0.0736\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 4/10 Loss 0.0672\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 5/10 Loss 0.0613\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 6/10 Loss 0.0565\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 7/10 Loss 0.0527\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 8/10 Loss 0.0498\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 9/10 Loss 0.0475\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 10/10 Loss 0.0458\n",
            "=====================================================================\n",
            "[+] Model with xavier_relu_approx: Avg test Loss ==> 3.261318108974359, Accuracy ==> 0.8218149038461539\n"
          ]
        }
      ],
      "source": [
        "##############################\n",
        "#                            #\n",
        "# TRAINING AND EVAL PIPELINE #\n",
        "#                            #\n",
        "##############################\n",
        "\n",
        "## TEST\n",
        "## init models \n",
        "#methods = [\"he\", \"xavier\", \"random\"] ##he init blows up values with square\n",
        "#methods = [\"xavier\",\"he\"]\n",
        "#activations = [\"relu_approx\",\"relu\"]\n",
        "#models = {}\n",
        "#sigmoid = True\n",
        "#for method in methods:\n",
        "#  for activation in activations:\n",
        "#    models[method+\"_\"+activation] = SimpleNet(batch_size=dataHandler.batch_size,\n",
        "#                                    activation=activation,\n",
        "#                                    init_method=method,\n",
        "#                                    verbose=False,\n",
        "#                                    sigmoid=sigmoid).to(device=device)\n",
        "## TEST\n",
        "\n",
        "models = {}\n",
        "#models[\"xavier_relu\"] = SimpleNet(batch_size=dataHandler.batch_size,\n",
        "#                                    activation=\"relu\",\n",
        "#                                    init_method=\"xavier\",\n",
        "#                                    verbose=False,\n",
        "#                                    sigmoid=True).to(device=device)\n",
        "#models[\"he_relu\"] = SimpleNet(batch_size=dataHandler.batch_size,\n",
        "#                                    activation=\"relu\",\n",
        "#                                    init_method=\"he\",\n",
        "#                                    verbose=False,\n",
        "#                                    sigmoid=True).to(device=device)\n",
        "\n",
        "## Most promising model. With approximated sigmoid we can increase accuracy\n",
        "## up to 96%, but it's not faithful to the original model, plus it is more complex\n",
        "models[\"xavier_relu_approx\"] = SimpleNet(batch_size=dataHandler.batch_size,\n",
        "                                    activation=\"relu_approx\",\n",
        "                                    init_method=\"xavier\",\n",
        "                                    sigmoid=\"none\",\n",
        "                                    verbose=False).to(device=device)\n",
        "\n",
        "#models[\"he_relu_approx\"] = SimpleNet(batch_size=dataHandler.batch_size,\n",
        "#                                    activation=\"relu_approx\",\n",
        "#                                    init_method=\"he\",\n",
        "#                                    verbose=False,\n",
        "#                                    sigmoid=False).to(device=device)\n",
        "\n",
        "scores = {}\n",
        "\n",
        "for key, model in models.items():\n",
        "  logger = Logger(\"./logs/\",f\"SimpleNet_{key}\")\n",
        "  model.apply(model.weights_init)\n",
        "  train(logger, model, dataHandler, num_epochs=400, lr=0.001)\n",
        "  loss, accuracy = eval(logger, model, dataHandler)\n",
        "  scores[key] = {\"loss\":loss, \"accuracy\":accuracy}\n",
        "  torch.save(model, f\"./models/SimpleNet_{key}.pt\")\n",
        "\n",
        "\n",
        "for key, metrics in scores.items():\n",
        "  print(\"=====================================================================\")\n",
        "  print(f\"[+] Model with {key}: Avg test Loss ==> {metrics['loss']}, Accuracy ==> {metrics['accuracy']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
