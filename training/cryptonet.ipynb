{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYmdRutcpdeR"
      },
      "source": [
        "# Implementation of (simplified) CryptoNet and AlexNet for inference under homomorphic encryption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "prOXZ9RESeYD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "## interactive off\n",
        "plt.ioff()\n",
        "## setup torch enviro\n",
        "torch.manual_seed(42)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aopv51NWVxYq"
      },
      "source": [
        "Approximated Relus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wuBiEkXTV0Cn"
      },
      "outputs": [],
      "source": [
        "def approx_relu_2d(x):\n",
        "  \"\"\"2-degree approx of relu in [-6,6] from https://arxiv.org/pdf/2009.03727.pdf\"\"\"\n",
        "  a = 0.563059\n",
        "  b = 0.5\n",
        "  c = 0.078047\n",
        "  x_2 = torch.square(x)\n",
        "  return a + b*x + c*x_2\n",
        "  \n",
        "def approx_relu_4d(x):\n",
        "  \"\"\"4-degree approx of relu in [-6,6] from https://arxiv.org/pdf/2009.03727.pdf\"\"\"\n",
        "  a = 0.119782\n",
        "  b = 0.5\n",
        "  c = 0.147298\n",
        "  d = -0.002015\n",
        "  x_2 = torch.square(x)\n",
        "  x_4 = torch.square(x_2)\n",
        "  return a + b*x + c*x_2 + d*x_4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrxhF5Msda_u"
      },
      "source": [
        "CryptoNet from [Microsoft](https://www.microsoft.com/en-us/research/publication/cryptonets-applying-neural-networks-to-encrypted-data-with-high-throughput-and-accuracy/) ==> couldn't replicate results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "bGKsu0dNuGEH",
        "outputId": "9821ebf2-0c7d-42a4-dc53-39d3c11c03b6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nclass ScaledAvgPool2d(nn.Module):\\n    \"\"\"Define the ScaledAvgPool layer, a.k.a the Sum Pool\"\"\"\\n    def __init__(self,kernel_size):\\n      super().__init__()\\n      self.kernel_size = kernel_size\\n      self.AvgPool = nn.AvgPool2d(kernel_size=self.kernel_size, stride=1, padding=int(math.ceil((kernel_size-1)/2)))\\n\\n    def forward(self,x):\\n      return (self.kernel_size**2)*self.AvgPool(x)\\n    \\n\\nclass CryptoNet(nn.Module):\\n  \"\"\"\\n    Original 9-layer network used during training\\n    CURRENTLY NOT WORKING\\n  \"\"\"\\n  def __init__(self, verbose):\\n    super().__init__()\\n    self.verbose = verbose\\n    self.pad = F.pad\\n    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\\n    self.square1 = torch.square\\n    self.scaledAvgPool1 = ScaledAvgPool2d(kernel_size=3)\\n    self.conv2 = nn.Conv2d(in_channels=5, out_channels=50, kernel_size=5, stride=2)\\n    self.scaledAvgPool2 = ScaledAvgPool2d(kernel_size=3)\\n    self.fc1 = nn.Linear(in_features=1250, out_features=100)\\n    self.square2 = torch.square\\n    self.fc2 = nn.Linear(in_features=100, out_features=10)\\n    self.sigmoid = nn.Sigmoid()\\n\\n  def forward(self, x):\\n    x = self.pad(x, (1,0,1,0))\\n    if self.verbose:\\n      print(\"Start --> \",x.mean())\\n    x = self.conv1(x)\\n    if self.verbose:\\n      print(\"Conv1 --> \",x.mean())\\n    x = self.square1(x)\\n    if self.verbose:\\n      print(\"Sq --> \",x.mean())\\n    x = self.scaledAvgPool1(x)\\n    if self.verbose:\\n      print(\"Pool --> \",x.mean())\\n    x = self.conv2(x)\\n    if self.verbose:\\n      print(\"Conv2 --> \",x.mean())\\n    x = self.scaledAvgPool2(x)\\n    if self.verbose:\\n      print(\"Pool --> \",x.mean())\\n    ## Flatten\\n    x = x.reshape(x.shape[0], -1)\\n    x = self.fc1(x)\\n    if self.verbose:\\n      print(\"fc1 --> \",x.mean())\\n    x = self.square2(x)\\n    if self.verbose:\\n      print(\"Square --> \",x.mean())\\n    x = self.fc2(x)\\n    if self.verbose:\\n      print(\"fc2 --> \",x.mean())\\n    x = self.sigmoid(x)\\n    return x\\n\\n  def weights_init(self, m):\\n    \"\"\" Custom initilization to avoid square activation to blow up \"\"\"\\n    for m in self.children():\\n      if isinstance(m,nn.Conv2d):\\n        nn.init.kaiming_uniform_(m.weight, a=0, mode=\\'fan_in\\', nonlinearity=\\'relu\\')\\n      elif isinstance(m, nn.Linear):\\n        nn.init.uniform_(m.weight, 1e-4,1e-3)\\n'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "class ScaledAvgPool2d(nn.Module):\n",
        "    \"\"\"Define the ScaledAvgPool layer, a.k.a the Sum Pool\"\"\"\n",
        "    def __init__(self,kernel_size):\n",
        "      super().__init__()\n",
        "      self.kernel_size = kernel_size\n",
        "      self.AvgPool = nn.AvgPool2d(kernel_size=self.kernel_size, stride=1, padding=int(math.ceil((kernel_size-1)/2)))\n",
        "\n",
        "    def forward(self,x):\n",
        "      return (self.kernel_size**2)*self.AvgPool(x)\n",
        "    \n",
        "\n",
        "class CryptoNet(nn.Module):\n",
        "  \"\"\"\n",
        "    Original 9-layer network used during training\n",
        "    CURRENTLY NOT WORKING\n",
        "  \"\"\"\n",
        "  def __init__(self, verbose):\n",
        "    super().__init__()\n",
        "    self.verbose = verbose\n",
        "    self.pad = F.pad\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\n",
        "    self.square1 = torch.square\n",
        "    self.scaledAvgPool1 = ScaledAvgPool2d(kernel_size=3)\n",
        "    self.conv2 = nn.Conv2d(in_channels=5, out_channels=50, kernel_size=5, stride=2)\n",
        "    self.scaledAvgPool2 = ScaledAvgPool2d(kernel_size=3)\n",
        "    self.fc1 = nn.Linear(in_features=1250, out_features=100)\n",
        "    self.square2 = torch.square\n",
        "    self.fc2 = nn.Linear(in_features=100, out_features=10)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pad(x, (1,0,1,0))\n",
        "    if self.verbose:\n",
        "      print(\"Start --> \",x.mean())\n",
        "    x = self.conv1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Conv1 --> \",x.mean())\n",
        "    x = self.square1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Sq --> \",x.mean())\n",
        "    x = self.scaledAvgPool1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Pool --> \",x.mean())\n",
        "    x = self.conv2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Conv2 --> \",x.mean())\n",
        "    x = self.scaledAvgPool2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Pool --> \",x.mean())\n",
        "    ## Flatten\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.fc1(x)\n",
        "    if self.verbose:\n",
        "      print(\"fc1 --> \",x.mean())\n",
        "    x = self.square2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Square --> \",x.mean())\n",
        "    x = self.fc2(x)\n",
        "    if self.verbose:\n",
        "      print(\"fc2 --> \",x.mean())\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "  def weights_init(self, m):\n",
        "    \"\"\" Custom initilization to avoid square activation to blow up \"\"\"\n",
        "    for m in self.children():\n",
        "      if isinstance(m,nn.Conv2d):\n",
        "        nn.init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        nn.init.uniform_(m.weight, 1e-4,1e-3)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tunwxIS3XLgm"
      },
      "outputs": [],
      "source": [
        "class SimpleNet(nn.Module):\n",
        "  '''\n",
        "    Simpliefied network used in paper for inference https://www.microsoft.com/en-us/research/publication/cryptonets-applying-neural-networks-to-encrypted-data-with-high-throughput-and-accuracy/\n",
        "  '''\n",
        "  def __init__(self, batch_size : int, activation : str, init_method : str, verbose : bool):\n",
        "    super().__init__()\n",
        "    self.verbose = verbose\n",
        "    self.init_method = init_method\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    if activation == \"square\":\n",
        "      self.activation = torch.square\n",
        "    elif activation == \"relu\":\n",
        "      self.activation = nn.ReLU()\n",
        "    elif activation == \"a-relu-2d\":\n",
        "      self.activation = approx_relu_2d\n",
        "    elif activation == \"a-relu-4d\":\n",
        "      self.activation = approx_relu_4d\n",
        "\n",
        "    self.pad = F.pad\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\n",
        "    self.pool1 = nn.Conv2d(in_channels=5, out_channels=100, kernel_size=13, stride=1000)\n",
        "    self.pool2 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(100,1), stride=1000)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pad(x, (1,0,1,0))\n",
        "    x = self.conv1(x)\n",
        "    x = self.activation(self.pool1(x))\n",
        "    x = x.reshape([self.batch_size,1,100,1]) #batch_size tensors in 1 channel, 100x1\n",
        "    x = self.activation(self.pool2(x))\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    return x\n",
        " \n",
        "  def weights_init(self, m):\n",
        "    for m in self.children():\n",
        "      if isinstance(m,nn.Conv2d):\n",
        "        if self.init_method == \"he\":\n",
        "          nn.init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "        elif self.init_method == \"xavier\":\n",
        "          nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2))\n",
        "        elif self.init_method == \"uniform\":\n",
        "          nn.init.uniform_(m.weight, -0.5, 0.5)\n",
        "        elif self.init_method == \"norm\":\n",
        "          nn.init.normal_(m.weight, 0.0, 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYTSG_NYkN8e"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sX-7JDDtHOo"
      },
      "source": [
        "Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3zxoMQRRsF1o"
      },
      "outputs": [],
      "source": [
        "class DataHandler():\n",
        "  def __init__(self, dataset : str, batch_size : int):\n",
        "    if dataset == \"MNIST\":\n",
        "      self.batch_size = batch_size\n",
        "      \n",
        "     \n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "      \n",
        "    train_ds = MNIST(\"data/\", train=True, download=True, transform=transform)\n",
        "    test_ds = MNIST(\"data/\", train=False, download=True, transform=transform)\n",
        "    self.train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle=True, drop_last=True,num_workers=2, pin_memory=True)\n",
        "    self.test_dl = DataLoader(test_ds, batch_size = batch_size, shuffle=True, drop_last=True,num_workers=2, pin_memory=True)\n",
        "\n",
        "dataHandler = DataHandler(dataset=\"MNIST\", batch_size=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXXqZm508qA1"
      },
      "source": [
        "Plot gradient flow for debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h3f19IYJ8nIQ"
      },
      "outputs": [],
      "source": [
        "def plot_grad_flow(named_parameters):\n",
        "    ## From https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063\n",
        "    ## Beware it's a little bit tricky to interpret results\n",
        "    '''Plots the gradients flowing through different layers in the net during training.\n",
        "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
        "    \n",
        "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
        "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
        "\n",
        "    ave_grads = []\n",
        "    max_grads = []\n",
        "    layers = []\n",
        "    for n, p in named_parameters:\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\n",
        "            layers.append(n)\n",
        "            ave_grads.append(p.grad.abs().mean())\n",
        "            max_grads.append(p.grad.abs().max())\n",
        "            print(f\"Layer {n}, grad avg {p.grad.mean()}, data {p.data.mean()}\")\n",
        "    plt.bar(np.arange(len(max_grads)), max(max_grads), alpha=0.1, lw=1, color=\"c\")\n",
        "    plt.bar(np.arange(len(max_grads)), np.mean(ave_grads), alpha=0.1, lw=1, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(left=0, right=len(ave_grads))\n",
        "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")\n",
        "    plt.grid(True)\n",
        "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
        "                Line2D([0], [0], color=\"b\", lw=4),\n",
        "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OsCh6ldz9PN"
      },
      "source": [
        "Train and test pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XfZdPxHKZwKi"
      },
      "outputs": [],
      "source": [
        "## training params setup\n",
        "learning_rate = 3e-4\n",
        "total_step = len(dataHandler.train_dl)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "class Logger():\n",
        "  def __init__(self, path=\"./logs\", name=\"SimpleNet\"):\n",
        "    self.path = path\n",
        "    self.name = name\n",
        "    self.log = []\n",
        "  \n",
        "  def log_step(self, epoch, step, loss, accuracy):\n",
        "    self.log.append(f\"[!] Training Epoch {epoch+1}, step {step+1} ==> loss {loss}, accuracy {accuracy}\")\n",
        "  \n",
        "  def log_batch(self, batch, loss, accuracy):\n",
        "    self.log.append(f\"[!] Test batch {batch+1} ==> loss {loss}, accuracy {accuracy}\")\n",
        "  \n",
        "  def finalize(self, test_loss, test_accuracy):\n",
        "    self.log.append(\"=================================\")\n",
        "    self.log.append(f\"[+] Average test Loss ==> {test_loss:.4f}\")\n",
        "    self.log.append(f\"[+] Test accuracy ==> {test_accuracy * 100:.2f}\")\n",
        "    with open(self.path+self.name+\"_log.txt\", \"w+\") as f:\n",
        "      f.write(\"\\n\".join(self.log))\n",
        "\n",
        "## PLOT HELPER\n",
        "def plot_history(key, train, history):\n",
        "  \"\"\" \n",
        "    Plot loss and accuracy history during model run\n",
        "    Input:\n",
        "          key : str => name of the model\n",
        "          train : bool => training 1 or test 0\n",
        "          history : dict{str : list of floats}\n",
        "  \"\"\"\n",
        "  if train:\n",
        "    when = \"train\"\n",
        "  else:\n",
        "    when = \"test\"\n",
        "  fig, ax = plt.subplots( 1, 2, figsize = (12,4) )\n",
        "  ax[0].plot(history['loss'], label = when+\"----\"+key)\n",
        "  ax[0].set_title( \"Loss\" )\n",
        "  ax[0].set_xlabel( \"Epochs\" )\n",
        "  ax[0].set_ylabel( \"Loss\" )\n",
        "  ax[0].grid( True )\n",
        "  ax[0].legend()\n",
        "\n",
        "  ax[1].plot(history['accuracy'], label = when+\"----\"+key)\n",
        "  ax[1].set_title( \"Accuracy\" )\n",
        "  ax[1].set_xlabel( \"Epochs\" )\n",
        "  ax[1].set_ylabel( \"Accuracy\" )\n",
        "  ax[1].grid( True )\n",
        "  ax[1].legend()\n",
        "\n",
        "  plt.savefig(f\"./images/{key}_{when}.png\")\n",
        "  plt.close()\n",
        "\n",
        "## TRAIN\n",
        "def train(logger, model, dataHandler, num_epochs, TPU=False):\n",
        "  num_epochs = num_epochs\n",
        "  model.train()\n",
        "  #optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  trainHistory = {}\n",
        "  trainHistory['loss'] = []\n",
        "  trainHistory['accuracy'] = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    for i, (data, labels) in enumerate(dataHandler.train_dl):\n",
        "      data = data.to(device=device)\n",
        "      labels = labels.to(device=device)\n",
        "      #labels = labels.to(torch.float32)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      predictions = model(data)\n",
        "      loss = criterion(predictions, labels)\n",
        "      loss.backward()\n",
        "      \n",
        "      if model.verbose:\n",
        "        print(f\"[?] Step {i+1} Epoch {epoch+1}\")\n",
        "        plot_grad_flow(model.named_parameters())\n",
        "      \n",
        "      if not TPU:\n",
        "        optimizer.step()\n",
        "      else:\n",
        "        xm.optimizer_step(optimizer, barrier=True) ## if TPU \n",
        "      \n",
        "      _, predicted_labels = predictions.max(1)\n",
        "      num_correct += (predicted_labels == labels).sum().item()\n",
        "      num_samples += predicted_labels.size(0)\n",
        "      \n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      if (i+1) % 100 == 0:\n",
        "        logger.log_step(epoch, i, epoch_loss/(i+1), num_correct/num_samples)\n",
        "      \n",
        "    trainHistory['loss'].append(loss.item())\n",
        "    trainHistory['accuracy'].append(num_correct/num_samples)\n",
        "    \n",
        "  plot_history(logger.name, True, trainHistory)\n",
        "\n",
        "\n",
        "## EVAL \n",
        "def eval(logger, model, dataHandler):\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "\n",
        "  model.eval()\n",
        "  testHistory = {}\n",
        "  testHistory['loss'] = []\n",
        "  testHistory['accuracy'] = []\n",
        "  test_loss = 0\n",
        "  test_accuracy = 0\n",
        "  with torch.no_grad():\n",
        "    for batch, (data,labels) in enumerate(dataHandler.test_dl):\n",
        "        data = data.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "        ## Forward Pass\n",
        "        predictions = model(data)\n",
        "        loss = criterion(predictions, labels).item()\n",
        "        test_loss += loss\n",
        "        _, predicted_labels = predictions.max(1)\n",
        "        num_correct += (predicted_labels == labels).sum().item()\n",
        "        num_samples += predicted_labels.size(0)\n",
        "        testHistory['loss'].append(loss)\n",
        "        testHistory['accuracy'].append(float(num_correct) / float(num_samples))\n",
        "        logger.log_batch(batch+1, loss, float(num_correct) / float(num_samples))\n",
        "    \n",
        "    test_accuracy = float(num_correct) / float(num_samples)\n",
        "    test_loss = test_loss/len(dataHandler.test_dl)\n",
        "    logger.finalize(test_loss, test_accuracy)\n",
        "\n",
        "    plot_history(logger.name, False, testHistory)\n",
        "\n",
        "  return test_loss, test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30gVJjuXdxc1"
      },
      "source": [
        "\n",
        "Training and evaluation of SimpleNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tn61Mth4dz-l",
        "outputId": "917579cf-bbc8-4371-b1c3-a799e53d6153"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-4c48620c3a2f>:38: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "  fig, ax = plt.subplots( 1, 2, figsize = (12,4) )\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b6c434933e5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./logs/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"SimpleNet_{key}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataHandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-4c48620c3a2f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(logger, model, dataHandler, num_epochs, TPU)\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "##############################\n",
        "#                            #\n",
        "# TRAINING AND EVAL PIPELINE #\n",
        "#                            #\n",
        "##############################\n",
        "\n",
        "## init models\n",
        "methods = [\"random\", \"he\", \"xavier\"]\n",
        "activations = [\"relu\", \"square\", \"a-relu-2d\", \"a-relu-4d\"]\n",
        "models = {}\n",
        "\n",
        "for method in methods:\n",
        "  for activation in activations:\n",
        "    models[method+\"_\"+activation] = SimpleNet(batch_size=dataHandler.batch_size, activation=activation, init_method=method,verbose=False).to(device=device)\n",
        "scores = {}\n",
        "\n",
        "## Testing of different stuff ==> result was best xavier+square\n",
        "for key, model in models.items():\n",
        "  logger = Logger(\"./logs/\",f\"SimpleNet_{key}\")\n",
        "  model.apply(model.weights_init)\n",
        "  train(logger, model, dataHandler, num_epochs=10)\n",
        "  loss, accuracy = eval(logger, model, dataHandler)\n",
        "  scores[key] = {\"loss\":loss, \"accuracy\":accuracy}\n",
        "  torch.save(model, f\"SimpleNet_{key}.pt\")\n",
        "\n",
        "## Best Model on 10 epochs\n",
        "#key = \"xavier_relu\"\n",
        "#model = models[key]\n",
        "#model.apply(model.weights_init)\n",
        "#train(key, model, dataHandler, num_epochs=150, TPU=False)\n",
        "#loss, accuracy = eval(key,model, dataHandler)\n",
        "#scores[key] = {\"loss\":loss, \"accuracy\":accuracy}\n",
        "#torch.save(model, f\"SimpleNet_{key}.pt\")\n",
        "\n",
        "for key, metrics in scores.items():\n",
        "  print(\"=====================================================================\")\n",
        "  print(f\"[+] Model with {key}: Avg test Loss ==> {metrics['loss']}, Accuracy ==> {metrics['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PAiAjR4u4gH"
      },
      "source": [
        "# Results of SimpleNet evaluation\n",
        "Best model seems to be the one with square activation and xavier initialization."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
