{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYmdRutcpdeR"
      },
      "source": [
        "# Implementation of (simplified) CryptoNet for inference under homomorphic encryption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "prOXZ9RESeYD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "from activation import relu_approx, sigmoid_approx\n",
        "from logger import Logger\n",
        "from dataHandler import DataHandler\n",
        "from utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrxhF5Msda_u"
      },
      "source": [
        "CryptoNet from [Microsoft](https://www.microsoft.com/en-us/research/publication/cryptonets-applying-neural-networks-to-encrypted-data-with-high-throughput-and-accuracy/) ==> couldn't replicate results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "bGKsu0dNuGEH",
        "outputId": "9821ebf2-0c7d-42a4-dc53-39d3c11c03b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nclass ScaledAvgPool2d(nn.Module):\\n    \"\"\"Define the ScaledAvgPool layer, a.k.a the Sum Pool\"\"\"\\n    def __init__(self,kernel_size):\\n      super().__init__()\\n      self.kernel_size = kernel_size\\n      self.AvgPool = nn.AvgPool2d(kernel_size=self.kernel_size, stride=1, padding=int(math.ceil((kernel_size-1)/2)))\\n\\n    def forward(self,x):\\n      return (self.kernel_size**2)*self.AvgPool(x)\\n    \\n\\nclass CryptoNet(nn.Module):\\n  \"\"\"\\n    Original 9-layer network used during training\\n    CURRENTLY NOT WORKING\\n  \"\"\"\\n  def __init__(self, verbose):\\n    super().__init__()\\n    self.verbose = verbose\\n    self.pad = F.pad\\n    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\\n    self.square1 = torch.square\\n    self.scaledAvgPool1 = ScaledAvgPool2d(kernel_size=3)\\n    self.conv2 = nn.Conv2d(in_channels=5, out_channels=50, kernel_size=5, stride=2)\\n    self.scaledAvgPool2 = ScaledAvgPool2d(kernel_size=3)\\n    self.fc1 = nn.Linear(in_features=1250, out_features=100)\\n    self.square2 = torch.square\\n    self.fc2 = nn.Linear(in_features=100, out_features=10)\\n    self.sigmoid = nn.Sigmoid()\\n\\n  def forward(self, x):\\n    x = self.pad(x, (1,0,1,0))\\n    if self.verbose:\\n      print(\"Start --> \",x.mean())\\n    x = self.conv1(x)\\n    if self.verbose:\\n      print(\"Conv1 --> \",x.mean())\\n    x = self.square1(x)\\n    if self.verbose:\\n      print(\"Sq --> \",x.mean())\\n    x = self.scaledAvgPool1(x)\\n    if self.verbose:\\n      print(\"Pool --> \",x.mean())\\n    x = self.conv2(x)\\n    if self.verbose:\\n      print(\"Conv2 --> \",x.mean())\\n    x = self.scaledAvgPool2(x)\\n    if self.verbose:\\n      print(\"Pool --> \",x.mean())\\n    ## Flatten\\n    x = x.reshape(x.shape[0], -1)\\n    x = self.fc1(x)\\n    if self.verbose:\\n      print(\"fc1 --> \",x.mean())\\n    x = self.square2(x)\\n    if self.verbose:\\n      print(\"Square --> \",x.mean())\\n    x = self.fc2(x)\\n    if self.verbose:\\n      print(\"fc2 --> \",x.mean())\\n    x = self.sigmoid(x)\\n    return x\\n\\n  def weights_init(self, m):\\n    \"\"\" Custom initilization to avoid square activation to blow up \"\"\"\\n    for m in self.children():\\n      if isinstance(m,nn.Conv2d):\\n        nn.init.kaiming_uniform_(m.weight, a=0, mode=\\'fan_in\\', nonlinearity=\\'relu\\')\\n      elif isinstance(m, nn.Linear):\\n        nn.init.uniform_(m.weight, 1e-4,1e-3)\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "class ScaledAvgPool2d(nn.Module):\n",
        "    \"\"\"Define the ScaledAvgPool layer, a.k.a the Sum Pool\"\"\"\n",
        "    def __init__(self,kernel_size):\n",
        "      super().__init__()\n",
        "      self.kernel_size = kernel_size\n",
        "      self.AvgPool = nn.AvgPool2d(kernel_size=self.kernel_size, stride=1, padding=int(math.ceil((kernel_size-1)/2)))\n",
        "\n",
        "    def forward(self,x):\n",
        "      return (self.kernel_size**2)*self.AvgPool(x)\n",
        "    \n",
        "\n",
        "class CryptoNet(nn.Module):\n",
        "  \"\"\"\n",
        "    Original 9-layer network used during training\n",
        "    CURRENTLY NOT WORKING\n",
        "  \"\"\"\n",
        "  def __init__(self, verbose):\n",
        "    super().__init__()\n",
        "    self.verbose = verbose\n",
        "    self.pad = F.pad\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\n",
        "    self.square1 = torch.square\n",
        "    self.scaledAvgPool1 = ScaledAvgPool2d(kernel_size=3)\n",
        "    self.conv2 = nn.Conv2d(in_channels=5, out_channels=50, kernel_size=5, stride=2)\n",
        "    self.scaledAvgPool2 = ScaledAvgPool2d(kernel_size=3)\n",
        "    self.fc1 = nn.Linear(in_features=1250, out_features=100)\n",
        "    self.square2 = torch.square\n",
        "    self.fc2 = nn.Linear(in_features=100, out_features=10)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pad(x, (1,0,1,0))\n",
        "    if self.verbose:\n",
        "      print(\"Start --> \",x.mean())\n",
        "    x = self.conv1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Conv1 --> \",x.mean())\n",
        "    x = self.square1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Sq --> \",x.mean())\n",
        "    x = self.scaledAvgPool1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Pool --> \",x.mean())\n",
        "    x = self.conv2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Conv2 --> \",x.mean())\n",
        "    x = self.scaledAvgPool2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Pool --> \",x.mean())\n",
        "    ## Flatten\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.fc1(x)\n",
        "    if self.verbose:\n",
        "      print(\"fc1 --> \",x.mean())\n",
        "    x = self.square2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Square --> \",x.mean())\n",
        "    x = self.fc2(x)\n",
        "    if self.verbose:\n",
        "      print(\"fc2 --> \",x.mean())\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "  def weights_init(self, m):\n",
        "    \"\"\" Custom initilization to avoid square activation to blow up \"\"\"\n",
        "    for m in self.children():\n",
        "      if isinstance(m,nn.Conv2d):\n",
        "        nn.init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        nn.init.uniform_(m.weight, 1e-4,1e-3)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tunwxIS3XLgm"
      },
      "outputs": [],
      "source": [
        "class SimpleNet(nn.Module):\n",
        "  '''\n",
        "    Simpliefied network used in paper for inference https://www.microsoft.com/en-us/research/publication/cryptonets-applying-neural-networks-to-encrypted-data-with-high-throughput-and-accuracy/\n",
        "  '''\n",
        "  def __init__(self, batch_size : int, activation : str, init_method : str, verbose : bool, sigmoid : bool):\n",
        "    super().__init__()\n",
        "    self.verbose = verbose\n",
        "    self.init_method = init_method\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    if activation == \"square\":\n",
        "      self.activation = torch.square\n",
        "    elif activation == \"relu\":\n",
        "      self.activation = nn.ReLU()\n",
        "    elif activation == \"relu_approx\":\n",
        "      self.activation = relu_approx\n",
        "\n",
        "    if sigmoid:\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "    else:\n",
        "      self.sigmoid = sigmoid_approx\n",
        "\n",
        "    self.pad = F.pad\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\n",
        "    self.pool1 = nn.Conv2d(in_channels=5, out_channels=100, kernel_size=13, stride=1000)\n",
        "    self.pool2 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(100,1), stride=1000)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pad(x, (1,0,1,0))\n",
        "    x = self.conv1(x)\n",
        "    x = self.activation(self.pool1(x))\n",
        "    #print(x[0])\n",
        "    x = x.reshape([self.batch_size,1,100,1]) #batch_size tensors in 1 channel, 100x1\n",
        "    x = self.activation(self.pool2(x))\n",
        "    #print(x[0])\n",
        "    x = self.sigmoid(x) ##needed for the probabilities\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    return x\n",
        " \n",
        "  def weights_init(self, m):\n",
        "    for m in self.children():\n",
        "      if isinstance(m,nn.Conv2d):\n",
        "        if self.init_method == \"he\":\n",
        "          nn.init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "        elif self.init_method == \"xavier\":\n",
        "          nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2))\n",
        "        elif self.init_method == \"uniform\":\n",
        "          nn.init.uniform_(m.weight, -0.5, 0.5)\n",
        "        elif self.init_method == \"norm\":\n",
        "          nn.init.normal_(m.weight, 0.0, 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sX-7JDDtHOo"
      },
      "source": [
        "Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3zxoMQRRsF1o"
      },
      "outputs": [],
      "source": [
        "dataHandler = DataHandler(dataset=\"MNIST\", batch_size=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OsCh6ldz9PN"
      },
      "source": [
        "Train and test pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tn61Mth4dz-l",
        "outputId": "917579cf-bbc8-4371-b1c3-a799e53d6153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[?] SimpleNet_xavier_relu_approx Epoch 1/150 Loss 0.1953\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 2/150 Loss 0.1952\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 3/150 Loss 0.1952\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 4/150 Loss 0.1952\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 5/150 Loss 0.1952\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 6/150 Loss 0.1952\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 7/150 Loss 0.1952\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 8/150 Loss 0.1949\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 9/150 Loss 0.1940\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 10/150 Loss 0.1926\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 11/150 Loss 0.1920\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 12/150 Loss 0.1909\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 13/150 Loss 0.1882\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 14/150 Loss 0.1885\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 15/150 Loss 0.1850\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 16/150 Loss 0.1828\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 17/150 Loss 0.1798\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 18/150 Loss 0.1796\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 19/150 Loss 0.1787\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 20/150 Loss 0.1782\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 21/150 Loss 0.1772\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 22/150 Loss 0.1744\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 23/150 Loss 0.1744\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 24/150 Loss 0.1768\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 25/150 Loss 0.1747\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 26/150 Loss 0.1733\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 27/150 Loss 0.1743\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 28/150 Loss 0.1734\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 29/150 Loss 0.1742\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 30/150 Loss 0.1729\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 31/150 Loss 0.1722\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 32/150 Loss 0.1736\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 33/150 Loss 0.1728\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 34/150 Loss 0.1730\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 35/150 Loss 0.1727\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 36/150 Loss 0.1739\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 37/150 Loss 0.1726\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 38/150 Loss 0.1726\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 39/150 Loss 0.1712\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 40/150 Loss 0.1724\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 41/150 Loss 0.1711\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 42/150 Loss 0.1719\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 43/150 Loss 0.1715\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 44/150 Loss 0.1728\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 45/150 Loss 0.1714\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 46/150 Loss 0.1702\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 47/150 Loss 0.1715\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 48/150 Loss 0.1708\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 49/150 Loss 0.1705\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 50/150 Loss 0.1681\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 51/150 Loss 0.1708\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 52/150 Loss 0.1701\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 53/150 Loss 0.1719\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 54/150 Loss 0.1711\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 55/150 Loss 0.1708\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 56/150 Loss 0.1712\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 57/150 Loss 0.1698\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 58/150 Loss 0.1711\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 59/150 Loss 0.1711\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 60/150 Loss 0.1693\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 61/150 Loss 0.1707\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 62/150 Loss 0.1712\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 63/150 Loss 0.1703\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 64/150 Loss 0.1693\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 65/150 Loss 0.1701\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 66/150 Loss 0.1707\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 67/150 Loss 0.1719\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 68/150 Loss 0.1710\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 69/150 Loss 0.1702\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 70/150 Loss 0.1706\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 71/150 Loss 0.1689\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 72/150 Loss 0.1700\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 73/150 Loss 0.1711\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 74/150 Loss 0.1721\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 75/150 Loss 0.1712\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 76/150 Loss 0.1698\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 77/150 Loss 0.1692\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 78/150 Loss 0.1702\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 79/150 Loss 0.1698\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 80/150 Loss 0.1715\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 81/150 Loss 0.1714\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 82/150 Loss 0.1706\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 83/150 Loss 0.1707\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 84/150 Loss 0.1699\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 85/150 Loss 0.1705\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 86/150 Loss 0.1703\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 87/150 Loss 0.1702\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 88/150 Loss 0.1712\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 89/150 Loss 0.1711\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 90/150 Loss 0.1699\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 91/150 Loss 0.1703\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 92/150 Loss 0.1704\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 93/150 Loss 0.1705\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 94/150 Loss 0.1692\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 95/150 Loss 0.1698\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 96/150 Loss 0.1697\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 97/150 Loss 0.1695\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 98/150 Loss 0.1720\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 99/150 Loss 0.1703\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 100/150 Loss 0.1699\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 101/150 Loss 0.1699\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 102/150 Loss 0.1698\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 103/150 Loss 0.1699\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 104/150 Loss 0.1699\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 105/150 Loss 0.1707\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 106/150 Loss 0.1697\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 107/150 Loss 0.1697\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 108/150 Loss 0.1692\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 109/150 Loss 0.1700\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 110/150 Loss 0.1692\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 111/150 Loss 0.1695\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 112/150 Loss 0.1712\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 113/150 Loss 0.1701\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 114/150 Loss 0.1697\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 115/150 Loss 0.1700\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 116/150 Loss 0.1705\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 117/150 Loss 0.1698\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 118/150 Loss 0.1705\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 119/150 Loss 0.1695\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 120/150 Loss 0.1706\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 121/150 Loss 0.1711\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 122/150 Loss 0.1702\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 123/150 Loss 0.1688\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 124/150 Loss 0.1710\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 125/150 Loss 0.1706\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 126/150 Loss 0.1688\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 127/150 Loss 0.1699\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 128/150 Loss 0.1705\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 129/150 Loss 0.1708\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 130/150 Loss 0.1691\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 131/150 Loss 0.1704\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 132/150 Loss 0.1709\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 133/150 Loss 0.1686\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 134/150 Loss 0.1699\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 135/150 Loss 0.1699\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 136/150 Loss 0.1695\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 137/150 Loss 0.1701\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 138/150 Loss 0.1711\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 139/150 Loss 0.1708\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 140/150 Loss 0.1692\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 141/150 Loss 0.1706\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 142/150 Loss 0.1695\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 143/150 Loss 0.1696\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 144/150 Loss 0.1705\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 145/150 Loss 0.1694\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 146/150 Loss 0.1705\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 147/150 Loss 0.1697\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 148/150 Loss 0.1699\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 149/150 Loss 0.1686\n",
            "[?] SimpleNet_xavier_relu_approx Epoch 150/150 Loss 0.1692\n",
            "[?] SimpleNet_xavier_square Epoch 1/150 Loss 0.2485\n",
            "[?] SimpleNet_xavier_square Epoch 2/150 Loss 0.2439\n",
            "[?] SimpleNet_xavier_square Epoch 3/150 Loss 0.2382\n",
            "[?] SimpleNet_xavier_square Epoch 4/150 Loss 0.2362\n",
            "[?] SimpleNet_xavier_square Epoch 5/150 Loss 0.2350\n",
            "[?] SimpleNet_xavier_square Epoch 6/150 Loss 0.2345\n",
            "[?] SimpleNet_xavier_square Epoch 7/150 Loss 0.2345\n",
            "[?] SimpleNet_xavier_square Epoch 8/150 Loss 0.2343\n",
            "[?] SimpleNet_xavier_square Epoch 9/150 Loss 0.2336\n",
            "[?] SimpleNet_xavier_square Epoch 10/150 Loss 0.2332\n",
            "[?] SimpleNet_xavier_square Epoch 11/150 Loss 0.2334\n",
            "[?] SimpleNet_xavier_square Epoch 12/150 Loss 0.2329\n",
            "[?] SimpleNet_xavier_square Epoch 13/150 Loss 0.2334\n",
            "[?] SimpleNet_xavier_square Epoch 14/150 Loss 0.2339\n",
            "[?] SimpleNet_xavier_square Epoch 15/150 Loss 0.2332\n",
            "[?] SimpleNet_xavier_square Epoch 16/150 Loss 0.2324\n",
            "[?] SimpleNet_xavier_square Epoch 17/150 Loss 0.2334\n",
            "[?] SimpleNet_xavier_square Epoch 18/150 Loss 0.2321\n",
            "[?] SimpleNet_xavier_square Epoch 19/150 Loss 0.2318\n",
            "[?] SimpleNet_xavier_square Epoch 20/150 Loss 0.2327\n",
            "[?] SimpleNet_xavier_square Epoch 21/150 Loss 0.2321\n",
            "[?] SimpleNet_xavier_square Epoch 22/150 Loss 0.2330\n",
            "[?] SimpleNet_xavier_square Epoch 23/150 Loss 0.2335\n",
            "[?] SimpleNet_xavier_square Epoch 24/150 Loss 0.2327\n",
            "[?] SimpleNet_xavier_square Epoch 25/150 Loss 0.2335\n",
            "[?] SimpleNet_xavier_square Epoch 26/150 Loss 0.2316\n",
            "[?] SimpleNet_xavier_square Epoch 27/150 Loss 0.2334\n",
            "[?] SimpleNet_xavier_square Epoch 28/150 Loss 0.2330\n",
            "[?] SimpleNet_xavier_square Epoch 29/150 Loss 0.2316\n",
            "[?] SimpleNet_xavier_square Epoch 30/150 Loss 0.2313\n",
            "[?] SimpleNet_xavier_square Epoch 31/150 Loss 0.2322\n",
            "[?] SimpleNet_xavier_square Epoch 32/150 Loss 0.2309\n",
            "[?] SimpleNet_xavier_square Epoch 33/150 Loss 0.2323\n",
            "[?] SimpleNet_xavier_square Epoch 34/150 Loss 0.2318\n",
            "[?] SimpleNet_xavier_square Epoch 35/150 Loss 0.2316\n",
            "[?] SimpleNet_xavier_square Epoch 36/150 Loss 0.2306\n",
            "[?] SimpleNet_xavier_square Epoch 37/150 Loss 0.2321\n",
            "[?] SimpleNet_xavier_square Epoch 38/150 Loss 0.2327\n",
            "[?] SimpleNet_xavier_square Epoch 39/150 Loss 0.2330\n",
            "[?] SimpleNet_xavier_square Epoch 40/150 Loss 0.2320\n",
            "[?] SimpleNet_xavier_square Epoch 41/150 Loss 0.2320\n",
            "[?] SimpleNet_xavier_square Epoch 42/150 Loss 0.2315\n",
            "[?] SimpleNet_xavier_square Epoch 43/150 Loss 0.2324\n",
            "[?] SimpleNet_xavier_square Epoch 44/150 Loss 0.2321\n",
            "[?] SimpleNet_xavier_square Epoch 45/150 Loss 0.2314\n",
            "[?] SimpleNet_xavier_square Epoch 46/150 Loss 0.2317\n",
            "[?] SimpleNet_xavier_square Epoch 47/150 Loss 0.2322\n",
            "[?] SimpleNet_xavier_square Epoch 48/150 Loss 0.2318\n",
            "[?] SimpleNet_xavier_square Epoch 49/150 Loss 0.2315\n",
            "[?] SimpleNet_xavier_square Epoch 50/150 Loss 0.2313\n",
            "[?] SimpleNet_xavier_square Epoch 51/150 Loss 0.2302\n",
            "[?] SimpleNet_xavier_square Epoch 52/150 Loss 0.2317\n",
            "[?] SimpleNet_xavier_square Epoch 53/150 Loss 0.2309\n",
            "[?] SimpleNet_xavier_square Epoch 54/150 Loss 0.2315\n",
            "[?] SimpleNet_xavier_square Epoch 55/150 Loss 0.2318\n",
            "[?] SimpleNet_xavier_square Epoch 56/150 Loss 0.2319\n",
            "[?] SimpleNet_xavier_square Epoch 57/150 Loss 0.2324\n",
            "[?] SimpleNet_xavier_square Epoch 58/150 Loss 0.2313\n",
            "[?] SimpleNet_xavier_square Epoch 59/150 Loss 0.2313\n",
            "[?] SimpleNet_xavier_square Epoch 60/150 Loss 0.2319\n",
            "[?] SimpleNet_xavier_square Epoch 61/150 Loss 0.2300\n",
            "[?] SimpleNet_xavier_square Epoch 62/150 Loss 0.2318\n",
            "[?] SimpleNet_xavier_square Epoch 63/150 Loss 0.2311\n",
            "[?] SimpleNet_xavier_square Epoch 64/150 Loss 0.2313\n",
            "[?] SimpleNet_xavier_square Epoch 65/150 Loss 0.2313\n",
            "[?] SimpleNet_xavier_square Epoch 66/150 Loss 0.2315\n",
            "[?] SimpleNet_xavier_square Epoch 67/150 Loss 0.2316\n",
            "[?] SimpleNet_xavier_square Epoch 68/150 Loss 0.2307\n",
            "[?] SimpleNet_xavier_square Epoch 69/150 Loss 0.2320\n",
            "[?] SimpleNet_xavier_square Epoch 70/150 Loss 0.2323\n",
            "[?] SimpleNet_xavier_square Epoch 71/150 Loss 0.2311\n",
            "[?] SimpleNet_xavier_square Epoch 72/150 Loss 0.2313\n",
            "[?] SimpleNet_xavier_square Epoch 73/150 Loss 0.2314\n",
            "[?] SimpleNet_xavier_square Epoch 74/150 Loss 0.2308\n",
            "[?] SimpleNet_xavier_square Epoch 75/150 Loss 0.2320\n",
            "[?] SimpleNet_xavier_square Epoch 76/150 Loss 0.2315\n",
            "[?] SimpleNet_xavier_square Epoch 77/150 Loss 0.2311\n",
            "[?] SimpleNet_xavier_square Epoch 78/150 Loss 0.2312\n",
            "[?] SimpleNet_xavier_square Epoch 79/150 Loss 0.2303\n",
            "[?] SimpleNet_xavier_square Epoch 80/150 Loss 0.2318\n",
            "[?] SimpleNet_xavier_square Epoch 81/150 Loss 0.2314\n",
            "[?] SimpleNet_xavier_square Epoch 82/150 Loss 0.2305\n",
            "[?] SimpleNet_xavier_square Epoch 83/150 Loss 0.2320\n",
            "[?] SimpleNet_xavier_square Epoch 84/150 Loss 0.2311\n",
            "[?] SimpleNet_xavier_square Epoch 85/150 Loss 0.2315\n",
            "[?] SimpleNet_xavier_square Epoch 86/150 Loss 0.2314\n",
            "[?] SimpleNet_xavier_square Epoch 87/150 Loss 0.2310\n",
            "[?] SimpleNet_xavier_square Epoch 88/150 Loss 0.2310\n",
            "[?] SimpleNet_xavier_square Epoch 89/150 Loss 0.2309\n",
            "[?] SimpleNet_xavier_square Epoch 90/150 Loss 0.2313\n",
            "[?] SimpleNet_xavier_square Epoch 91/150 Loss 0.2316\n",
            "[?] SimpleNet_xavier_square Epoch 92/150 Loss 0.2307\n",
            "[?] SimpleNet_xavier_square Epoch 93/150 Loss 0.2307\n",
            "[?] SimpleNet_xavier_square Epoch 94/150 Loss 0.2324\n",
            "[?] SimpleNet_xavier_square Epoch 95/150 Loss 0.2314\n",
            "[?] SimpleNet_xavier_square Epoch 96/150 Loss 0.2308\n",
            "[?] SimpleNet_xavier_square Epoch 97/150 Loss 0.2308\n",
            "[?] SimpleNet_xavier_square Epoch 98/150 Loss 0.2320\n",
            "[?] SimpleNet_xavier_square Epoch 99/150 Loss 0.2316\n",
            "[?] SimpleNet_xavier_square Epoch 100/150 Loss 0.2314\n",
            "[?] SimpleNet_xavier_square Epoch 101/150 Loss 0.2311\n",
            "[?] SimpleNet_xavier_square Epoch 102/150 Loss 0.2318\n",
            "[?] SimpleNet_xavier_square Epoch 103/150 Loss 0.2306\n",
            "[?] SimpleNet_xavier_square Epoch 104/150 Loss 0.2309\n",
            "[?] SimpleNet_xavier_square Epoch 105/150 Loss 0.2305\n",
            "[?] SimpleNet_xavier_square Epoch 106/150 Loss 0.2310\n",
            "[?] SimpleNet_xavier_square Epoch 107/150 Loss 0.2316\n",
            "[?] SimpleNet_xavier_square Epoch 108/150 Loss 0.2312\n",
            "[?] SimpleNet_xavier_square Epoch 109/150 Loss 0.2310\n",
            "[?] SimpleNet_xavier_square Epoch 110/150 Loss 0.2310\n",
            "[?] SimpleNet_xavier_square Epoch 111/150 Loss 0.2313\n",
            "[?] SimpleNet_xavier_square Epoch 112/150 Loss 0.2307\n",
            "[?] SimpleNet_xavier_square Epoch 113/150 Loss 0.2312\n",
            "[?] SimpleNet_xavier_square Epoch 114/150 Loss 0.2316\n",
            "[?] SimpleNet_xavier_square Epoch 115/150 Loss 0.2314\n",
            "[?] SimpleNet_xavier_square Epoch 116/150 Loss 0.2308\n",
            "[?] SimpleNet_xavier_square Epoch 117/150 Loss 0.2309\n",
            "[?] SimpleNet_xavier_square Epoch 118/150 Loss 0.2309\n",
            "[?] SimpleNet_xavier_square Epoch 119/150 Loss 0.2312\n",
            "[?] SimpleNet_xavier_square Epoch 120/150 Loss 0.2307\n",
            "[?] SimpleNet_xavier_square Epoch 121/150 Loss 0.2306\n",
            "[?] SimpleNet_xavier_square Epoch 122/150 Loss 0.2314\n",
            "[?] SimpleNet_xavier_square Epoch 123/150 Loss 0.2306\n",
            "[?] SimpleNet_xavier_square Epoch 124/150 Loss 0.2325\n",
            "[?] SimpleNet_xavier_square Epoch 125/150 Loss 0.2311\n",
            "[?] SimpleNet_xavier_square Epoch 126/150 Loss 0.2311\n",
            "[?] SimpleNet_xavier_square Epoch 127/150 Loss 0.2312\n",
            "[?] SimpleNet_xavier_square Epoch 128/150 Loss 0.2320\n",
            "[?] SimpleNet_xavier_square Epoch 129/150 Loss 0.2322\n",
            "[?] SimpleNet_xavier_square Epoch 130/150 Loss 0.2308\n",
            "[?] SimpleNet_xavier_square Epoch 131/150 Loss 0.2312\n",
            "[?] SimpleNet_xavier_square Epoch 132/150 Loss 0.2308\n",
            "[?] SimpleNet_xavier_square Epoch 133/150 Loss 0.2307\n",
            "[?] SimpleNet_xavier_square Epoch 134/150 Loss 0.2306\n",
            "[?] SimpleNet_xavier_square Epoch 135/150 Loss 0.2312\n",
            "[?] SimpleNet_xavier_square Epoch 136/150 Loss 0.2312\n",
            "[?] SimpleNet_xavier_square Epoch 137/150 Loss 0.2314\n",
            "[?] SimpleNet_xavier_square Epoch 138/150 Loss 0.2308\n",
            "[?] SimpleNet_xavier_square Epoch 139/150 Loss 0.2314\n",
            "[?] SimpleNet_xavier_square Epoch 140/150 Loss 0.2314\n",
            "[?] SimpleNet_xavier_square Epoch 141/150 Loss 0.2315\n",
            "[?] SimpleNet_xavier_square Epoch 142/150 Loss 0.2319\n",
            "[?] SimpleNet_xavier_square Epoch 143/150 Loss 0.2320\n",
            "[?] SimpleNet_xavier_square Epoch 144/150 Loss 0.2314\n",
            "[?] SimpleNet_xavier_square Epoch 145/150 Loss 0.2307\n",
            "[?] SimpleNet_xavier_square Epoch 146/150 Loss 0.2310\n",
            "[?] SimpleNet_xavier_square Epoch 147/150 Loss 0.2313\n",
            "[?] SimpleNet_xavier_square Epoch 148/150 Loss 0.2315\n",
            "[?] SimpleNet_xavier_square Epoch 149/150 Loss 0.2306\n",
            "[?] SimpleNet_xavier_square Epoch 150/150 Loss 0.2303\n",
            "[?] SimpleNet_random_relu_approx Epoch 1/150 Loss 0.1953\n",
            "[?] SimpleNet_random_relu_approx Epoch 2/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 3/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 4/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 5/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 6/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 7/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 8/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 9/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 10/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 11/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 12/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 13/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 14/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 15/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 16/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 17/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 18/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 19/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 20/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 21/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 22/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 23/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 24/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 25/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 26/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 27/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 28/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 29/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 30/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 31/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 32/150 Loss 0.1952\n",
            "[?] SimpleNet_random_relu_approx Epoch 33/150 Loss 0.1951\n",
            "[?] SimpleNet_random_relu_approx Epoch 34/150 Loss 0.1951\n",
            "[?] SimpleNet_random_relu_approx Epoch 35/150 Loss 0.1946\n",
            "[?] SimpleNet_random_relu_approx Epoch 36/150 Loss 0.1932\n",
            "[?] SimpleNet_random_relu_approx Epoch 37/150 Loss 0.1931\n",
            "[?] SimpleNet_random_relu_approx Epoch 38/150 Loss 0.1910\n",
            "[?] SimpleNet_random_relu_approx Epoch 39/150 Loss 0.1892\n",
            "[?] SimpleNet_random_relu_approx Epoch 40/150 Loss 0.1885\n",
            "[?] SimpleNet_random_relu_approx Epoch 41/150 Loss 0.1855\n",
            "[?] SimpleNet_random_relu_approx Epoch 42/150 Loss 0.1854\n",
            "[?] SimpleNet_random_relu_approx Epoch 43/150 Loss 0.1832\n",
            "[?] SimpleNet_random_relu_approx Epoch 44/150 Loss 0.1820\n",
            "[?] SimpleNet_random_relu_approx Epoch 45/150 Loss 0.1778\n",
            "[?] SimpleNet_random_relu_approx Epoch 46/150 Loss 0.1775\n",
            "[?] SimpleNet_random_relu_approx Epoch 47/150 Loss 0.1770\n",
            "[?] SimpleNet_random_relu_approx Epoch 48/150 Loss 0.1757\n",
            "[?] SimpleNet_random_relu_approx Epoch 49/150 Loss 0.1742\n",
            "[?] SimpleNet_random_relu_approx Epoch 50/150 Loss 0.1767\n",
            "[?] SimpleNet_random_relu_approx Epoch 51/150 Loss 0.1762\n",
            "[?] SimpleNet_random_relu_approx Epoch 52/150 Loss 0.1740\n",
            "[?] SimpleNet_random_relu_approx Epoch 53/150 Loss 0.1734\n",
            "[?] SimpleNet_random_relu_approx Epoch 54/150 Loss 0.1719\n",
            "[?] SimpleNet_random_relu_approx Epoch 55/150 Loss 0.1741\n",
            "[?] SimpleNet_random_relu_approx Epoch 56/150 Loss 0.1735\n",
            "[?] SimpleNet_random_relu_approx Epoch 57/150 Loss 0.1740\n",
            "[?] SimpleNet_random_relu_approx Epoch 58/150 Loss 0.1732\n",
            "[?] SimpleNet_random_relu_approx Epoch 59/150 Loss 0.1714\n",
            "[?] SimpleNet_random_relu_approx Epoch 60/150 Loss 0.1731\n",
            "[?] SimpleNet_random_relu_approx Epoch 61/150 Loss 0.1731\n",
            "[?] SimpleNet_random_relu_approx Epoch 62/150 Loss 0.1714\n",
            "[?] SimpleNet_random_relu_approx Epoch 63/150 Loss 0.1733\n",
            "[?] SimpleNet_random_relu_approx Epoch 64/150 Loss 0.1719\n",
            "[?] SimpleNet_random_relu_approx Epoch 65/150 Loss 0.1720\n",
            "[?] SimpleNet_random_relu_approx Epoch 66/150 Loss 0.1722\n",
            "[?] SimpleNet_random_relu_approx Epoch 67/150 Loss 0.1729\n",
            "[?] SimpleNet_random_relu_approx Epoch 68/150 Loss 0.1728\n",
            "[?] SimpleNet_random_relu_approx Epoch 69/150 Loss 0.1714\n",
            "[?] SimpleNet_random_relu_approx Epoch 70/150 Loss 0.1717\n",
            "[?] SimpleNet_random_relu_approx Epoch 71/150 Loss 0.1713\n",
            "[?] SimpleNet_random_relu_approx Epoch 72/150 Loss 0.1713\n",
            "[?] SimpleNet_random_relu_approx Epoch 73/150 Loss 0.1714\n",
            "[?] SimpleNet_random_relu_approx Epoch 74/150 Loss 0.1717\n",
            "[?] SimpleNet_random_relu_approx Epoch 75/150 Loss 0.1729\n",
            "[?] SimpleNet_random_relu_approx Epoch 76/150 Loss 0.1726\n",
            "[?] SimpleNet_random_relu_approx Epoch 77/150 Loss 0.1717\n",
            "[?] SimpleNet_random_relu_approx Epoch 78/150 Loss 0.1703\n",
            "[?] SimpleNet_random_relu_approx Epoch 79/150 Loss 0.1707\n",
            "[?] SimpleNet_random_relu_approx Epoch 80/150 Loss 0.1724\n",
            "[?] SimpleNet_random_relu_approx Epoch 81/150 Loss 0.1712\n",
            "[?] SimpleNet_random_relu_approx Epoch 82/150 Loss 0.1717\n",
            "[?] SimpleNet_random_relu_approx Epoch 83/150 Loss 0.1720\n",
            "[?] SimpleNet_random_relu_approx Epoch 84/150 Loss 0.1710\n",
            "[?] SimpleNet_random_relu_approx Epoch 85/150 Loss 0.1711\n",
            "[?] SimpleNet_random_relu_approx Epoch 86/150 Loss 0.1705\n",
            "[?] SimpleNet_random_relu_approx Epoch 87/150 Loss 0.1708\n",
            "[?] SimpleNet_random_relu_approx Epoch 88/150 Loss 0.1708\n",
            "[?] SimpleNet_random_relu_approx Epoch 89/150 Loss 0.1715\n",
            "[?] SimpleNet_random_relu_approx Epoch 90/150 Loss 0.1705\n",
            "[?] SimpleNet_random_relu_approx Epoch 91/150 Loss 0.1701\n",
            "[?] SimpleNet_random_relu_approx Epoch 92/150 Loss 0.1717\n",
            "[?] SimpleNet_random_relu_approx Epoch 93/150 Loss 0.1716\n",
            "[?] SimpleNet_random_relu_approx Epoch 94/150 Loss 0.1708\n",
            "[?] SimpleNet_random_relu_approx Epoch 95/150 Loss 0.1705\n",
            "[?] SimpleNet_random_relu_approx Epoch 96/150 Loss 0.1695\n",
            "[?] SimpleNet_random_relu_approx Epoch 97/150 Loss 0.1695\n",
            "[?] SimpleNet_random_relu_approx Epoch 98/150 Loss 0.1709\n",
            "[?] SimpleNet_random_relu_approx Epoch 99/150 Loss 0.1691\n",
            "[?] SimpleNet_random_relu_approx Epoch 100/150 Loss 0.1700\n",
            "[?] SimpleNet_random_relu_approx Epoch 101/150 Loss 0.1695\n",
            "[?] SimpleNet_random_relu_approx Epoch 102/150 Loss 0.1715\n",
            "[?] SimpleNet_random_relu_approx Epoch 103/150 Loss 0.1714\n",
            "[?] SimpleNet_random_relu_approx Epoch 104/150 Loss 0.1701\n",
            "[?] SimpleNet_random_relu_approx Epoch 105/150 Loss 0.1715\n",
            "[?] SimpleNet_random_relu_approx Epoch 106/150 Loss 0.1704\n",
            "[?] SimpleNet_random_relu_approx Epoch 107/150 Loss 0.1696\n",
            "[?] SimpleNet_random_relu_approx Epoch 108/150 Loss 0.1696\n",
            "[?] SimpleNet_random_relu_approx Epoch 109/150 Loss 0.1715\n",
            "[?] SimpleNet_random_relu_approx Epoch 110/150 Loss 0.1712\n",
            "[?] SimpleNet_random_relu_approx Epoch 111/150 Loss 0.1710\n",
            "[?] SimpleNet_random_relu_approx Epoch 112/150 Loss 0.1700\n",
            "[?] SimpleNet_random_relu_approx Epoch 113/150 Loss 0.1719\n"
          ]
        }
      ],
      "source": [
        "##############################\n",
        "#                            #\n",
        "# TRAINING AND EVAL PIPELINE #\n",
        "#                            #\n",
        "##############################\n",
        "\n",
        "## init models\n",
        "#methods = [\"he\", \"xavier\", \"random\"] ##he init blows up values with square\n",
        "methods = [\"xavier\",\"random\"]\n",
        "activations = [\"relu_approx\",\"square\"]#\"relu\"]\n",
        "models = {}\n",
        "sigmoid = True\n",
        "for method in methods:\n",
        "  for activation in activations:\n",
        "    models[method+\"_\"+activation] = SimpleNet(batch_size=dataHandler.batch_size,\n",
        "                                    activation=activation,\n",
        "                                    init_method=method,\n",
        "                                    verbose=False,\n",
        "                                    sigmoid=sigmoid).to(device=device)\n",
        "scores = {}\n",
        "\n",
        "## Testing of different stuff ==> result was best xavier+square\n",
        "for key, model in models.items():\n",
        "  if sigmoid:\n",
        "    logger = Logger(\"./logs/\",f\"SimpleNet_{key}_sigmoid\")\n",
        "  else:\n",
        "    logger = Logger(\"./logs/\",f\"SimpleNet_{key}_approx_sigmoid\")\n",
        "  model.apply(model.weights_init)\n",
        "  train(logger, model, dataHandler, num_epochs=150)\n",
        "  loss, accuracy = eval(logger, model, dataHandler)\n",
        "  scores[key] = {\"loss\":loss, \"accuracy\":accuracy}\n",
        "  if sigmoid:\n",
        "    torch.save(model, f\"SimpleNet_{key}_sigmoid.pt\")\n",
        "  else:\n",
        "    torch.save(model, f\"SimpleNet_{key}_approx_sigmoid.pt\")\n",
        "\n",
        "\n",
        "for key, metrics in scores.items():\n",
        "  print(\"=====================================================================\")\n",
        "  print(f\"[+] Model with {key}: Avg test Loss ==> {metrics['loss']}, Accuracy ==> {metrics['accuracy']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
