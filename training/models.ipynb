{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYmdRutcpdeR"
      },
      "source": [
        "# CryptoNet and AlexNet implementation and training on MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "prOXZ9RESeYD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import save_image\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUluBfyZuKld"
      },
      "source": [
        "Dummy model for testing training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "Ii4rDPKqVPYT",
        "outputId": "7d69ad62-2841-44dc-d803-e69d0ee7eaec"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n## DUMMY MODEL\\nclass CNN(nn.Module):\\n    def __init__(self):\\n        super(CNN, self).__init__()        \\n        self.conv1 = nn.Sequential(         \\n            nn.Conv2d(\\n                in_channels=1,              \\n                out_channels=16,            \\n                kernel_size=5,              \\n                stride=1,                   \\n                padding=2,                  \\n            ),                              \\n            nn.ReLU(),                      \\n            nn.MaxPool2d(kernel_size=2),    \\n        )\\n        self.conv2 = nn.Sequential(         \\n            nn.Conv2d(16, 32, 5, 1, 2),     \\n            nn.ReLU(),                      \\n            nn.MaxPool2d(2),                \\n        )        # fully connected layer, output 10 classes\\n        self.out = nn.Linear(32 * 7 * 7, 10)    \\n        \\n    def forward(self, x):\\n        x = self.conv1(x)\\n        x = self.conv2(x)        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\\n        x = x.view(x.size(0), -1)       \\n        output = self.out(x)\\n        return output  # return x for visualization\\n  '"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "## DUMMY MODEL\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()        \n",
        "        self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=1,              \n",
        "                out_channels=16,            \n",
        "                kernel_size=5,              \n",
        "                stride=1,                   \n",
        "                padding=2,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(kernel_size=2),    \n",
        "        )\n",
        "        self.conv2 = nn.Sequential(         \n",
        "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(2),                \n",
        "        )        # fully connected layer, output 10 classes\n",
        "        self.out = nn.Linear(32 * 7 * 7, 10)    \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "        x = x.view(x.size(0), -1)       \n",
        "        output = self.out(x)\n",
        "        return output  # return x for visualization\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrxhF5Msda_u"
      },
      "source": [
        "CryptoNet from [Microsoft](https://www.microsoft.com/en-us/research/publication/cryptonets-applying-neural-networks-to-encrypted-data-with-high-throughput-and-accuracy/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bGKsu0dNuGEH"
      },
      "outputs": [],
      "source": [
        "class ScaledAvgPool2d(nn.Module):\n",
        "    \"\"\"Define the ScaledAvgPool layer, a.k.a the Sum Pool\"\"\"\n",
        "    def __init__(self,kernel_size):\n",
        "      super().__init__()\n",
        "      self.kernel_size = kernel_size\n",
        "      self.AvgPool = nn.AvgPool2d(kernel_size=self.kernel_size, stride=1, padding=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "      return (self.kernel_size**2)*self.AvgPool(x)\n",
        "    \n",
        "\n",
        "class CryptoNet(nn.Module):\n",
        "  '''\n",
        "    Original 9-layer network used during training\n",
        "    CURRENTLY NOT WORKING\n",
        "  '''\n",
        "  def __init__(self, verbose):\n",
        "    super().__init__()\n",
        "    self.verbose = verbose\n",
        "    self.pad = F.pad\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\n",
        "    self.square1 = torch.square\n",
        "    self.scaledAvgPool1 = ScaledAvgPool2d(kernel_size=3)\n",
        "    self.conv2 = nn.Conv2d(in_channels=5, out_channels=50, kernel_size=5, stride=2)\n",
        "    self.scaledAvgPool2 = ScaledAvgPool2d(kernel_size=3)\n",
        "    self.fc1 = nn.Linear(in_features=1250, out_features=100)\n",
        "    self.square2 = torch.square\n",
        "    self.fc2 = nn.Linear(in_features=100, out_features=10)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pad(x, (1,0,1,0))\n",
        "    if self.verbose:\n",
        "      print(\"Start --> \",x.mean())\n",
        "    x = self.conv1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Conv1 --> \",x.mean())\n",
        "    x = self.square1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Sq --> \",x.mean())\n",
        "    x = self.scaledAvgPool1(x)\n",
        "    if self.verbose:\n",
        "      print(\"Pool --> \",x.mean())\n",
        "    x = self.conv2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Conv2 --> \",x.mean())\n",
        "    x = self.scaledAvgPool2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Pool --> \",x.mean())\n",
        "    ## Flatten\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.fc1(x)\n",
        "    if self.verbose:\n",
        "      print(\"fc1 --> \",x.mean())\n",
        "    x = self.square2(x)\n",
        "    if self.verbose:\n",
        "      print(\"Square --> \",x.mean())\n",
        "    x = self.fc2(x)\n",
        "    if self.verbose:\n",
        "      print(\"fc2 --> \",x.mean())\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "  def weights_init(self, m):\n",
        "    \"\"\" Custom initilization to avoid square activation to blow up \"\"\"\n",
        "    for m in self.children():\n",
        "      if isinstance(m,nn.Conv2d):\n",
        "        nn.init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        nn.init.uniform_(m.weight, 1e-4,1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "tunwxIS3XLgm"
      },
      "outputs": [],
      "source": [
        "class SimpleNet(nn.Module):\n",
        "  '''\n",
        "    Simpliefied network used in paper for inference https://www.microsoft.com/en-us/research/publication/cryptonets-applying-neural-networks-to-encrypted-data-with-high-throughput-and-accuracy/\n",
        "  '''\n",
        "  def __init__(self, batch_size : int, init_method : str, verbose : bool):\n",
        "    super().__init__()\n",
        "    self.verbose = verbose\n",
        "    self.init_method = init_method\n",
        "    self.batch_size = batch_size\n",
        "    self.pad = F.pad\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, stride=2)\n",
        "    self.square1 = torch.square\n",
        "    self.pool1 = nn.Conv2d(in_channels=5, out_channels=100, kernel_size=13, stride=1000)\n",
        "    self.square2 = torch.square\n",
        "    self.pool2 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(100,1), stride=1000)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pad(x, (1,0,1,0))\n",
        "    x = self.conv1(x)\n",
        "    x = self.square1(self.pool1(x))\n",
        "    x = x.reshape([self.batch_size,1,100,1]) #batch_size tensors in 1 channel, 100x1\n",
        "    x = self.square2(self.pool2(x))\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    return x\n",
        " \n",
        "  def weights_init(self, m):\n",
        "    for m in self.children():\n",
        "      if isinstance(m,nn.Conv2d):\n",
        "        if self.init_method == \"he\":\n",
        "          nn.init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
        "        elif self.init_method == \"xavier\":\n",
        "          nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2))\n",
        "        elif self.init_method == \"uniform\":\n",
        "          nn.init.uniform_(m.weight, -0.5, 0.5)\n",
        "        elif self.init_method == \"norm\":\n",
        "          nn.init.normal_(m.weight, 0.0, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "68bU0ORv0Xb5"
      },
      "outputs": [],
      "source": [
        "def approx_relu(x):\n",
        "  \"\"\"4-degree approx of relu in [-6,6] from https://arxiv.org/pdf/2009.03727.pdf\"\"\"\n",
        "  a = 0.119782\n",
        "  b = 0.5\n",
        "  c = 0.204875\n",
        "  d = -0.0063896\n",
        "  x_2 = torch.square(x)\n",
        "  x_4 = torch.square(x_2)\n",
        "  return a + b*x + c*x_2 + d*x_4\n",
        "  \n",
        "class AlexNet(nn.Module):\n",
        "  def __init__(self, verbose: bool):\n",
        "    super().__init__()\n",
        "    self.verbose = verbose\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4, padding=0)\n",
        "    self.pool = nn.AvgPool2d(kernel_size=3, stride=2) ## this used to be MaxPool\n",
        "    self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding= 2)\n",
        "    self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding= 1)\n",
        "    self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "    self.fc1  = nn.Linear(in_features= 9216, out_features= 4096)\n",
        "    self.fc2  = nn.Linear(in_features= 4096, out_features= 4096)\n",
        "    self.fc3 = nn.Linear(in_features=4096 , out_features=10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    \"\"\"\n",
        "      TO DO:\n",
        "        probably we must use square because the degree explodes with the number of layers and becomes too computationally complex\n",
        "        refer to https://arxiv.org/pdf/1412.6181.pdf     \n",
        "    \"\"\"\n",
        "    x = approx_relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = approx_relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    x = approx_relu(self.conv3(x))\n",
        "    x = approx_relu(self.conv4(x))\n",
        "    x = approx_relu(self.conv5(x))\n",
        "    x = self.pool(x)\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = approx_relu(self.fc1(x))\n",
        "    x = approx_relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    x = torch.sigmoid(x) ##was softmax\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sX-7JDDtHOo"
      },
      "source": [
        "Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3zxoMQRRsF1o"
      },
      "outputs": [],
      "source": [
        "class DataHandler():\n",
        "  def __init__(self, dataset : str, batch_size : int, model : str):\n",
        "    if dataset == \"MNIST\":\n",
        "      self.batch_size = batch_size\n",
        "      if model == \"crypto\":\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "      elif model == \"alex\":\n",
        "        transform = transforms.Compose([\n",
        "          transforms.Resize((227, 227)),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "\n",
        "      train_ds = MNIST(\"data/\", train=True, download=True, transform=transform)\n",
        "      test_ds = MNIST(\"data/\", train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "      self.train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle=True, drop_last=True)\n",
        "      self.test_dl = DataLoader(test_ds, batch_size = batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXXqZm508qA1"
      },
      "source": [
        "Plot gradient flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "h3f19IYJ8nIQ"
      },
      "outputs": [],
      "source": [
        "def plot_grad_flow(named_parameters):\n",
        "    ## From https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063\n",
        "    ## Beware it's a little bit tricky to interpret results\n",
        "    '''Plots the gradients flowing through different layers in the net during training.\n",
        "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
        "    \n",
        "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
        "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
        "\n",
        "    ave_grads = []\n",
        "    max_grads = []\n",
        "    layers = []\n",
        "    for n, p in named_parameters:\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\n",
        "            layers.append(n)\n",
        "            ave_grads.append(p.grad.abs().mean())\n",
        "            max_grads.append(p.grad.abs().max())\n",
        "            print(f\"Layer {n}, grad avg {p.grad.mean()}, data {p.data.mean()}\")\n",
        "    plt.bar(np.arange(len(max_grads)), max(max_grads), alpha=0.1, lw=1, color=\"c\")\n",
        "    plt.bar(np.arange(len(max_grads)), np.mean(ave_grads), alpha=0.1, lw=1, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(left=0, right=len(ave_grads))\n",
        "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")\n",
        "    plt.grid(True)\n",
        "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
        "                Line2D([0], [0], color=\"b\", lw=4),\n",
        "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuwFtAqgtLYp"
      },
      "source": [
        "Training of 9-layer CryptoNet --> not working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnkgcAJktEu9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "## setup torch enviro\n",
        "torch.manual_seed(9325345339582034)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "## init model\n",
        "model = CryptoNet(verbose=False)\n",
        "model.apply(model.weights_init)\n",
        "model = model.to(device=device)\n",
        "\n",
        "dataHandler = DataHandler(\"MNIST\")\n",
        "\n",
        "## training params setup\n",
        "learning_rate = 3e-4\n",
        "momentum = 0.9\n",
        "num_epochs = 5000\n",
        "total_step = len(dataHandler.train_dl)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (data, labels) in enumerate(dataHandler.train_dl):\n",
        "    data = data.to(device=device)\n",
        "    labels = labels.to(device=device)\n",
        "    #labels = labels.to(torch.float32)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(data)\n",
        "    loss = criterion(predictions, labels)\n",
        "    loss.backward()\n",
        "    if model.verbose:\n",
        "      print(f\"[?] Step {i+1} Epoch {epoch+1}\")\n",
        "      plot_grad_flow(model.named_parameters())\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 50 == 0:\n",
        "      print ('[!] Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "\n",
        "torch.save(model, \"cryptoNet.pt\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OsCh6ldz9PN"
      },
      "source": [
        "Train and test pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "XfZdPxHKZwKi"
      },
      "outputs": [],
      "source": [
        "## setup torch enviro\n",
        "torch.manual_seed(9325345339582034)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dataHandler = DataHandler(dataset=\"MNIST\", batch_size=256, model=\"crypto\")\n",
        "\n",
        "## training params setup\n",
        "learning_rate = 3e-4\n",
        "total_step = len(dataHandler.train_dl)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, dataHandler, num_epochs):\n",
        "  num_epochs = num_epochs\n",
        "  model.train()\n",
        "  #optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  for epoch in range(num_epochs):\n",
        "    for i, (data, labels) in enumerate(dataHandler.train_dl):\n",
        "      data = data.to(device=device)\n",
        "      labels = labels.to(device=device)\n",
        "      #labels = labels.to(torch.float32)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      predictions = model(data)\n",
        "      loss = criterion(predictions, labels)\n",
        "      loss.backward()\n",
        "      if model.verbose:\n",
        "        print(f\"[?] Step {i+1} Epoch {epoch+1}\")\n",
        "        plot_grad_flow(model.named_parameters())\n",
        "      optimizer.step()\n",
        "\n",
        "      if (i+1) % 100 == 0:\n",
        "        print(\"=====================================================================================================================\")\n",
        "        print ('[!] Train Epoch [{}/{}], Step [{}/{}] ==> Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "\n",
        "def eval(model, dataHandler):\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  accuracy = 0\n",
        "  for _, (data,labels) in enumerate(dataHandler.test_dl):\n",
        "      data = data.to(device=\"cpu\")\n",
        "      labels = labels.to(device=\"cpu\")\n",
        "      ## Forward Pass\n",
        "      predictions = model(data)\n",
        "      loss += criterion(predictions, labels).item()\n",
        "      _, predicted_labels = predictions.max(1)\n",
        "      num_correct += (predicted_labels == labels).sum()\n",
        "      num_samples += predicted_labels.size(0)\n",
        "  \n",
        "  accuracy = float(num_correct) / float(num_samples)\n",
        "  loss = loss/len(dataHandler.test_dl)\n",
        "  print(\"======================================================================\")\n",
        "  print(f\"Average test Loss ==> {loss / len(dataHandler.test_dl)}\")\n",
        "  print(f\"Test accuracy ==> {float(num_correct) / float(num_samples) * 100:.2f}\")\n",
        "  return loss, accuracy "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6rZxIHu0HSU"
      },
      "source": [
        "Training of Simple-CryptoNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taDDWRZIb2yW",
        "outputId": "4558399a-9d3b-4600-b711-6387100428ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=====================================================================================================================\n",
            "[!] Train Epoch [15/50], Step [200/234] ==> Loss: 2414104748752896.0000\n",
            "=====================================================================================================================\n",
            "[!] Train Epoch [16/50], Step [100/234] ==> Loss: 2573159735754752.0000\n"
          ]
        }
      ],
      "source": [
        "## init models\n",
        "models = {\n",
        "    \"random\" : SimpleNet(batch_size=dataHandler.batch_size, init_method=\"random\",verbose=False).to(device=device),\n",
        "    \"he\" : SimpleNet(batch_size=dataHandler.batch_size, init_method=\"he\",verbose=False).to(device=device),\n",
        "    \"xavier\" : SimpleNet(batch_size=dataHandler.batch_size, init_method=\"xavier\",verbose=False).to(device=device),\n",
        "    #\"uniform\" : SimpleNet(batch_size=dataHandler.batch_size, init_method=\"uniform\",verbose=False).to(device=device),\n",
        "    \"norm\" : SimpleNet(batch_size=dataHandler.batch_size, init_method=\"norm\",verbose=False).to(device=device),\n",
        "}\n",
        "scores = {}\n",
        "for method, model in models.items():\n",
        "  model.apply(model.weights_init)\n",
        "  train(model, dataHandler, num_epochs=50)\n",
        "  loss, accuracy = eval(model, dataHandler)\n",
        "  scores[method] = {\"loss\":loss, \"accuracy\":accuracy}\n",
        "  torch.save(model, f\"SimpleNet_{method}.pt\")\n",
        "\n",
        "for method, metrics in scores.items():\n",
        "  print(\"======================================================================\")\n",
        "  print(f\"[+] Model with {method}: Avg test Loss ==> {metrics['loss']}, Accuracy ==> {metrics['accuracy']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS1nmc_4eM6f"
      },
      "source": [
        "Training of AlexNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gB2eH_bjx2K"
      },
      "outputs": [],
      "source": [
        "dataHandler = dataHandler = DataHandler(dataset=\"MNIST\", batch_size=256, model=\"alex\")\n",
        "model = AlexNet(False).to(device=device)\n",
        "train(model, dataHandler, 10)\n",
        "eval(model, dataHandler)\n",
        "torch.save(model, \"AlexNet.pt\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
